{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import numpy as np\n",
    "def print_accuracy(f, X, y):\n",
    "    accuracy = 100*np.sum(f(X) == y)/len(y)\n",
    "    print(\"Accuracy = {0}%\".format(accuracy))\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_normal = np.arange(0, 15, 1)\n",
    "num_overwhelmed = [3]*len(num_normal)\n",
    "num_shortcut = [1, 2, 3]\n",
    "num_shortcut = np.repeat(np.array(num_shortcut), 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from FeaturePoison import insert_feature_noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = shap.datasets.iris()\n",
    "stg_results = {\n",
    "    '# features': [],\n",
    "    \"accuracy\": []\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100: loss=1.133566 valid_loss=1.085879\n",
      "Epoch: 200: loss=0.765084 valid_loss=0.677843\n",
      "Epoch: 300: loss=0.363273 valid_loss=0.269407\n",
      "Epoch: 400: loss=0.261523 valid_loss=0.121529\n",
      "Epoch: 500: loss=0.191832 valid_loss=0.074987\n",
      "Epoch: 600: loss=0.400192 valid_loss=0.088916\n",
      "Epoch: 700: loss=0.152795 valid_loss=0.047971\n",
      "Epoch: 800: loss=0.154857 valid_loss=0.050353\n",
      "Epoch: 900: loss=0.137464 valid_loss=0.034855\n",
      "Epoch: 1000: loss=0.131917 valid_loss=0.031622\n",
      "Accuracy = 100.0%\n",
      "100.0%, 5/8 features\n",
      "Epoch: 100: loss=1.176074 valid_loss=1.119501\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 200: loss=1.170214 valid_loss=1.111194\n",
      "Epoch: 300: loss=0.998562 valid_loss=0.998719\n",
      "Epoch: 400: loss=0.523353 valid_loss=0.579712\n",
      "Epoch: 500: loss=0.310882 valid_loss=0.321728\n",
      "Epoch: 600: loss=0.246531 valid_loss=0.167505\n",
      "Epoch: 700: loss=0.191787 valid_loss=0.141882\n",
      "Epoch: 800: loss=0.175927 valid_loss=0.101692\n",
      "Epoch: 900: loss=0.189078 valid_loss=0.119423\n",
      "Epoch: 1000: loss=0.156575 valid_loss=0.068605\n",
      "Accuracy = 100.0%\n",
      "100.0%, 8/10 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100: loss=1.158779 valid_loss=1.111201\n",
      "Epoch: 200: loss=0.964535 valid_loss=0.949152\n",
      "Epoch: 300: loss=0.512112 valid_loss=0.513560\n",
      "Epoch: 400: loss=0.383921 valid_loss=0.254993\n",
      "Epoch: 500: loss=0.259825 valid_loss=0.253071\n",
      "Epoch: 600: loss=0.193670 valid_loss=0.128653\n",
      "Epoch: 700: loss=0.199495 valid_loss=0.099764\n",
      "Epoch: 800: loss=0.162096 valid_loss=0.079187\n",
      "Epoch: 900: loss=0.156504 valid_loss=0.071846\n",
      "Epoch: 1000: loss=0.151670 valid_loss=0.063647\n",
      "Accuracy = 100.0%\n",
      "100.0%, 12/12 features\n",
      "Epoch: 100: loss=1.177913 valid_loss=1.119025\n",
      "Epoch: 200: loss=1.131998 valid_loss=1.087696\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 300: loss=0.628811 valid_loss=0.619433\n",
      "Epoch: 400: loss=0.417981 valid_loss=0.377064\n",
      "Epoch: 500: loss=0.284442 valid_loss=0.220847\n",
      "Epoch: 600: loss=0.212726 valid_loss=0.136172\n",
      "Epoch: 700: loss=0.182384 valid_loss=0.106939\n",
      "Epoch: 800: loss=0.169524 valid_loss=0.085205\n",
      "Epoch: 900: loss=0.166900 valid_loss=0.078400\n",
      "Epoch: 1000: loss=0.182481 valid_loss=0.093633\n",
      "Accuracy = 100.0%\n",
      "100.0%, 14/14 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100: loss=1.147425 valid_loss=1.088701\n",
      "Epoch: 200: loss=0.600021 valid_loss=0.598437\n",
      "Epoch: 300: loss=0.444577 valid_loss=0.294937\n",
      "Epoch: 400: loss=0.250380 valid_loss=0.194738\n",
      "Epoch: 500: loss=0.208651 valid_loss=0.125236\n",
      "Epoch: 600: loss=0.192829 valid_loss=0.089159\n",
      "Epoch: 700: loss=0.163230 valid_loss=0.077077\n",
      "Epoch: 800: loss=0.159120 valid_loss=0.068253\n",
      "Epoch: 900: loss=0.164673 valid_loss=0.070367\n",
      "Epoch: 1000: loss=0.155772 valid_loss=0.055227\n",
      "Accuracy = 100.0%\n",
      "100.0%, 16/16 features\n",
      "Epoch: 100: loss=1.169481 valid_loss=1.102681\n",
      "Epoch: 200: loss=0.769365 valid_loss=0.758566\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 300: loss=0.381866 valid_loss=0.295065\n",
      "Epoch: 400: loss=0.230322 valid_loss=0.174832\n",
      "Epoch: 500: loss=0.257380 valid_loss=0.127879\n",
      "Epoch: 600: loss=0.231503 valid_loss=0.083673\n",
      "Epoch: 700: loss=0.166629 valid_loss=0.062984\n",
      "Epoch: 800: loss=0.206171 valid_loss=0.091044\n",
      "Epoch: 900: loss=0.162621 valid_loss=0.051034\n",
      "Epoch: 1000: loss=0.157165 valid_loss=0.049504\n",
      "Accuracy = 96.66666666666667%\n",
      "96.66666666666667%, 19/19 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100: loss=1.089987 valid_loss=1.054755\n",
      "Epoch: 200: loss=0.619358 valid_loss=0.552077\n",
      "Epoch: 300: loss=0.347305 valid_loss=0.496141\n",
      "Epoch: 400: loss=0.400260 valid_loss=0.401208\n",
      "Epoch: 500: loss=0.205967 valid_loss=0.109310\n",
      "Epoch: 600: loss=0.172228 valid_loss=0.102420\n",
      "Epoch: 700: loss=0.152602 valid_loss=0.070264\n",
      "Epoch: 800: loss=0.149959 valid_loss=0.056604\n",
      "Epoch: 900: loss=0.123287 valid_loss=0.050780\n",
      "Epoch: 1000: loss=0.113677 valid_loss=0.044653\n",
      "Accuracy = 100.0%\n",
      "100.0%, 21/21 features\n",
      "Epoch: 100: loss=1.177386 valid_loss=1.118917\n",
      "Epoch: 200: loss=1.164655 valid_loss=1.112162\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 300: loss=1.020325 valid_loss=0.974296\n",
      "Epoch: 400: loss=0.481447 valid_loss=0.599078\n",
      "Epoch: 500: loss=0.293896 valid_loss=0.263769\n",
      "Epoch: 600: loss=0.240918 valid_loss=0.138779\n",
      "Epoch: 700: loss=0.176457 valid_loss=0.109593\n",
      "Epoch: 800: loss=0.170387 valid_loss=0.093931\n",
      "Epoch: 900: loss=0.158907 valid_loss=0.072621\n",
      "Epoch: 1000: loss=0.196556 valid_loss=0.070183\n",
      "Accuracy = 96.66666666666667%\n",
      "96.66666666666667%, 23/23 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100: loss=1.173985 valid_loss=1.115159\n",
      "Epoch: 200: loss=1.156203 valid_loss=1.066777\n",
      "Epoch: 300: loss=0.578520 valid_loss=0.645041\n",
      "Epoch: 400: loss=0.325339 valid_loss=0.229454\n",
      "Epoch: 500: loss=0.259215 valid_loss=0.305543\n",
      "Epoch: 600: loss=0.234433 valid_loss=0.265007\n",
      "Epoch: 700: loss=0.165104 valid_loss=0.117380\n",
      "Epoch: 800: loss=0.162411 valid_loss=0.087292\n",
      "Epoch: 900: loss=0.148134 valid_loss=0.060065\n",
      "Epoch: 1000: loss=0.143200 valid_loss=0.059674\n",
      "Accuracy = 96.66666666666667%\n",
      "96.66666666666667%, 25/25 features\n",
      "Epoch: 100: loss=1.147725 valid_loss=1.103868\n",
      "Epoch: 200: loss=0.798400 valid_loss=0.820311\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 300: loss=0.425967 valid_loss=0.405856\n",
      "Epoch: 400: loss=0.490233 valid_loss=0.240464\n",
      "Epoch: 500: loss=0.211959 valid_loss=0.182419\n",
      "Epoch: 600: loss=0.202313 valid_loss=0.148844\n",
      "Epoch: 700: loss=0.183114 valid_loss=0.111080\n",
      "Epoch: 800: loss=0.163924 valid_loss=0.092996\n",
      "Epoch: 900: loss=0.166435 valid_loss=0.095743\n",
      "Epoch: 1000: loss=0.171313 valid_loss=0.071445\n",
      "Accuracy = 96.66666666666667%\n",
      "96.66666666666667%, 27/27 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100: loss=1.168273 valid_loss=1.100886\n",
      "Epoch: 200: loss=0.774567 valid_loss=0.709750\n",
      "Epoch: 300: loss=0.401700 valid_loss=0.365463\n",
      "Epoch: 400: loss=0.227414 valid_loss=0.184477\n",
      "Epoch: 500: loss=0.229155 valid_loss=0.147816\n",
      "Epoch: 600: loss=0.221326 valid_loss=0.077347\n",
      "Epoch: 700: loss=0.151725 valid_loss=0.070266\n",
      "Epoch: 800: loss=0.219923 valid_loss=0.089771\n",
      "Epoch: 900: loss=0.138178 valid_loss=0.054921\n",
      "Epoch: 1000: loss=0.126896 valid_loss=0.045226\n",
      "Accuracy = 100.0%\n",
      "100.0%, 30/30 features\n",
      "Epoch: 100: loss=1.166536 valid_loss=1.103501\n",
      "Epoch: 200: loss=0.917752 valid_loss=0.842867\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 300: loss=0.513960 valid_loss=0.413880\n",
      "Epoch: 400: loss=0.335211 valid_loss=0.428129\n",
      "Epoch: 500: loss=0.262340 valid_loss=0.207472\n",
      "Epoch: 600: loss=0.221976 valid_loss=0.125725\n",
      "Epoch: 700: loss=0.181173 valid_loss=0.128018\n",
      "Epoch: 800: loss=0.156133 valid_loss=0.103475\n",
      "Epoch: 900: loss=0.169962 valid_loss=0.108895\n",
      "Epoch: 1000: loss=0.279127 valid_loss=0.133212\n",
      "Accuracy = 93.33333333333333%\n",
      "93.33333333333333%, 32/32 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100: loss=1.168101 valid_loss=1.110348\n",
      "Epoch: 200: loss=0.831717 valid_loss=0.815111\n",
      "Epoch: 300: loss=0.387099 valid_loss=0.309647\n",
      "Epoch: 400: loss=0.261859 valid_loss=0.158610\n",
      "Epoch: 500: loss=0.334300 valid_loss=0.575675\n",
      "Epoch: 600: loss=0.246232 valid_loss=0.101257\n",
      "Epoch: 700: loss=0.163640 valid_loss=0.070939\n",
      "Epoch: 800: loss=0.162369 valid_loss=0.061161\n",
      "Epoch: 900: loss=0.151043 valid_loss=0.052368\n",
      "Epoch: 1000: loss=0.152221 valid_loss=0.073838\n",
      "Accuracy = 96.66666666666667%\n",
      "96.66666666666667%, 34/34 features\n",
      "Epoch: 100: loss=1.160808 valid_loss=1.113113\n",
      "Epoch: 200: loss=1.089078 valid_loss=0.959416\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 300: loss=0.588829 valid_loss=0.590830\n",
      "Epoch: 400: loss=0.469029 valid_loss=0.489878\n",
      "Epoch: 500: loss=0.375248 valid_loss=0.364958\n",
      "Epoch: 600: loss=0.307939 valid_loss=0.263737\n",
      "Epoch: 700: loss=0.231513 valid_loss=0.222531\n",
      "Epoch: 800: loss=0.214039 valid_loss=0.137845\n",
      "Epoch: 900: loss=0.532264 valid_loss=0.152213\n",
      "Epoch: 1000: loss=0.194960 valid_loss=0.120117\n",
      "Accuracy = 100.0%\n",
      "100.0%, 36/36 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100: loss=1.155871 valid_loss=1.111439\n",
      "Epoch: 200: loss=1.064121 valid_loss=0.912503\n",
      "Epoch: 300: loss=0.415162 valid_loss=0.433704\n",
      "Epoch: 400: loss=0.266339 valid_loss=0.199397\n",
      "Epoch: 500: loss=0.360661 valid_loss=0.431531\n",
      "Epoch: 600: loss=0.191903 valid_loss=0.127900\n",
      "Epoch: 700: loss=0.173551 valid_loss=0.105225\n",
      "Epoch: 800: loss=0.163907 valid_loss=0.181052\n",
      "Epoch: 900: loss=0.199313 valid_loss=0.111589\n",
      "Epoch: 1000: loss=0.149905 valid_loss=0.083614\n",
      "Accuracy = 96.66666666666667%\n",
      "96.66666666666667%, 38/38 features\n",
      "Epoch: 100: loss=1.168460 valid_loss=1.114314\n",
      "Epoch: 200: loss=1.054796 valid_loss=1.019567\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 300: loss=0.550914 valid_loss=0.541234\n",
      "Epoch: 400: loss=0.427523 valid_loss=0.371536\n",
      "Epoch: 500: loss=0.301384 valid_loss=0.252974\n",
      "Epoch: 600: loss=0.232565 valid_loss=0.164380\n",
      "Epoch: 700: loss=0.177722 valid_loss=0.095275\n",
      "Epoch: 800: loss=0.215150 valid_loss=0.081899\n",
      "Epoch: 900: loss=0.148586 valid_loss=0.053672\n",
      "Epoch: 1000: loss=0.148586 valid_loss=0.046522\n",
      "Accuracy = 100.0%\n",
      "100.0%, 5/8 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100: loss=1.140759 valid_loss=1.094593\n",
      "Epoch: 200: loss=1.085443 valid_loss=0.769926\n",
      "Epoch: 300: loss=0.378230 valid_loss=0.390275\n",
      "Epoch: 400: loss=0.255654 valid_loss=0.161614\n",
      "Epoch: 500: loss=0.207842 valid_loss=0.112246\n",
      "Epoch: 600: loss=0.170954 valid_loss=0.091882\n",
      "Epoch: 700: loss=0.162254 valid_loss=0.079529\n",
      "Epoch: 800: loss=0.151933 valid_loss=0.062425\n",
      "Epoch: 900: loss=0.154895 valid_loss=0.072779\n",
      "Epoch: 1000: loss=0.142812 valid_loss=0.057905\n",
      "Accuracy = 96.66666666666667%\n",
      "96.66666666666667%, 7/10 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100: loss=1.151526 valid_loss=1.107347\n",
      "Epoch: 200: loss=0.812484 valid_loss=0.848366\n",
      "Epoch: 300: loss=0.544964 valid_loss=0.540719\n",
      "Epoch: 400: loss=0.407892 valid_loss=0.412458\n",
      "Epoch: 500: loss=0.354194 valid_loss=0.344002\n",
      "Epoch: 600: loss=0.233024 valid_loss=0.262877\n",
      "Epoch: 700: loss=0.194552 valid_loss=0.139425\n",
      "Epoch: 800: loss=0.175725 valid_loss=0.107502\n",
      "Epoch: 900: loss=0.169941 valid_loss=0.090710\n",
      "Epoch: 1000: loss=0.158304 valid_loss=0.082107\n",
      "Accuracy = 96.66666666666667%\n",
      "96.66666666666667%, 10/12 features\n",
      "Epoch: 100: loss=1.162916 valid_loss=1.113326\n",
      "Epoch: 200: loss=0.987188 valid_loss=0.993838\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 300: loss=0.487048 valid_loss=0.472704\n",
      "Epoch: 400: loss=0.492197 valid_loss=0.300865\n",
      "Epoch: 500: loss=0.228633 valid_loss=0.190638\n",
      "Epoch: 600: loss=0.200662 valid_loss=0.134298\n",
      "Epoch: 700: loss=0.189169 valid_loss=0.102513\n",
      "Epoch: 800: loss=0.174070 valid_loss=0.090688\n",
      "Epoch: 900: loss=0.162677 valid_loss=0.068895\n",
      "Epoch: 1000: loss=0.143224 valid_loss=0.060601\n",
      "Accuracy = 100.0%\n",
      "100.0%, 14/14 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100: loss=1.174480 valid_loss=1.117647\n",
      "Epoch: 200: loss=1.132812 valid_loss=1.087208\n",
      "Epoch: 300: loss=0.640548 valid_loss=0.626665\n",
      "Epoch: 400: loss=0.386679 valid_loss=0.331832\n",
      "Epoch: 500: loss=0.310611 valid_loss=0.324615\n",
      "Epoch: 600: loss=0.217221 valid_loss=0.121792\n",
      "Epoch: 700: loss=0.175037 valid_loss=0.090972\n",
      "Epoch: 800: loss=0.157548 valid_loss=0.071122\n",
      "Epoch: 900: loss=0.147728 valid_loss=0.057723\n",
      "Epoch: 1000: loss=0.150402 valid_loss=0.057054\n",
      "Accuracy = 96.66666666666667%\n",
      "96.66666666666667%, 16/16 features\n",
      "Epoch: 100: loss=1.178042 valid_loss=1.117188\n",
      "Epoch: 200: loss=1.144259 valid_loss=1.079513\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 300: loss=0.526874 valid_loss=0.522141\n",
      "Epoch: 400: loss=0.295868 valid_loss=0.222173\n",
      "Epoch: 500: loss=0.281875 valid_loss=0.135526\n",
      "Epoch: 600: loss=0.181700 valid_loss=0.111474\n",
      "Epoch: 700: loss=0.168111 valid_loss=0.100178\n",
      "Epoch: 800: loss=0.155908 valid_loss=0.115365\n",
      "Epoch: 900: loss=0.163635 valid_loss=0.095633\n",
      "Epoch: 1000: loss=0.156912 valid_loss=0.097197\n",
      "Accuracy = 93.33333333333333%\n",
      "93.33333333333333%, 19/19 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100: loss=1.165159 valid_loss=1.106201\n",
      "Epoch: 200: loss=0.885885 valid_loss=0.861220\n",
      "Epoch: 300: loss=0.719242 valid_loss=0.430113\n",
      "Epoch: 400: loss=0.299382 valid_loss=0.281358\n",
      "Epoch: 500: loss=0.368944 valid_loss=0.221106\n",
      "Epoch: 600: loss=0.239474 valid_loss=0.139665\n",
      "Epoch: 700: loss=0.151897 valid_loss=0.101792\n",
      "Epoch: 800: loss=0.162018 valid_loss=0.072650\n",
      "Epoch: 900: loss=0.180092 valid_loss=0.059283\n",
      "Epoch: 1000: loss=0.132930 valid_loss=0.051916\n",
      "Accuracy = 100.0%\n",
      "100.0%, 21/21 features\n",
      "Epoch: 100: loss=1.177635 valid_loss=1.117648\n",
      "Epoch: 200: loss=1.158254 valid_loss=1.093801\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 300: loss=0.680596 valid_loss=0.709671\n",
      "Epoch: 400: loss=0.360059 valid_loss=0.320843\n",
      "Epoch: 500: loss=0.251362 valid_loss=0.171108\n",
      "Epoch: 600: loss=0.206552 valid_loss=0.122199\n",
      "Epoch: 700: loss=0.187871 valid_loss=0.106940\n",
      "Epoch: 800: loss=0.180297 valid_loss=0.098852\n",
      "Epoch: 900: loss=0.147299 valid_loss=0.056286\n",
      "Epoch: 1000: loss=0.146514 valid_loss=0.049764\n",
      "Accuracy = 100.0%\n",
      "100.0%, 23/23 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100: loss=1.171301 valid_loss=1.115871\n",
      "Epoch: 200: loss=1.019820 valid_loss=1.039240\n",
      "Epoch: 300: loss=0.598783 valid_loss=0.577039\n",
      "Epoch: 400: loss=0.301609 valid_loss=0.292071\n",
      "Epoch: 500: loss=0.338012 valid_loss=0.177707\n",
      "Epoch: 600: loss=0.275050 valid_loss=0.206523\n",
      "Epoch: 700: loss=0.181821 valid_loss=0.143446\n",
      "Epoch: 800: loss=0.159709 valid_loss=0.111375\n",
      "Epoch: 900: loss=0.142635 valid_loss=0.098282\n",
      "Epoch: 1000: loss=0.137293 valid_loss=0.085924\n",
      "Accuracy = 96.66666666666667%\n",
      "96.66666666666667%, 25/25 features\n",
      "Epoch: 100: loss=1.157407 valid_loss=1.110065\n",
      "Epoch: 200: loss=0.820560 valid_loss=0.859947\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 300: loss=0.470923 valid_loss=0.390778\n",
      "Epoch: 400: loss=0.283094 valid_loss=0.179580\n",
      "Epoch: 500: loss=0.220726 valid_loss=0.127029\n",
      "Epoch: 600: loss=0.308987 valid_loss=0.156881\n",
      "Epoch: 700: loss=0.237094 valid_loss=0.066916\n",
      "Epoch: 800: loss=0.156508 valid_loss=0.049181\n",
      "Epoch: 900: loss=0.151264 valid_loss=0.039476\n",
      "Epoch: 1000: loss=0.138685 valid_loss=0.033028\n",
      "Accuracy = 100.0%\n",
      "100.0%, 27/27 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100: loss=1.145285 valid_loss=1.108354\n",
      "Epoch: 200: loss=0.811239 valid_loss=0.937957\n",
      "Epoch: 300: loss=0.400165 valid_loss=0.367892\n",
      "Epoch: 400: loss=0.269000 valid_loss=0.297562\n",
      "Epoch: 500: loss=0.207905 valid_loss=0.181133\n",
      "Epoch: 600: loss=0.179058 valid_loss=0.165546\n",
      "Epoch: 700: loss=0.186420 valid_loss=0.196482\n",
      "Epoch: 800: loss=0.139616 valid_loss=0.155934\n",
      "Epoch: 900: loss=0.158161 valid_loss=0.148566\n",
      "Epoch: 1000: loss=0.133755 valid_loss=0.154865\n",
      "Accuracy = 96.66666666666667%\n",
      "96.66666666666667%, 30/30 features\n",
      "Epoch: 100: loss=1.175438 valid_loss=1.119621\n",
      "Epoch: 200: loss=1.159335 valid_loss=1.102214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 300: loss=0.725989 valid_loss=0.720517\n",
      "Epoch: 400: loss=0.383303 valid_loss=0.428618\n",
      "Epoch: 500: loss=0.236105 valid_loss=0.209099\n",
      "Epoch: 600: loss=0.264553 valid_loss=0.182835\n",
      "Epoch: 700: loss=0.161462 valid_loss=0.105768\n",
      "Epoch: 800: loss=0.142459 valid_loss=0.078152\n",
      "Epoch: 900: loss=0.138074 valid_loss=0.071285\n",
      "Epoch: 1000: loss=0.194874 valid_loss=0.072969\n",
      "Accuracy = 96.66666666666667%\n",
      "96.66666666666667%, 32/32 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100: loss=1.155883 valid_loss=1.105669\n",
      "Epoch: 200: loss=0.833033 valid_loss=0.849676\n",
      "Epoch: 300: loss=0.466553 valid_loss=0.364617\n",
      "Epoch: 400: loss=0.252893 valid_loss=0.232040\n",
      "Epoch: 500: loss=0.219813 valid_loss=0.196149\n",
      "Epoch: 600: loss=0.281722 valid_loss=0.199067\n",
      "Epoch: 700: loss=0.160071 valid_loss=0.119123\n",
      "Epoch: 800: loss=0.150021 valid_loss=0.099550\n",
      "Epoch: 900: loss=0.155787 valid_loss=0.153400\n",
      "Epoch: 1000: loss=0.151036 valid_loss=0.076297\n",
      "Accuracy = 96.66666666666667%\n",
      "96.66666666666667%, 34/34 features\n",
      "Epoch: 100: loss=1.169994 valid_loss=1.113849\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 200: loss=0.943081 valid_loss=0.933863\n",
      "Epoch: 300: loss=0.681099 valid_loss=0.671732\n",
      "Epoch: 400: loss=0.393129 valid_loss=0.393888\n",
      "Epoch: 500: loss=0.270883 valid_loss=0.261246\n",
      "Epoch: 600: loss=0.238617 valid_loss=0.250416\n",
      "Epoch: 700: loss=0.406583 valid_loss=1.528657\n",
      "Epoch: 800: loss=0.179065 valid_loss=0.111191\n",
      "Epoch: 900: loss=0.160839 valid_loss=0.080808\n",
      "Epoch: 1000: loss=0.164395 valid_loss=0.067478\n",
      "Accuracy = 100.0%\n",
      "100.0%, 36/36 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100: loss=1.161787 valid_loss=1.116936\n",
      "Epoch: 200: loss=0.970109 valid_loss=1.007610\n",
      "Epoch: 300: loss=0.471740 valid_loss=0.487776\n",
      "Epoch: 400: loss=0.338038 valid_loss=0.411033\n",
      "Epoch: 500: loss=0.254501 valid_loss=0.185871\n",
      "Epoch: 600: loss=0.174173 valid_loss=0.185803\n",
      "Epoch: 700: loss=2.779272 valid_loss=0.543373\n",
      "Epoch: 800: loss=0.149998 valid_loss=0.123180\n",
      "Epoch: 900: loss=0.160750 valid_loss=0.138846\n",
      "Epoch: 1000: loss=0.129019 valid_loss=0.116643\n",
      "Accuracy = 96.66666666666667%\n",
      "96.66666666666667%, 38/38 features\n",
      "Epoch: 100: loss=1.145519 valid_loss=1.099855\n",
      "Epoch: 200: loss=0.858913 valid_loss=0.712071\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 300: loss=0.383766 valid_loss=0.270660\n",
      "Epoch: 400: loss=0.220921 valid_loss=0.123802\n",
      "Epoch: 500: loss=0.212366 valid_loss=0.069763\n",
      "Epoch: 600: loss=0.164130 valid_loss=0.051567\n",
      "Epoch: 700: loss=0.146957 valid_loss=0.042178\n",
      "Epoch: 800: loss=0.139858 valid_loss=0.036532\n",
      "Epoch: 900: loss=0.132635 valid_loss=0.031100\n",
      "Epoch: 1000: loss=0.134798 valid_loss=0.030298\n",
      "Accuracy = 100.0%\n",
      "100.0%, 5/8 features\n",
      "Epoch: 100: loss=1.175901 valid_loss=1.111486\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 200: loss=1.022286 valid_loss=0.997354\n",
      "Epoch: 300: loss=0.569333 valid_loss=0.794108\n",
      "Epoch: 400: loss=0.356707 valid_loss=0.383097\n",
      "Epoch: 500: loss=0.247775 valid_loss=0.265405\n",
      "Epoch: 600: loss=0.201241 valid_loss=0.139778\n",
      "Epoch: 700: loss=0.189758 valid_loss=0.096809\n",
      "Epoch: 800: loss=0.176962 valid_loss=0.077097\n",
      "Epoch: 900: loss=0.159805 valid_loss=0.063513\n",
      "Epoch: 1000: loss=0.153508 valid_loss=0.058863\n",
      "Accuracy = 96.66666666666667%\n",
      "96.66666666666667%, 7/10 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100: loss=1.169076 valid_loss=1.098238\n",
      "Epoch: 200: loss=0.805125 valid_loss=0.765565\n",
      "Epoch: 300: loss=0.469745 valid_loss=0.445086\n",
      "Epoch: 400: loss=0.272773 valid_loss=0.260575\n",
      "Epoch: 500: loss=0.632887 valid_loss=0.142391\n",
      "Epoch: 600: loss=0.187395 valid_loss=0.102575\n",
      "Epoch: 700: loss=0.171082 valid_loss=0.093616\n",
      "Epoch: 800: loss=0.156222 valid_loss=0.076816\n",
      "Epoch: 900: loss=0.146779 valid_loss=0.062444\n",
      "Epoch: 1000: loss=0.139662 valid_loss=0.069187\n",
      "Accuracy = 96.66666666666667%\n",
      "96.66666666666667%, 9/12 features\n",
      "Epoch: 100: loss=1.159857 valid_loss=1.100164\n",
      "Epoch: 200: loss=0.858784 valid_loss=0.814138\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 300: loss=0.791825 valid_loss=0.584190\n",
      "Epoch: 400: loss=0.320180 valid_loss=0.254841\n",
      "Epoch: 500: loss=0.234145 valid_loss=0.158408\n",
      "Epoch: 600: loss=0.197233 valid_loss=0.123955\n",
      "Epoch: 700: loss=0.176579 valid_loss=0.127174\n",
      "Epoch: 800: loss=0.167430 valid_loss=0.071609\n",
      "Epoch: 900: loss=0.152602 valid_loss=0.059388\n",
      "Epoch: 1000: loss=0.150842 valid_loss=0.054307\n",
      "Accuracy = 100.0%\n",
      "100.0%, 12/14 features\n",
      "Epoch: 100: loss=1.173049 valid_loss=1.116072\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 200: loss=1.104517 valid_loss=1.070833\n",
      "Epoch: 300: loss=1.344929 valid_loss=0.853841\n",
      "Epoch: 400: loss=0.357086 valid_loss=0.304401\n",
      "Epoch: 500: loss=0.271696 valid_loss=0.277835\n",
      "Epoch: 600: loss=0.221444 valid_loss=0.112729\n",
      "Epoch: 700: loss=0.180053 valid_loss=0.095865\n",
      "Epoch: 800: loss=0.214915 valid_loss=0.096437\n",
      "Epoch: 900: loss=0.158336 valid_loss=0.071108\n",
      "Epoch: 1000: loss=0.153704 valid_loss=0.081298\n",
      "Accuracy = 100.0%\n",
      "100.0%, 16/16 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100: loss=1.144358 valid_loss=1.063775\n",
      "Epoch: 200: loss=0.490576 valid_loss=0.475711\n",
      "Epoch: 300: loss=0.404494 valid_loss=0.248225\n",
      "Epoch: 400: loss=0.214025 valid_loss=0.191748\n",
      "Epoch: 500: loss=0.181857 valid_loss=0.101166\n",
      "Epoch: 600: loss=0.205327 valid_loss=0.090228\n",
      "Epoch: 700: loss=0.177179 valid_loss=0.079643\n",
      "Epoch: 800: loss=0.166506 valid_loss=0.068242\n",
      "Epoch: 900: loss=0.165486 valid_loss=0.072840\n",
      "Epoch: 1000: loss=0.164503 valid_loss=0.068499\n",
      "Accuracy = 96.66666666666667%\n",
      "96.66666666666667%, 19/19 features\n",
      "Epoch: 100: loss=1.149670 valid_loss=1.094493\n",
      "Epoch: 200: loss=0.658442 valid_loss=0.637326\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 300: loss=0.384992 valid_loss=0.308144\n",
      "Epoch: 400: loss=0.314831 valid_loss=0.388797\n",
      "Epoch: 500: loss=0.218324 valid_loss=0.134859\n",
      "Epoch: 600: loss=0.251666 valid_loss=0.247270\n",
      "Epoch: 700: loss=0.173374 valid_loss=0.103536\n",
      "Epoch: 800: loss=0.171099 valid_loss=0.085260\n",
      "Epoch: 900: loss=0.164163 valid_loss=0.063260\n",
      "Epoch: 1000: loss=0.155398 valid_loss=0.062492\n",
      "Accuracy = 96.66666666666667%\n",
      "96.66666666666667%, 21/21 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100: loss=1.175623 valid_loss=1.117975\n",
      "Epoch: 200: loss=1.142852 valid_loss=1.097504\n",
      "Epoch: 300: loss=0.784272 valid_loss=0.711203\n",
      "Epoch: 400: loss=0.337129 valid_loss=0.304644\n",
      "Epoch: 500: loss=0.222139 valid_loss=0.201522\n",
      "Epoch: 600: loss=0.180334 valid_loss=0.137944\n",
      "Epoch: 700: loss=0.163312 valid_loss=0.134686\n",
      "Epoch: 800: loss=0.148208 valid_loss=0.114386\n",
      "Epoch: 900: loss=0.168651 valid_loss=0.149499\n",
      "Epoch: 1000: loss=0.128330 valid_loss=0.144943\n",
      "Accuracy = 93.33333333333333%\n",
      "93.33333333333333%, 23/23 features\n",
      "Epoch: 100: loss=1.174574 valid_loss=1.104240\n",
      "Epoch: 200: loss=1.032436 valid_loss=0.929375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 300: loss=0.548592 valid_loss=0.623348\n",
      "Epoch: 400: loss=0.298206 valid_loss=0.209373\n",
      "Epoch: 500: loss=0.207607 valid_loss=0.170990\n",
      "Epoch: 600: loss=0.212713 valid_loss=0.089510\n",
      "Epoch: 700: loss=0.170828 valid_loss=0.079343\n",
      "Epoch: 800: loss=0.140118 valid_loss=0.061390\n",
      "Epoch: 900: loss=0.208475 valid_loss=0.127944\n",
      "Epoch: 1000: loss=0.131364 valid_loss=0.046779\n",
      "Accuracy = 96.66666666666667%\n",
      "96.66666666666667%, 25/25 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100: loss=1.170233 valid_loss=1.111609\n",
      "Epoch: 200: loss=1.064563 valid_loss=1.052043\n",
      "Epoch: 300: loss=0.588803 valid_loss=0.525663\n",
      "Epoch: 400: loss=0.270532 valid_loss=0.207735\n",
      "Epoch: 500: loss=0.210869 valid_loss=0.131097\n",
      "Epoch: 600: loss=0.189825 valid_loss=0.088404\n",
      "Epoch: 700: loss=0.170282 valid_loss=0.085232\n",
      "Epoch: 800: loss=0.157557 valid_loss=0.063241\n",
      "Epoch: 900: loss=0.155914 valid_loss=0.084786\n",
      "Epoch: 1000: loss=0.154227 valid_loss=0.042013\n",
      "Accuracy = 100.0%\n",
      "100.0%, 27/27 features\n",
      "Epoch: 100: loss=1.158770 valid_loss=1.091496\n",
      "Epoch: 200: loss=0.730337 valid_loss=0.667463\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 300: loss=0.659412 valid_loss=0.268557\n",
      "Epoch: 400: loss=0.234822 valid_loss=0.160814\n",
      "Epoch: 500: loss=0.202985 valid_loss=0.176868\n",
      "Epoch: 600: loss=0.302094 valid_loss=0.134404\n",
      "Epoch: 700: loss=0.168704 valid_loss=0.087557\n",
      "Epoch: 800: loss=0.168492 valid_loss=0.105658\n",
      "Epoch: 900: loss=0.154979 valid_loss=0.050505\n",
      "Epoch: 1000: loss=0.182775 valid_loss=0.097276\n",
      "Accuracy = 93.33333333333333%\n",
      "93.33333333333333%, 30/30 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100: loss=1.163057 valid_loss=1.108203\n",
      "Epoch: 200: loss=0.907687 valid_loss=0.777971\n",
      "Epoch: 300: loss=0.464949 valid_loss=0.445237\n",
      "Epoch: 400: loss=0.338909 valid_loss=0.215623\n",
      "Epoch: 500: loss=0.389995 valid_loss=0.420227\n",
      "Epoch: 600: loss=0.203024 valid_loss=0.117082\n",
      "Epoch: 700: loss=0.176116 valid_loss=0.144585\n",
      "Epoch: 800: loss=0.414423 valid_loss=0.135884\n",
      "Epoch: 900: loss=0.153192 valid_loss=0.085628\n",
      "Epoch: 1000: loss=0.135883 valid_loss=0.070575\n",
      "Accuracy = 100.0%\n",
      "100.0%, 32/32 features\n",
      "Epoch: 100: loss=1.128385 valid_loss=1.083242\n",
      "Epoch: 200: loss=0.620247 valid_loss=0.670252\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 300: loss=0.374769 valid_loss=0.254015\n",
      "Epoch: 400: loss=0.222023 valid_loss=0.157518\n",
      "Epoch: 500: loss=0.231122 valid_loss=0.120857\n",
      "Epoch: 600: loss=0.174244 valid_loss=0.131771\n",
      "Epoch: 700: loss=0.159653 valid_loss=0.090885\n",
      "Epoch: 800: loss=0.170986 valid_loss=0.070249\n",
      "Epoch: 900: loss=0.189470 valid_loss=0.067340\n",
      "Epoch: 1000: loss=0.148655 valid_loss=0.065754\n",
      "Accuracy = 96.66666666666667%\n",
      "96.66666666666667%, 34/34 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100: loss=1.176625 valid_loss=1.119598\n",
      "Epoch: 200: loss=1.156091 valid_loss=1.108404\n",
      "Epoch: 300: loss=1.001483 valid_loss=0.893944\n",
      "Epoch: 400: loss=0.608928 valid_loss=0.549114\n",
      "Epoch: 500: loss=0.259485 valid_loss=0.203621\n",
      "Epoch: 600: loss=0.254941 valid_loss=0.099135\n",
      "Epoch: 700: loss=0.177955 valid_loss=0.086155\n",
      "Epoch: 800: loss=0.198165 valid_loss=0.068365\n",
      "Epoch: 900: loss=0.339283 valid_loss=0.187567\n",
      "Epoch: 1000: loss=0.131046 valid_loss=0.044045\n",
      "Accuracy = 100.0%\n",
      "100.0%, 36/36 features\n",
      "Epoch: 100: loss=1.157938 valid_loss=1.117591\n",
      "Epoch: 200: loss=0.871410 valid_loss=0.898765\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 300: loss=0.772611 valid_loss=0.556232\n",
      "Epoch: 400: loss=0.538601 valid_loss=0.512279\n",
      "Epoch: 500: loss=0.213435 valid_loss=0.200490\n",
      "Epoch: 600: loss=0.218794 valid_loss=0.164756\n",
      "Epoch: 700: loss=0.202412 valid_loss=0.094011\n",
      "Epoch: 800: loss=0.149444 valid_loss=0.090412\n",
      "Epoch: 900: loss=0.147614 valid_loss=0.103068\n",
      "Epoch: 1000: loss=0.153388 valid_loss=0.067574\n",
      "Accuracy = 100.0%\n",
      "100.0%, 38/38 features\n",
      "Epoch: 100: loss=1.155798 valid_loss=1.077180\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 200: loss=0.626665 valid_loss=0.596707\n",
      "Epoch: 300: loss=0.380425 valid_loss=0.286258\n",
      "Epoch: 400: loss=0.236058 valid_loss=0.151198\n",
      "Epoch: 500: loss=0.199511 valid_loss=0.081203\n",
      "Epoch: 600: loss=0.175946 valid_loss=0.061195\n",
      "Epoch: 700: loss=0.147242 valid_loss=0.041869\n",
      "Epoch: 800: loss=0.139366 valid_loss=0.035082\n",
      "Epoch: 900: loss=0.133648 valid_loss=0.031098\n",
      "Epoch: 1000: loss=0.131206 valid_loss=0.032078\n",
      "Accuracy = 100.0%\n",
      "100.0%, 5/8 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100: loss=1.170919 valid_loss=1.118602\n",
      "Epoch: 200: loss=1.128275 valid_loss=1.083337\n",
      "Epoch: 300: loss=0.678708 valid_loss=0.689971\n",
      "Epoch: 400: loss=0.514694 valid_loss=0.468947\n",
      "Epoch: 500: loss=0.284133 valid_loss=0.239305\n",
      "Epoch: 600: loss=0.255193 valid_loss=0.169375\n",
      "Epoch: 700: loss=0.214255 valid_loss=0.114507\n",
      "Epoch: 800: loss=0.174092 valid_loss=0.094880\n",
      "Epoch: 900: loss=0.162875 valid_loss=0.082835\n",
      "Epoch: 1000: loss=0.159418 valid_loss=0.074926\n",
      "Accuracy = 96.66666666666667%\n",
      "96.66666666666667%, 6/10 features\n",
      "Epoch: 100: loss=1.154752 valid_loss=1.099419\n",
      "Epoch: 200: loss=0.894300 valid_loss=0.842219\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 300: loss=0.432703 valid_loss=0.482066\n",
      "Epoch: 400: loss=0.290658 valid_loss=0.394762\n",
      "Epoch: 500: loss=0.247825 valid_loss=0.244638\n",
      "Epoch: 600: loss=0.180476 valid_loss=0.101061\n",
      "Epoch: 700: loss=0.201297 valid_loss=0.083218\n",
      "Epoch: 800: loss=0.163521 valid_loss=0.063303\n",
      "Epoch: 900: loss=0.156498 valid_loss=0.065135\n",
      "Epoch: 1000: loss=0.289065 valid_loss=0.237962\n",
      "Accuracy = 83.33333333333333%\n",
      "83.33333333333333%, 12/12 features\n",
      "Epoch: 100: loss=1.155702 valid_loss=1.102076\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 200: loss=0.809896 valid_loss=0.813112\n",
      "Epoch: 300: loss=0.434604 valid_loss=0.405519\n",
      "Epoch: 400: loss=0.312101 valid_loss=0.314709\n",
      "Epoch: 500: loss=0.227199 valid_loss=0.185899\n",
      "Epoch: 600: loss=0.201097 valid_loss=0.116284\n",
      "Epoch: 700: loss=0.170134 valid_loss=0.096731\n",
      "Epoch: 800: loss=0.163444 valid_loss=0.077664\n",
      "Epoch: 900: loss=0.157780 valid_loss=0.069728\n",
      "Epoch: 1000: loss=0.157062 valid_loss=0.065439\n",
      "Accuracy = 100.0%\n",
      "100.0%, 14/14 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100: loss=1.172916 valid_loss=1.115698\n",
      "Epoch: 200: loss=1.127623 valid_loss=1.069232\n",
      "Epoch: 300: loss=0.628992 valid_loss=0.628163\n",
      "Epoch: 400: loss=0.366201 valid_loss=0.327816\n",
      "Epoch: 500: loss=0.266603 valid_loss=0.227222\n",
      "Epoch: 600: loss=0.285972 valid_loss=0.152259\n",
      "Epoch: 700: loss=0.201961 valid_loss=0.185449\n",
      "Epoch: 800: loss=0.177710 valid_loss=0.176309\n",
      "Epoch: 900: loss=0.168310 valid_loss=0.098708\n",
      "Epoch: 1000: loss=0.176357 valid_loss=0.095388\n",
      "Accuracy = 100.0%\n",
      "100.0%, 16/16 features\n",
      "Epoch: 100: loss=1.175164 valid_loss=1.117345\n",
      "Epoch: 200: loss=1.143646 valid_loss=1.095926\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 300: loss=0.646224 valid_loss=0.757652\n",
      "Epoch: 400: loss=0.527812 valid_loss=0.325373\n",
      "Epoch: 500: loss=0.281283 valid_loss=0.257738\n",
      "Epoch: 600: loss=0.194473 valid_loss=0.120889\n",
      "Epoch: 700: loss=0.204492 valid_loss=0.103368\n",
      "Epoch: 800: loss=0.243612 valid_loss=0.087767\n",
      "Epoch: 900: loss=0.190833 valid_loss=0.161281\n",
      "Epoch: 1000: loss=0.141079 valid_loss=0.088870\n",
      "Accuracy = 96.66666666666667%\n",
      "96.66666666666667%, 19/19 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100: loss=1.111741 valid_loss=1.068678\n",
      "Epoch: 200: loss=0.620881 valid_loss=0.620620\n",
      "Epoch: 300: loss=0.540084 valid_loss=0.500810\n",
      "Epoch: 400: loss=0.255303 valid_loss=0.183959\n",
      "Epoch: 500: loss=0.221983 valid_loss=0.153187\n",
      "Epoch: 600: loss=0.191982 valid_loss=0.119637\n",
      "Epoch: 700: loss=0.200073 valid_loss=0.117804\n",
      "Epoch: 800: loss=0.145038 valid_loss=0.079723\n",
      "Epoch: 900: loss=0.226068 valid_loss=0.119607\n",
      "Epoch: 1000: loss=0.135192 valid_loss=0.058717\n",
      "Accuracy = 96.66666666666667%\n",
      "96.66666666666667%, 21/21 features\n",
      "Epoch: 100: loss=1.167295 valid_loss=1.119829\n",
      "Epoch: 200: loss=1.040520 valid_loss=1.023070\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 300: loss=0.555422 valid_loss=0.515472\n",
      "Epoch: 400: loss=0.418843 valid_loss=0.368818\n",
      "Epoch: 500: loss=0.243921 valid_loss=0.193199\n",
      "Epoch: 600: loss=0.196791 valid_loss=0.190172\n",
      "Epoch: 700: loss=0.173174 valid_loss=0.111239\n",
      "Epoch: 800: loss=0.183612 valid_loss=0.158217\n",
      "Epoch: 900: loss=0.180454 valid_loss=0.083933\n",
      "Epoch: 1000: loss=0.197676 valid_loss=0.068150\n",
      "Accuracy = 100.0%\n",
      "100.0%, 23/23 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100: loss=1.168936 valid_loss=1.109411\n",
      "Epoch: 200: loss=0.949435 valid_loss=0.961112\n",
      "Epoch: 300: loss=0.596508 valid_loss=0.351698\n",
      "Epoch: 400: loss=0.279351 valid_loss=0.214689\n",
      "Epoch: 500: loss=0.211274 valid_loss=0.133369\n",
      "Epoch: 600: loss=0.170179 valid_loss=0.153439\n",
      "Epoch: 700: loss=0.202155 valid_loss=0.097696\n",
      "Epoch: 800: loss=0.165818 valid_loss=0.098272\n",
      "Epoch: 900: loss=0.157240 valid_loss=0.064770\n",
      "Epoch: 1000: loss=0.201155 valid_loss=0.101874\n",
      "Accuracy = 93.33333333333333%\n",
      "93.33333333333333%, 25/25 features\n",
      "Epoch: 100: loss=1.178831 valid_loss=1.116351\n",
      "Epoch: 200: loss=1.102180 valid_loss=1.064675\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 300: loss=0.528866 valid_loss=0.484498\n",
      "Epoch: 400: loss=0.326534 valid_loss=0.311135\n",
      "Epoch: 500: loss=0.223439 valid_loss=0.166147\n",
      "Epoch: 600: loss=0.214904 valid_loss=0.117540\n",
      "Epoch: 700: loss=0.208710 valid_loss=0.104948\n",
      "Epoch: 800: loss=0.209931 valid_loss=0.123238\n",
      "Epoch: 900: loss=0.152454 valid_loss=0.060848\n",
      "Epoch: 1000: loss=0.146720 valid_loss=0.072423\n",
      "Accuracy = 93.33333333333333%\n",
      "93.33333333333333%, 27/27 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100: loss=1.150423 valid_loss=1.103983\n",
      "Epoch: 200: loss=0.977069 valid_loss=0.916176\n",
      "Epoch: 300: loss=0.724662 valid_loss=0.621752\n",
      "Epoch: 400: loss=0.390476 valid_loss=0.317910\n",
      "Epoch: 500: loss=0.237164 valid_loss=0.154257\n",
      "Epoch: 600: loss=0.210018 valid_loss=0.105974\n",
      "Epoch: 700: loss=0.160466 valid_loss=0.072908\n",
      "Epoch: 800: loss=0.172242 valid_loss=0.056864\n",
      "Epoch: 900: loss=0.160423 valid_loss=0.051766\n",
      "Epoch: 1000: loss=0.143846 valid_loss=0.045518\n",
      "Accuracy = 100.0%\n",
      "100.0%, 30/30 features\n",
      "Epoch: 100: loss=1.175759 valid_loss=1.115436\n",
      "Epoch: 200: loss=1.121286 valid_loss=1.070904\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 300: loss=0.575575 valid_loss=0.581539\n",
      "Epoch: 400: loss=0.354264 valid_loss=0.326972\n",
      "Epoch: 500: loss=0.251974 valid_loss=0.214179\n",
      "Epoch: 600: loss=0.190821 valid_loss=0.131466\n",
      "Epoch: 700: loss=0.180150 valid_loss=0.100585\n",
      "Epoch: 800: loss=0.468320 valid_loss=0.105448\n",
      "Epoch: 900: loss=0.148794 valid_loss=0.080568\n",
      "Epoch: 1000: loss=0.210477 valid_loss=0.078220\n",
      "Accuracy = 100.0%\n",
      "100.0%, 32/32 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100: loss=1.133748 valid_loss=1.092751\n",
      "Epoch: 200: loss=0.920839 valid_loss=0.746179\n",
      "Epoch: 300: loss=0.409861 valid_loss=0.321662\n",
      "Epoch: 400: loss=0.249781 valid_loss=0.261538\n",
      "Epoch: 500: loss=0.234575 valid_loss=0.155973\n",
      "Epoch: 600: loss=0.184981 valid_loss=0.156889\n",
      "Epoch: 700: loss=0.203409 valid_loss=0.077499\n",
      "Epoch: 800: loss=0.197209 valid_loss=0.117682\n",
      "Epoch: 900: loss=0.462363 valid_loss=0.117159\n",
      "Epoch: 1000: loss=0.159706 valid_loss=0.072279\n",
      "Accuracy = 96.66666666666667%\n",
      "96.66666666666667%, 34/34 features\n",
      "Epoch: 100: loss=1.178563 valid_loss=1.121212\n",
      "Epoch: 200: loss=1.169752 valid_loss=1.116745\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 300: loss=1.096519 valid_loss=1.077808\n",
      "Epoch: 400: loss=0.659820 valid_loss=0.654174\n",
      "Epoch: 500: loss=0.311643 valid_loss=0.279349\n",
      "Epoch: 600: loss=0.221461 valid_loss=0.153515\n",
      "Epoch: 700: loss=0.184576 valid_loss=0.119503\n",
      "Epoch: 800: loss=0.160008 valid_loss=0.139332\n",
      "Epoch: 900: loss=0.148938 valid_loss=0.125617\n",
      "Epoch: 1000: loss=0.132100 valid_loss=0.066212\n",
      "Accuracy = 100.0%\n",
      "100.0%, 36/36 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100: loss=1.173319 valid_loss=1.112012\n",
      "Epoch: 200: loss=1.114748 valid_loss=1.047597\n",
      "Epoch: 300: loss=0.618648 valid_loss=0.561935\n",
      "Epoch: 400: loss=0.535785 valid_loss=0.374968\n",
      "Epoch: 500: loss=0.224481 valid_loss=0.193584\n",
      "Epoch: 600: loss=0.235806 valid_loss=0.182835\n",
      "Epoch: 700: loss=0.173830 valid_loss=0.088734\n",
      "Epoch: 800: loss=0.164977 valid_loss=0.054893\n",
      "Epoch: 900: loss=0.171444 valid_loss=0.069163\n",
      "Epoch: 1000: loss=0.197741 valid_loss=0.049732\n",
      "Accuracy = 100.0%\n",
      "100.0%, 38/38 features\n",
      "Epoch: 100: loss=1.156661 valid_loss=1.102123\n",
      "Epoch: 200: loss=0.717753 valid_loss=0.720493\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 300: loss=0.380097 valid_loss=0.369952\n",
      "Epoch: 400: loss=0.238353 valid_loss=0.158957\n",
      "Epoch: 500: loss=0.185934 valid_loss=0.087875\n",
      "Epoch: 600: loss=0.165506 valid_loss=0.064947\n",
      "Epoch: 700: loss=0.152668 valid_loss=0.049294\n",
      "Epoch: 800: loss=0.141896 valid_loss=0.040070\n",
      "Epoch: 900: loss=0.149691 valid_loss=0.043038\n",
      "Epoch: 1000: loss=0.130417 valid_loss=0.035230\n",
      "Accuracy = 96.66666666666667%\n",
      "96.66666666666667%, 5/8 features\n",
      "Epoch: 100: loss=1.171834 valid_loss=1.109001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 200: loss=0.946983 valid_loss=0.943580\n",
      "Epoch: 300: loss=0.537101 valid_loss=0.661629\n",
      "Epoch: 400: loss=0.394717 valid_loss=0.645782\n",
      "Epoch: 500: loss=0.280509 valid_loss=0.229874\n",
      "Epoch: 600: loss=0.233065 valid_loss=0.168315\n",
      "Epoch: 700: loss=0.196382 valid_loss=0.137188\n",
      "Epoch: 800: loss=0.189409 valid_loss=0.118198\n",
      "Epoch: 900: loss=0.162008 valid_loss=0.105885\n",
      "Epoch: 1000: loss=0.163286 valid_loss=0.096420\n",
      "Accuracy = 100.0%\n",
      "100.0%, 6/10 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100: loss=1.172078 valid_loss=1.111642\n",
      "Epoch: 200: loss=0.958119 valid_loss=0.946740\n",
      "Epoch: 300: loss=0.460886 valid_loss=0.519025\n",
      "Epoch: 400: loss=0.304855 valid_loss=0.265718\n",
      "Epoch: 500: loss=0.220820 valid_loss=0.156884\n",
      "Epoch: 600: loss=0.187643 valid_loss=0.104408\n",
      "Epoch: 700: loss=0.171793 valid_loss=0.086915\n",
      "Epoch: 800: loss=0.158931 valid_loss=0.076131\n",
      "Epoch: 900: loss=0.154897 valid_loss=0.069858\n",
      "Epoch: 1000: loss=0.149343 valid_loss=0.072126\n",
      "Accuracy = 96.66666666666667%\n",
      "96.66666666666667%, 11/12 features\n",
      "Epoch: 100: loss=1.175903 valid_loss=1.118286\n",
      "Epoch: 200: loss=1.140754 valid_loss=1.104624\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 300: loss=0.861436 valid_loss=0.796978\n",
      "Epoch: 400: loss=0.439938 valid_loss=0.431614\n",
      "Epoch: 500: loss=0.334906 valid_loss=0.219193\n",
      "Epoch: 600: loss=0.218216 valid_loss=0.151060\n",
      "Epoch: 700: loss=0.191894 valid_loss=0.114881\n",
      "Epoch: 800: loss=0.231776 valid_loss=0.123350\n",
      "Epoch: 900: loss=0.176743 valid_loss=0.108925\n",
      "Epoch: 1000: loss=0.140922 valid_loss=0.067793\n",
      "Accuracy = 96.66666666666667%\n",
      "96.66666666666667%, 13/14 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100: loss=1.173945 valid_loss=1.118814\n",
      "Epoch: 200: loss=1.115992 valid_loss=1.089341\n",
      "Epoch: 300: loss=0.792251 valid_loss=0.731933\n",
      "Epoch: 400: loss=0.491708 valid_loss=0.497940\n",
      "Epoch: 500: loss=0.367699 valid_loss=0.326579\n",
      "Epoch: 600: loss=0.264735 valid_loss=0.236861\n",
      "Epoch: 700: loss=0.237146 valid_loss=0.152885\n",
      "Epoch: 800: loss=0.186374 valid_loss=0.111228\n",
      "Epoch: 900: loss=0.161576 valid_loss=0.096334\n",
      "Epoch: 1000: loss=0.170393 valid_loss=0.103488\n",
      "Accuracy = 100.0%\n",
      "100.0%, 16/16 features\n",
      "Epoch: 100: loss=1.151380 valid_loss=1.038268\n",
      "Epoch: 200: loss=0.806154 valid_loss=0.507234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 300: loss=0.274649 valid_loss=0.196811\n",
      "Epoch: 400: loss=0.349750 valid_loss=0.511156\n",
      "Epoch: 500: loss=0.182258 valid_loss=0.093590\n",
      "Epoch: 600: loss=0.184625 valid_loss=0.092422\n",
      "Epoch: 700: loss=0.146606 valid_loss=0.075141\n",
      "Epoch: 800: loss=0.145424 valid_loss=0.098625\n",
      "Epoch: 900: loss=0.162423 valid_loss=0.064102\n",
      "Epoch: 1000: loss=0.129399 valid_loss=0.066707\n",
      "Accuracy = 96.66666666666667%\n",
      "96.66666666666667%, 19/19 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100: loss=1.173427 valid_loss=1.108269\n",
      "Epoch: 200: loss=1.010252 valid_loss=0.893976\n",
      "Epoch: 300: loss=0.496130 valid_loss=0.407696\n",
      "Epoch: 400: loss=0.420934 valid_loss=0.361429\n",
      "Epoch: 500: loss=0.269116 valid_loss=0.174397\n",
      "Epoch: 600: loss=0.208872 valid_loss=0.107616\n",
      "Epoch: 700: loss=0.195898 valid_loss=0.119492\n",
      "Epoch: 800: loss=0.227460 valid_loss=0.084817\n",
      "Epoch: 900: loss=0.161088 valid_loss=0.073484\n",
      "Epoch: 1000: loss=0.149097 valid_loss=0.045984\n",
      "Accuracy = 100.0%\n",
      "100.0%, 21/21 features\n",
      "Epoch: 100: loss=1.175478 valid_loss=1.116547\n",
      "Epoch: 200: loss=1.159062 valid_loss=1.094490\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 300: loss=0.788139 valid_loss=0.702376\n",
      "Epoch: 400: loss=0.363694 valid_loss=0.296236\n",
      "Epoch: 500: loss=0.225843 valid_loss=0.235999\n",
      "Epoch: 600: loss=0.225556 valid_loss=0.183561\n",
      "Epoch: 700: loss=0.256018 valid_loss=0.148696\n",
      "Epoch: 800: loss=0.153829 valid_loss=0.071908\n",
      "Epoch: 900: loss=0.143729 valid_loss=0.058715\n",
      "Epoch: 1000: loss=0.153446 valid_loss=0.069233\n",
      "Accuracy = 96.66666666666667%\n",
      "96.66666666666667%, 23/23 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100: loss=1.158837 valid_loss=1.102251\n",
      "Epoch: 200: loss=0.820101 valid_loss=0.800102\n",
      "Epoch: 300: loss=0.535873 valid_loss=0.563189\n",
      "Epoch: 400: loss=0.392018 valid_loss=0.387656\n",
      "Epoch: 500: loss=0.320067 valid_loss=0.329156\n",
      "Epoch: 600: loss=0.242466 valid_loss=0.143809\n",
      "Epoch: 700: loss=0.220985 valid_loss=0.123583\n",
      "Epoch: 800: loss=0.222540 valid_loss=0.097513\n",
      "Epoch: 900: loss=0.152362 valid_loss=0.081583\n",
      "Epoch: 1000: loss=0.180849 valid_loss=0.073780\n",
      "Accuracy = 96.66666666666667%\n",
      "96.66666666666667%, 25/25 features\n",
      "Epoch: 100: loss=1.177298 valid_loss=1.121768\n",
      "Epoch: 200: loss=1.156210 valid_loss=1.112478\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 300: loss=0.986723 valid_loss=0.952080\n",
      "Epoch: 400: loss=0.530261 valid_loss=0.364168\n",
      "Epoch: 500: loss=0.295839 valid_loss=0.209687\n",
      "Epoch: 600: loss=0.262462 valid_loss=0.160117\n",
      "Epoch: 700: loss=0.238149 valid_loss=0.106906\n",
      "Epoch: 800: loss=0.170573 valid_loss=0.100023\n",
      "Epoch: 900: loss=0.221082 valid_loss=0.090287\n",
      "Epoch: 1000: loss=0.178051 valid_loss=0.151591\n",
      "Accuracy = 93.33333333333333%\n",
      "93.33333333333333%, 27/27 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100: loss=1.162887 valid_loss=1.113845\n",
      "Epoch: 200: loss=0.970345 valid_loss=0.990311\n",
      "Epoch: 300: loss=0.668548 valid_loss=0.646187\n",
      "Epoch: 400: loss=0.368126 valid_loss=0.370912\n",
      "Epoch: 500: loss=0.318238 valid_loss=0.201771\n",
      "Epoch: 600: loss=0.302359 valid_loss=0.190076\n",
      "Epoch: 700: loss=0.188931 valid_loss=0.132694\n",
      "Epoch: 800: loss=0.188442 valid_loss=0.102936\n",
      "Epoch: 900: loss=0.163169 valid_loss=0.090878\n",
      "Epoch: 1000: loss=0.159177 valid_loss=0.088238\n",
      "Accuracy = 100.0%\n",
      "100.0%, 30/30 features\n",
      "Epoch: 100: loss=1.155893 valid_loss=1.111026\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 200: loss=0.861386 valid_loss=0.829025\n",
      "Epoch: 300: loss=0.385127 valid_loss=0.391946\n",
      "Epoch: 400: loss=0.348830 valid_loss=0.258134\n",
      "Epoch: 500: loss=0.196407 valid_loss=0.128100\n",
      "Epoch: 600: loss=0.195747 valid_loss=0.132675\n",
      "Epoch: 700: loss=0.237448 valid_loss=0.127234\n",
      "Epoch: 800: loss=0.146471 valid_loss=0.064496\n",
      "Epoch: 900: loss=0.131083 valid_loss=0.053024\n",
      "Epoch: 1000: loss=0.194153 valid_loss=0.043460\n",
      "Accuracy = 100.0%\n",
      "100.0%, 32/32 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100: loss=1.146258 valid_loss=1.110140\n",
      "Epoch: 200: loss=0.830068 valid_loss=0.833943\n",
      "Epoch: 300: loss=0.444983 valid_loss=0.452097\n",
      "Epoch: 400: loss=0.277765 valid_loss=0.267241\n",
      "Epoch: 500: loss=0.245808 valid_loss=0.201657\n",
      "Epoch: 600: loss=0.188771 valid_loss=0.218006\n",
      "Epoch: 700: loss=0.181506 valid_loss=0.157371\n",
      "Epoch: 800: loss=0.155367 valid_loss=0.144491\n",
      "Epoch: 900: loss=0.154998 valid_loss=0.146917\n",
      "Epoch: 1000: loss=0.158699 valid_loss=0.151467\n",
      "Accuracy = 90.0%\n",
      "90.0%, 34/34 features\n",
      "Epoch: 100: loss=1.131627 valid_loss=1.074313\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 200: loss=0.673956 valid_loss=0.544133\n",
      "Epoch: 300: loss=0.379055 valid_loss=0.434023\n",
      "Epoch: 400: loss=0.259251 valid_loss=0.299727\n",
      "Epoch: 500: loss=0.170677 valid_loss=0.137195\n",
      "Epoch: 600: loss=0.162855 valid_loss=0.112950\n",
      "Epoch: 700: loss=0.195406 valid_loss=0.086154\n",
      "Epoch: 800: loss=0.128615 valid_loss=0.058769\n",
      "Epoch: 900: loss=0.131465 valid_loss=0.059292\n",
      "Epoch: 1000: loss=0.127898 valid_loss=0.057829\n",
      "Accuracy = 100.0%\n",
      "100.0%, 36/36 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100: loss=1.169200 valid_loss=1.113927\n",
      "Epoch: 200: loss=1.038036 valid_loss=1.007386\n",
      "Epoch: 300: loss=0.497895 valid_loss=0.491182\n",
      "Epoch: 400: loss=0.305726 valid_loss=0.450753\n",
      "Epoch: 500: loss=0.260472 valid_loss=0.189392\n",
      "Epoch: 600: loss=0.202430 valid_loss=0.130862\n",
      "Epoch: 700: loss=0.186393 valid_loss=0.123997\n",
      "Epoch: 800: loss=0.165508 valid_loss=0.083365\n",
      "Epoch: 900: loss=0.148202 valid_loss=0.071402\n",
      "Epoch: 1000: loss=0.150006 valid_loss=0.090377\n",
      "Accuracy = 96.66666666666667%\n",
      "96.66666666666667%, 38/38 features\n",
      "Epoch: 100: loss=1.129566 valid_loss=1.079098\n",
      "Epoch: 200: loss=0.772151 valid_loss=0.576054\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 300: loss=0.411824 valid_loss=0.276597\n",
      "Epoch: 400: loss=0.219518 valid_loss=0.113328\n",
      "Epoch: 500: loss=0.406797 valid_loss=0.071394\n",
      "Epoch: 600: loss=0.162105 valid_loss=0.050558\n",
      "Epoch: 700: loss=0.154832 valid_loss=0.045161\n",
      "Epoch: 800: loss=0.145787 valid_loss=0.034744\n",
      "Epoch: 900: loss=0.141222 valid_loss=0.031056\n",
      "Epoch: 1000: loss=0.145092 valid_loss=0.027747\n",
      "Accuracy = 100.0%\n",
      "100.0%, 5/8 features\n",
      "Epoch: 100: loss=1.142182 valid_loss=1.085624\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 200: loss=0.644626 valid_loss=0.659481\n",
      "Epoch: 300: loss=0.394006 valid_loss=0.332238\n",
      "Epoch: 400: loss=0.322205 valid_loss=0.205662\n",
      "Epoch: 500: loss=0.197501 valid_loss=0.154735\n",
      "Epoch: 600: loss=0.174837 valid_loss=0.094640\n",
      "Epoch: 700: loss=0.183379 valid_loss=0.083693\n",
      "Epoch: 800: loss=0.156040 valid_loss=0.069962\n",
      "Epoch: 900: loss=0.172672 valid_loss=0.070651\n",
      "Epoch: 1000: loss=0.156214 valid_loss=0.066031\n",
      "Accuracy = 96.66666666666667%\n",
      "96.66666666666667%, 7/10 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100: loss=1.175689 valid_loss=1.080812\n",
      "Epoch: 200: loss=0.699834 valid_loss=0.629894\n",
      "Epoch: 300: loss=0.351387 valid_loss=0.343572\n",
      "Epoch: 400: loss=0.262974 valid_loss=0.192028\n",
      "Epoch: 500: loss=0.278971 valid_loss=0.206376\n",
      "Epoch: 600: loss=0.170562 valid_loss=0.083532\n",
      "Epoch: 700: loss=0.161126 valid_loss=0.069321\n",
      "Epoch: 800: loss=0.292615 valid_loss=0.137348\n",
      "Epoch: 900: loss=0.152128 valid_loss=0.054150\n",
      "Epoch: 1000: loss=0.137319 valid_loss=0.044491\n",
      "Accuracy = 100.0%\n",
      "100.0%, 11/12 features\n",
      "Epoch: 100: loss=1.169773 valid_loss=1.112282\n",
      "Epoch: 200: loss=1.056988 valid_loss=1.009485\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 300: loss=0.482529 valid_loss=0.458936\n",
      "Epoch: 400: loss=0.352564 valid_loss=0.315358\n",
      "Epoch: 500: loss=0.512219 valid_loss=0.782363\n",
      "Epoch: 600: loss=0.219845 valid_loss=0.168028\n",
      "Epoch: 700: loss=0.203987 valid_loss=0.091438\n",
      "Epoch: 800: loss=0.168262 valid_loss=0.072997\n",
      "Epoch: 900: loss=0.152036 valid_loss=0.061872\n",
      "Epoch: 1000: loss=0.197584 valid_loss=0.080022\n",
      "Accuracy = 96.66666666666667%\n",
      "96.66666666666667%, 14/14 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100: loss=1.160978 valid_loss=1.095844\n",
      "Epoch: 200: loss=0.771791 valid_loss=0.751201\n",
      "Epoch: 300: loss=0.553564 valid_loss=0.482005\n",
      "Epoch: 400: loss=0.325596 valid_loss=0.296368\n",
      "Epoch: 500: loss=0.241157 valid_loss=0.181360\n",
      "Epoch: 600: loss=0.190620 valid_loss=0.139523\n",
      "Epoch: 700: loss=0.177477 valid_loss=0.097070\n",
      "Epoch: 800: loss=0.183386 valid_loss=0.081896\n",
      "Epoch: 900: loss=0.192680 valid_loss=0.162894\n",
      "Epoch: 1000: loss=0.203001 valid_loss=0.299906\n",
      "Accuracy = 76.66666666666667%\n",
      "76.66666666666667%, 16/16 features\n",
      "Epoch: 100: loss=1.157784 valid_loss=1.103892\n",
      "Epoch: 200: loss=0.894415 valid_loss=0.826664\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 300: loss=0.529210 valid_loss=0.507775\n",
      "Epoch: 400: loss=0.436542 valid_loss=0.356725\n",
      "Epoch: 500: loss=0.293739 valid_loss=0.273828\n",
      "Epoch: 600: loss=0.346353 valid_loss=0.149810\n",
      "Epoch: 700: loss=0.200643 valid_loss=0.158841\n",
      "Epoch: 800: loss=0.192110 valid_loss=0.082029\n",
      "Epoch: 900: loss=0.155048 valid_loss=0.072644\n",
      "Epoch: 1000: loss=0.147602 valid_loss=0.061834\n",
      "Accuracy = 100.0%\n",
      "100.0%, 19/19 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100: loss=1.102866 valid_loss=1.051698\n",
      "Epoch: 200: loss=0.636877 valid_loss=0.719552\n",
      "Epoch: 300: loss=0.486496 valid_loss=0.421981\n",
      "Epoch: 400: loss=0.252202 valid_loss=0.163263\n",
      "Epoch: 500: loss=0.470414 valid_loss=0.115405\n",
      "Epoch: 600: loss=0.195751 valid_loss=0.090978\n",
      "Epoch: 700: loss=0.172132 valid_loss=0.080754\n",
      "Epoch: 800: loss=0.162609 valid_loss=0.063407\n",
      "Epoch: 900: loss=0.151858 valid_loss=0.082657\n",
      "Epoch: 1000: loss=0.158191 valid_loss=0.080117\n",
      "Accuracy = 96.66666666666667%\n",
      "96.66666666666667%, 21/21 features\n",
      "Epoch: 100: loss=1.175727 valid_loss=1.116209\n",
      "Epoch: 200: loss=1.114598 valid_loss=1.074540\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 300: loss=0.620913 valid_loss=0.585142\n",
      "Epoch: 400: loss=0.294064 valid_loss=0.339255\n",
      "Epoch: 500: loss=0.249191 valid_loss=0.147508\n",
      "Epoch: 600: loss=0.178157 valid_loss=0.107072\n",
      "Epoch: 700: loss=0.167284 valid_loss=0.095422\n",
      "Epoch: 800: loss=0.146032 valid_loss=0.078381\n",
      "Epoch: 900: loss=0.141360 valid_loss=0.085027\n",
      "Epoch: 1000: loss=0.228934 valid_loss=0.088492\n",
      "Accuracy = 93.33333333333333%\n",
      "93.33333333333333%, 23/23 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100: loss=1.153684 valid_loss=1.102353\n",
      "Epoch: 200: loss=0.800334 valid_loss=0.763921\n",
      "Epoch: 300: loss=0.639827 valid_loss=0.342821\n",
      "Epoch: 400: loss=0.347504 valid_loss=0.324438\n",
      "Epoch: 500: loss=0.304516 valid_loss=0.141153\n",
      "Epoch: 600: loss=0.184749 valid_loss=0.125819\n",
      "Epoch: 700: loss=0.158000 valid_loss=0.101080\n",
      "Epoch: 800: loss=0.256163 valid_loss=0.088390\n",
      "Epoch: 900: loss=0.158076 valid_loss=0.069575\n",
      "Epoch: 1000: loss=0.148458 valid_loss=0.056143\n",
      "Accuracy = 96.66666666666667%\n",
      "96.66666666666667%, 25/25 features\n",
      "Epoch: 100: loss=1.175731 valid_loss=1.120444\n",
      "Epoch: 200: loss=1.160295 valid_loss=1.113140\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 300: loss=1.068426 valid_loss=0.977677\n",
      "Epoch: 400: loss=0.448470 valid_loss=0.383723\n",
      "Epoch: 500: loss=0.289142 valid_loss=0.483093\n",
      "Epoch: 600: loss=0.197604 valid_loss=0.159717\n",
      "Epoch: 700: loss=0.191739 valid_loss=0.105574\n",
      "Epoch: 800: loss=0.157496 valid_loss=0.107555\n",
      "Epoch: 900: loss=0.178027 valid_loss=0.126985\n",
      "Epoch: 1000: loss=0.129726 valid_loss=0.107709\n",
      "Accuracy = 96.66666666666667%\n",
      "96.66666666666667%, 27/27 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100: loss=1.175022 valid_loss=1.115053\n",
      "Epoch: 200: loss=1.143995 valid_loss=1.081182\n",
      "Epoch: 300: loss=0.770216 valid_loss=0.754130\n",
      "Epoch: 400: loss=0.398122 valid_loss=0.419580\n",
      "Epoch: 500: loss=0.270595 valid_loss=0.284878\n",
      "Epoch: 600: loss=0.227647 valid_loss=0.169216\n",
      "Epoch: 700: loss=0.169891 valid_loss=0.112479\n",
      "Epoch: 800: loss=0.185045 valid_loss=0.077406\n",
      "Epoch: 900: loss=0.154202 valid_loss=0.052864\n",
      "Epoch: 1000: loss=0.136802 valid_loss=0.081960\n",
      "Accuracy = 96.66666666666667%\n",
      "96.66666666666667%, 30/30 features\n",
      "Epoch: 100: loss=1.175954 valid_loss=1.117773\n",
      "Epoch: 200: loss=1.065915 valid_loss=1.032059\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 300: loss=0.465768 valid_loss=0.415972\n",
      "Epoch: 400: loss=0.370588 valid_loss=0.223394\n",
      "Epoch: 500: loss=0.218753 valid_loss=0.159542\n",
      "Epoch: 600: loss=0.238236 valid_loss=0.129417\n",
      "Epoch: 700: loss=0.171184 valid_loss=0.094789\n",
      "Epoch: 800: loss=0.177989 valid_loss=0.073154\n",
      "Epoch: 900: loss=0.150245 valid_loss=0.063897\n",
      "Epoch: 1000: loss=0.131501 valid_loss=0.049321\n",
      "Accuracy = 100.0%\n",
      "100.0%, 32/32 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100: loss=1.167502 valid_loss=1.116459\n",
      "Epoch: 200: loss=0.954813 valid_loss=0.989411\n",
      "Epoch: 300: loss=0.507348 valid_loss=0.419390\n",
      "Epoch: 400: loss=0.275732 valid_loss=0.303169\n",
      "Epoch: 500: loss=0.252789 valid_loss=0.160205\n",
      "Epoch: 600: loss=0.172425 valid_loss=0.133702\n",
      "Epoch: 700: loss=0.182281 valid_loss=0.111246\n",
      "Epoch: 800: loss=0.157199 valid_loss=0.118977\n",
      "Epoch: 900: loss=0.158902 valid_loss=0.089516\n",
      "Epoch: 1000: loss=0.145513 valid_loss=0.091340\n",
      "Accuracy = 100.0%\n",
      "100.0%, 34/34 features\n",
      "Epoch: 100: loss=1.172757 valid_loss=1.120193\n",
      "Epoch: 200: loss=1.088161 valid_loss=1.058596\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 300: loss=0.590332 valid_loss=0.653580\n",
      "Epoch: 400: loss=0.302057 valid_loss=0.322731\n",
      "Epoch: 500: loss=0.229636 valid_loss=0.169188\n",
      "Epoch: 600: loss=0.337084 valid_loss=0.146254\n",
      "Epoch: 700: loss=0.190109 valid_loss=0.113281\n",
      "Epoch: 800: loss=0.151194 valid_loss=0.077455\n",
      "Epoch: 900: loss=0.158823 valid_loss=0.076581\n",
      "Epoch: 1000: loss=0.142979 valid_loss=0.063540\n",
      "Accuracy = 96.66666666666667%\n",
      "96.66666666666667%, 36/36 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100: loss=1.173420 valid_loss=1.114992\n",
      "Epoch: 200: loss=1.115086 valid_loss=1.069154\n",
      "Epoch: 300: loss=0.574245 valid_loss=0.582263\n",
      "Epoch: 400: loss=0.305900 valid_loss=0.338382\n",
      "Epoch: 500: loss=0.246669 valid_loss=0.207433\n",
      "Epoch: 600: loss=0.191973 valid_loss=0.186960\n",
      "Epoch: 700: loss=0.504895 valid_loss=0.507653\n",
      "Epoch: 800: loss=0.191769 valid_loss=0.117772\n",
      "Epoch: 900: loss=0.148693 valid_loss=0.081721\n",
      "Epoch: 1000: loss=0.136637 valid_loss=0.075805\n",
      "Accuracy = 100.0%\n",
      "100.0%, 38/38 features\n",
      "Epoch: 100: loss=1.125604 valid_loss=1.045964\n",
      "Epoch: 200: loss=0.586152 valid_loss=0.517362\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 300: loss=0.328042 valid_loss=0.272095\n",
      "Epoch: 400: loss=0.338852 valid_loss=0.115103\n",
      "Epoch: 500: loss=0.179249 valid_loss=0.064662\n",
      "Epoch: 600: loss=0.164794 valid_loss=0.049002\n",
      "Epoch: 700: loss=0.162367 valid_loss=0.048496\n",
      "Epoch: 800: loss=0.146502 valid_loss=0.036020\n",
      "Epoch: 900: loss=0.134500 valid_loss=0.032273\n",
      "Epoch: 1000: loss=0.128167 valid_loss=0.030323\n",
      "Accuracy = 100.0%\n",
      "100.0%, 4/8 features\n",
      "Epoch: 100: loss=1.167283 valid_loss=1.108916\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 200: loss=0.896296 valid_loss=0.882923\n",
      "Epoch: 300: loss=0.472567 valid_loss=0.481996\n",
      "Epoch: 400: loss=0.325197 valid_loss=0.268663\n",
      "Epoch: 500: loss=0.246061 valid_loss=0.169125\n",
      "Epoch: 600: loss=0.279241 valid_loss=0.112485\n",
      "Epoch: 700: loss=0.185245 valid_loss=0.099085\n",
      "Epoch: 800: loss=0.162119 valid_loss=0.081674\n",
      "Epoch: 900: loss=0.164005 valid_loss=0.068319\n",
      "Epoch: 1000: loss=0.150471 valid_loss=0.070365\n",
      "Accuracy = 100.0%\n",
      "100.0%, 7/10 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100: loss=1.170859 valid_loss=1.112378\n",
      "Epoch: 200: loss=1.095732 valid_loss=1.050233\n",
      "Epoch: 300: loss=0.610864 valid_loss=0.669519\n",
      "Epoch: 400: loss=0.382695 valid_loss=0.394761\n",
      "Epoch: 500: loss=0.269806 valid_loss=0.280997\n",
      "Epoch: 600: loss=0.235451 valid_loss=0.142313\n",
      "Epoch: 700: loss=0.190323 valid_loss=0.100601\n",
      "Epoch: 800: loss=0.177195 valid_loss=0.083509\n",
      "Epoch: 900: loss=0.157219 valid_loss=0.071370\n",
      "Epoch: 1000: loss=0.160090 valid_loss=0.063898\n",
      "Accuracy = 100.0%\n",
      "100.0%, 10/12 features\n",
      "Epoch: 100: loss=1.177301 valid_loss=1.120784\n",
      "Epoch: 200: loss=1.173127 valid_loss=1.116208\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 300: loss=1.124049 valid_loss=1.077354\n",
      "Epoch: 400: loss=0.608436 valid_loss=0.578732\n",
      "Epoch: 500: loss=0.495128 valid_loss=0.450406\n",
      "Epoch: 600: loss=0.292413 valid_loss=0.197112\n",
      "Epoch: 700: loss=0.208379 valid_loss=0.173531\n",
      "Epoch: 800: loss=0.188660 valid_loss=0.143318\n",
      "Epoch: 900: loss=0.164730 valid_loss=0.095379\n",
      "Epoch: 1000: loss=0.155171 valid_loss=0.082696\n",
      "Accuracy = 96.66666666666667%\n",
      "96.66666666666667%, 14/14 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100: loss=1.146290 valid_loss=1.101207\n",
      "Epoch: 200: loss=0.693039 valid_loss=0.685457\n",
      "Epoch: 300: loss=0.418238 valid_loss=0.345978\n",
      "Epoch: 400: loss=0.260169 valid_loss=0.309266\n",
      "Epoch: 500: loss=0.211991 valid_loss=0.141114\n",
      "Epoch: 600: loss=0.176412 valid_loss=0.097776\n",
      "Epoch: 700: loss=0.202052 valid_loss=0.082775\n",
      "Epoch: 800: loss=0.160355 valid_loss=0.069756\n",
      "Epoch: 900: loss=0.136427 valid_loss=0.076153\n",
      "Epoch: 1000: loss=0.162792 valid_loss=0.058980\n",
      "Accuracy = 100.0%\n",
      "100.0%, 16/16 features\n",
      "Epoch: 100: loss=1.163410 valid_loss=1.112658\n",
      "Epoch: 200: loss=0.978410 valid_loss=0.948642\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 300: loss=0.409451 valid_loss=0.333788\n",
      "Epoch: 400: loss=0.246802 valid_loss=0.152152\n",
      "Epoch: 500: loss=0.216834 valid_loss=0.102069\n",
      "Epoch: 600: loss=0.185651 valid_loss=0.096014\n",
      "Epoch: 700: loss=0.158129 valid_loss=0.077959\n",
      "Epoch: 800: loss=0.171486 valid_loss=0.060030\n",
      "Epoch: 900: loss=0.142887 valid_loss=0.050458\n",
      "Epoch: 1000: loss=0.165005 valid_loss=0.116134\n",
      "Accuracy = 93.33333333333333%\n",
      "93.33333333333333%, 19/19 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100: loss=1.178076 valid_loss=1.112847\n",
      "Epoch: 200: loss=1.020378 valid_loss=1.001573\n",
      "Epoch: 300: loss=0.530219 valid_loss=0.470251\n",
      "Epoch: 400: loss=0.345347 valid_loss=0.337271\n",
      "Epoch: 500: loss=0.392679 valid_loss=0.310467\n",
      "Epoch: 600: loss=0.203478 valid_loss=0.111949\n",
      "Epoch: 700: loss=0.171825 valid_loss=0.087420\n",
      "Epoch: 800: loss=0.163990 valid_loss=0.071631\n",
      "Epoch: 900: loss=0.150968 valid_loss=0.092620\n",
      "Epoch: 1000: loss=0.144995 valid_loss=0.054792\n",
      "Accuracy = 100.0%\n",
      "100.0%, 21/21 features\n",
      "Epoch: 100: loss=1.149758 valid_loss=1.092179\n",
      "Epoch: 200: loss=0.946764 valid_loss=0.770248\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 300: loss=0.604981 valid_loss=0.634020\n",
      "Epoch: 400: loss=0.338814 valid_loss=0.244666\n",
      "Epoch: 500: loss=0.214165 valid_loss=0.180883\n",
      "Epoch: 600: loss=0.195141 valid_loss=0.149372\n",
      "Epoch: 700: loss=0.164542 valid_loss=0.079204\n",
      "Epoch: 800: loss=0.190517 valid_loss=0.067239\n",
      "Epoch: 900: loss=0.128595 valid_loss=0.047244\n",
      "Epoch: 1000: loss=0.162969 valid_loss=0.050888\n",
      "Accuracy = 96.66666666666667%\n",
      "96.66666666666667%, 23/23 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100: loss=1.174030 valid_loss=1.117586\n",
      "Epoch: 200: loss=1.162679 valid_loss=1.102019\n",
      "Epoch: 300: loss=0.862692 valid_loss=0.769956\n",
      "Epoch: 400: loss=0.511068 valid_loss=0.419992\n",
      "Epoch: 500: loss=0.269329 valid_loss=0.179788\n",
      "Epoch: 600: loss=0.322532 valid_loss=0.133185\n",
      "Epoch: 700: loss=0.167092 valid_loss=0.106106\n",
      "Epoch: 800: loss=0.170787 valid_loss=0.128655\n",
      "Epoch: 900: loss=0.148293 valid_loss=0.080457\n",
      "Epoch: 1000: loss=0.149708 valid_loss=0.078063\n",
      "Accuracy = 96.66666666666667%\n",
      "96.66666666666667%, 25/25 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100: loss=1.161355 valid_loss=1.116713\n",
      "Epoch: 200: loss=0.928944 valid_loss=0.982447\n",
      "Epoch: 300: loss=0.449769 valid_loss=0.369403\n",
      "Epoch: 400: loss=0.307751 valid_loss=0.216847\n",
      "Epoch: 500: loss=0.193931 valid_loss=0.174899\n",
      "Epoch: 600: loss=0.198083 valid_loss=0.100683\n",
      "Epoch: 700: loss=0.240715 valid_loss=0.103174\n",
      "Epoch: 800: loss=0.282857 valid_loss=0.126482\n",
      "Epoch: 900: loss=0.172296 valid_loss=0.085137\n",
      "Epoch: 1000: loss=0.128772 valid_loss=0.073460\n",
      "Accuracy = 96.66666666666667%\n",
      "96.66666666666667%, 27/27 features\n",
      "Epoch: 100: loss=1.177492 valid_loss=1.117941\n",
      "Epoch: 200: loss=1.147198 valid_loss=1.102714\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 300: loss=0.782918 valid_loss=0.679359\n",
      "Epoch: 400: loss=0.394548 valid_loss=0.348789\n",
      "Epoch: 500: loss=0.208517 valid_loss=0.164338\n",
      "Epoch: 600: loss=0.184211 valid_loss=0.142752\n",
      "Epoch: 700: loss=0.173974 valid_loss=0.112592\n",
      "Epoch: 800: loss=0.153024 valid_loss=0.101972\n",
      "Epoch: 900: loss=0.186567 valid_loss=0.138518\n",
      "Epoch: 1000: loss=0.144728 valid_loss=0.067332\n",
      "Accuracy = 96.66666666666667%\n",
      "96.66666666666667%, 30/30 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100: loss=1.173205 valid_loss=1.116617\n",
      "Epoch: 200: loss=1.105474 valid_loss=1.069333\n",
      "Epoch: 300: loss=0.760178 valid_loss=0.734295\n",
      "Epoch: 400: loss=0.365167 valid_loss=0.369579\n",
      "Epoch: 500: loss=0.260127 valid_loss=0.222290\n",
      "Epoch: 600: loss=0.194438 valid_loss=0.136732\n",
      "Epoch: 700: loss=0.180314 valid_loss=0.116903\n",
      "Epoch: 800: loss=0.175680 valid_loss=0.096047\n",
      "Epoch: 900: loss=0.206926 valid_loss=0.137387\n",
      "Epoch: 1000: loss=0.188006 valid_loss=0.074375\n",
      "Accuracy = 96.66666666666667%\n",
      "96.66666666666667%, 32/32 features\n",
      "Epoch: 100: loss=1.172357 valid_loss=1.115263\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 200: loss=1.098950 valid_loss=1.055321\n",
      "Epoch: 300: loss=0.670618 valid_loss=0.671140\n",
      "Epoch: 400: loss=0.433709 valid_loss=0.445072\n",
      "Epoch: 500: loss=0.299405 valid_loss=0.231451\n",
      "Epoch: 600: loss=0.360037 valid_loss=0.168563\n",
      "Epoch: 700: loss=0.184102 valid_loss=0.134932\n",
      "Epoch: 800: loss=0.177048 valid_loss=0.319469\n",
      "Epoch: 900: loss=0.170947 valid_loss=0.215137\n",
      "Epoch: 1000: loss=0.286958 valid_loss=0.170909\n",
      "Accuracy = 90.0%\n",
      "90.0%, 34/34 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100: loss=1.163928 valid_loss=1.116268\n",
      "Epoch: 200: loss=1.132281 valid_loss=1.070140\n",
      "Epoch: 300: loss=0.666694 valid_loss=0.542490\n",
      "Epoch: 400: loss=0.346056 valid_loss=0.199496\n",
      "Epoch: 500: loss=0.232032 valid_loss=0.128837\n",
      "Epoch: 600: loss=0.186948 valid_loss=0.096467\n",
      "Epoch: 700: loss=0.205325 valid_loss=0.076876\n",
      "Epoch: 800: loss=0.152790 valid_loss=0.090442\n",
      "Epoch: 900: loss=0.136801 valid_loss=0.062808\n",
      "Epoch: 1000: loss=0.137427 valid_loss=0.048882\n",
      "Accuracy = 100.0%\n",
      "100.0%, 36/36 features\n",
      "Epoch: 100: loss=1.168650 valid_loss=1.079811\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 200: loss=0.645148 valid_loss=0.601215\n",
      "Epoch: 300: loss=0.458492 valid_loss=0.392132\n",
      "Epoch: 400: loss=0.251676 valid_loss=0.173408\n",
      "Epoch: 500: loss=0.217990 valid_loss=0.126597\n",
      "Epoch: 600: loss=0.549260 valid_loss=0.110193\n",
      "Epoch: 700: loss=0.174785 valid_loss=0.084113\n",
      "Epoch: 800: loss=0.140817 valid_loss=0.057949\n",
      "Epoch: 900: loss=0.166050 valid_loss=0.072726\n",
      "Epoch: 1000: loss=0.129601 valid_loss=0.038371\n",
      "Accuracy = 100.0%\n",
      "100.0%, 38/38 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100: loss=1.105042 valid_loss=1.095059\n",
      "Epoch: 200: loss=0.715804 valid_loss=0.731950\n",
      "Epoch: 300: loss=0.512842 valid_loss=0.454989\n",
      "Epoch: 400: loss=0.349577 valid_loss=0.269408\n",
      "Epoch: 500: loss=0.222444 valid_loss=0.174482\n",
      "Epoch: 600: loss=0.235983 valid_loss=0.101794\n",
      "Epoch: 700: loss=0.157962 valid_loss=0.058544\n",
      "Epoch: 800: loss=0.143688 valid_loss=0.045525\n",
      "Epoch: 900: loss=0.136443 valid_loss=0.039219\n",
      "Epoch: 1000: loss=0.143622 valid_loss=0.044317\n",
      "Accuracy = 96.66666666666667%\n",
      "96.66666666666667%, 5/8 features\n",
      "Epoch: 100: loss=1.168734 valid_loss=1.109909\n",
      "Epoch: 200: loss=0.973559 valid_loss=0.984494\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 300: loss=0.615234 valid_loss=0.590840\n",
      "Epoch: 400: loss=0.474292 valid_loss=0.237342\n",
      "Epoch: 500: loss=0.239069 valid_loss=0.174818\n",
      "Epoch: 600: loss=0.241532 valid_loss=0.157851\n",
      "Epoch: 700: loss=0.257514 valid_loss=0.093247\n",
      "Epoch: 800: loss=0.160240 valid_loss=0.080297\n",
      "Epoch: 900: loss=0.148636 valid_loss=0.067599\n",
      "Epoch: 1000: loss=0.167751 valid_loss=0.071551\n",
      "Accuracy = 96.66666666666667%\n",
      "96.66666666666667%, 7/10 features\n",
      "Epoch: 100: loss=1.045959 valid_loss=1.030523\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 200: loss=0.681705 valid_loss=0.637805\n",
      "Epoch: 300: loss=0.610659 valid_loss=0.517948\n",
      "Epoch: 400: loss=0.372204 valid_loss=0.349553\n",
      "Epoch: 500: loss=0.280370 valid_loss=0.248646\n",
      "Epoch: 600: loss=0.226015 valid_loss=0.149236\n",
      "Epoch: 700: loss=0.186842 valid_loss=0.108450\n",
      "Epoch: 800: loss=0.198654 valid_loss=0.087903\n",
      "Epoch: 900: loss=0.167535 valid_loss=0.084708\n",
      "Epoch: 1000: loss=0.154288 valid_loss=0.073737\n",
      "Accuracy = 96.66666666666667%\n",
      "96.66666666666667%, 10/12 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100: loss=1.161584 valid_loss=1.112256\n",
      "Epoch: 200: loss=1.008345 valid_loss=0.985443\n",
      "Epoch: 300: loss=0.477665 valid_loss=0.416461\n",
      "Epoch: 400: loss=0.608050 valid_loss=0.778012\n",
      "Epoch: 500: loss=0.224239 valid_loss=0.126407\n",
      "Epoch: 600: loss=0.213745 valid_loss=0.183018\n",
      "Epoch: 700: loss=0.184497 valid_loss=0.083725\n",
      "Epoch: 800: loss=0.170251 valid_loss=0.075599\n",
      "Epoch: 900: loss=0.160973 valid_loss=0.063293\n",
      "Epoch: 1000: loss=0.148708 valid_loss=0.061014\n",
      "Accuracy = 96.66666666666667%\n",
      "96.66666666666667%, 14/14 features\n",
      "Epoch: 100: loss=1.176406 valid_loss=1.119227\n",
      "Epoch: 200: loss=1.154773 valid_loss=1.109137\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 300: loss=0.946192 valid_loss=0.911721\n",
      "Epoch: 400: loss=0.498426 valid_loss=0.446076\n",
      "Epoch: 500: loss=0.334629 valid_loss=0.338265\n",
      "Epoch: 600: loss=0.249638 valid_loss=0.218239\n",
      "Epoch: 700: loss=0.217622 valid_loss=0.138623\n",
      "Epoch: 800: loss=0.185936 valid_loss=0.112533\n",
      "Epoch: 900: loss=0.168390 valid_loss=0.096381\n",
      "Epoch: 1000: loss=0.167901 valid_loss=0.086587\n",
      "Accuracy = 96.66666666666667%\n",
      "96.66666666666667%, 16/16 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100: loss=1.168283 valid_loss=1.115342\n",
      "Epoch: 200: loss=1.029285 valid_loss=1.018992\n",
      "Epoch: 300: loss=0.501871 valid_loss=0.436200\n",
      "Epoch: 400: loss=0.288955 valid_loss=0.214712\n",
      "Epoch: 500: loss=0.280899 valid_loss=0.162467\n",
      "Epoch: 600: loss=0.203817 valid_loss=0.125680\n",
      "Epoch: 700: loss=0.186659 valid_loss=0.144444\n",
      "Epoch: 800: loss=0.167228 valid_loss=0.105987\n",
      "Epoch: 900: loss=0.218854 valid_loss=0.102185\n",
      "Epoch: 1000: loss=0.170531 valid_loss=0.083509\n",
      "Accuracy = 96.66666666666667%\n",
      "96.66666666666667%, 19/19 features\n",
      "Epoch: 100: loss=1.155599 valid_loss=1.104482\n",
      "Epoch: 200: loss=0.864109 valid_loss=0.830741\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 300: loss=0.541365 valid_loss=0.580750\n",
      "Epoch: 400: loss=0.600157 valid_loss=0.318252\n",
      "Epoch: 500: loss=0.318347 valid_loss=0.177982\n",
      "Epoch: 600: loss=0.212023 valid_loss=0.191398\n",
      "Epoch: 700: loss=0.225164 valid_loss=0.095635\n",
      "Epoch: 800: loss=0.230132 valid_loss=0.132837\n",
      "Epoch: 900: loss=0.168758 valid_loss=0.096495\n",
      "Epoch: 1000: loss=0.177018 valid_loss=0.078062\n",
      "Accuracy = 93.33333333333333%\n",
      "93.33333333333333%, 21/21 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100: loss=1.178348 valid_loss=1.121137\n",
      "Epoch: 200: loss=1.176904 valid_loss=1.117700\n",
      "Epoch: 300: loss=1.153630 valid_loss=1.095212\n",
      "Epoch: 400: loss=1.031114 valid_loss=0.921778\n",
      "Epoch: 500: loss=0.564612 valid_loss=0.387338\n",
      "Epoch: 600: loss=0.297790 valid_loss=0.163133\n",
      "Epoch: 700: loss=0.179204 valid_loss=0.099445\n",
      "Epoch: 800: loss=0.271537 valid_loss=0.071008\n",
      "Epoch: 900: loss=0.198154 valid_loss=0.078644\n",
      "Epoch: 1000: loss=0.205863 valid_loss=0.103703\n",
      "Accuracy = 96.66666666666667%\n",
      "96.66666666666667%, 23/23 features\n",
      "Epoch: 100: loss=1.132682 valid_loss=1.098809\n",
      "Epoch: 200: loss=0.653530 valid_loss=0.672368\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 300: loss=0.327961 valid_loss=0.386982\n",
      "Epoch: 400: loss=0.329193 valid_loss=0.522987\n",
      "Epoch: 500: loss=0.275688 valid_loss=0.139918\n",
      "Epoch: 600: loss=0.191655 valid_loss=0.107449\n",
      "Epoch: 700: loss=0.160487 valid_loss=0.084397\n",
      "Epoch: 800: loss=0.183933 valid_loss=0.085291\n",
      "Epoch: 900: loss=0.146067 valid_loss=0.066622\n",
      "Epoch: 1000: loss=0.143788 valid_loss=0.062690\n",
      "Accuracy = 100.0%\n",
      "100.0%, 25/25 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100: loss=1.179378 valid_loss=1.119776\n",
      "Epoch: 200: loss=1.154208 valid_loss=1.102046\n",
      "Epoch: 300: loss=0.867236 valid_loss=0.881367\n",
      "Epoch: 400: loss=0.511078 valid_loss=0.447056\n",
      "Epoch: 500: loss=0.298278 valid_loss=0.250464\n",
      "Epoch: 600: loss=0.226951 valid_loss=0.141544\n",
      "Epoch: 700: loss=0.180868 valid_loss=0.127650\n",
      "Epoch: 800: loss=0.169152 valid_loss=0.082000\n",
      "Epoch: 900: loss=0.154817 valid_loss=0.070096\n",
      "Epoch: 1000: loss=0.134545 valid_loss=0.062548\n",
      "Accuracy = 100.0%\n",
      "100.0%, 27/27 features\n",
      "Epoch: 100: loss=1.173511 valid_loss=1.112641\n",
      "Epoch: 200: loss=1.126594 valid_loss=1.060482\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 300: loss=0.778778 valid_loss=0.586181\n",
      "Epoch: 400: loss=0.330220 valid_loss=0.278065\n",
      "Epoch: 500: loss=0.241734 valid_loss=0.175533\n",
      "Epoch: 600: loss=0.313693 valid_loss=0.125475\n",
      "Epoch: 700: loss=0.162473 valid_loss=0.081977\n",
      "Epoch: 800: loss=0.184116 valid_loss=0.095952\n",
      "Epoch: 900: loss=0.145270 valid_loss=0.059469\n",
      "Epoch: 1000: loss=0.153303 valid_loss=0.051539\n",
      "Accuracy = 100.0%\n",
      "100.0%, 30/30 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100: loss=1.154210 valid_loss=1.105746\n",
      "Epoch: 200: loss=0.755261 valid_loss=0.675538\n",
      "Epoch: 300: loss=0.539499 valid_loss=0.368111\n",
      "Epoch: 400: loss=0.254133 valid_loss=0.264345\n",
      "Epoch: 500: loss=0.370454 valid_loss=0.178648\n",
      "Epoch: 600: loss=0.374621 valid_loss=0.355206\n",
      "Epoch: 700: loss=0.158345 valid_loss=0.086726\n",
      "Epoch: 800: loss=0.145467 valid_loss=0.083414\n",
      "Epoch: 900: loss=0.238500 valid_loss=0.173224\n",
      "Epoch: 1000: loss=0.174521 valid_loss=0.067883\n",
      "Accuracy = 96.66666666666667%\n",
      "96.66666666666667%, 32/32 features\n",
      "Epoch: 100: loss=1.169555 valid_loss=1.116633\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 200: loss=1.080771 valid_loss=1.045528\n",
      "Epoch: 300: loss=0.533670 valid_loss=0.463742\n",
      "Epoch: 400: loss=0.289924 valid_loss=0.371649\n",
      "Epoch: 500: loss=0.252318 valid_loss=0.198800\n",
      "Epoch: 600: loss=0.290776 valid_loss=0.165599\n",
      "Epoch: 700: loss=0.169017 valid_loss=0.116328\n",
      "Epoch: 800: loss=0.193958 valid_loss=0.096464\n",
      "Epoch: 900: loss=0.170806 valid_loss=0.081515\n",
      "Epoch: 1000: loss=0.129908 valid_loss=0.075900\n",
      "Accuracy = 96.66666666666667%\n",
      "96.66666666666667%, 34/34 features\n",
      "Epoch: 100: loss=1.177800 valid_loss=1.120083\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 200: loss=1.165737 valid_loss=1.108624\n",
      "Epoch: 300: loss=1.011823 valid_loss=0.968656\n",
      "Epoch: 400: loss=0.511415 valid_loss=0.426125\n",
      "Epoch: 500: loss=0.260069 valid_loss=0.224494\n",
      "Epoch: 600: loss=0.200229 valid_loss=0.193512\n",
      "Epoch: 700: loss=0.299440 valid_loss=0.240497\n",
      "Epoch: 800: loss=0.160809 valid_loss=0.201408\n",
      "Epoch: 900: loss=0.217977 valid_loss=0.331002\n",
      "Epoch: 1000: loss=0.146968 valid_loss=0.178101\n",
      "Accuracy = 93.33333333333333%\n",
      "93.33333333333333%, 36/36 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100: loss=1.170910 valid_loss=1.113966\n",
      "Epoch: 200: loss=1.020318 valid_loss=0.988139\n",
      "Epoch: 300: loss=0.577534 valid_loss=0.588363\n",
      "Epoch: 400: loss=0.337159 valid_loss=0.295523\n",
      "Epoch: 500: loss=0.266583 valid_loss=0.211086\n",
      "Epoch: 600: loss=0.274594 valid_loss=0.161192\n",
      "Epoch: 700: loss=0.163503 valid_loss=0.137471\n",
      "Epoch: 800: loss=0.155963 valid_loss=0.141174\n",
      "Epoch: 900: loss=0.230182 valid_loss=0.142173\n",
      "Epoch: 1000: loss=0.137950 valid_loss=0.203456\n",
      "Accuracy = 96.66666666666667%\n",
      "96.66666666666667%, 38/38 features\n",
      "Epoch: 100: loss=1.156450 valid_loss=1.083934\n",
      "Epoch: 200: loss=0.600998 valid_loss=0.561567\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 300: loss=0.320770 valid_loss=0.243023\n",
      "Epoch: 400: loss=0.216480 valid_loss=0.128442\n",
      "Epoch: 500: loss=0.172174 valid_loss=0.068375\n",
      "Epoch: 600: loss=0.173264 valid_loss=0.063509\n",
      "Epoch: 700: loss=0.165966 valid_loss=0.053884\n",
      "Epoch: 800: loss=0.141003 valid_loss=0.038239\n",
      "Epoch: 900: loss=0.147128 valid_loss=0.044896\n",
      "Epoch: 1000: loss=0.132417 valid_loss=0.037357\n",
      "Accuracy = 100.0%\n",
      "100.0%, 5/8 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100: loss=1.168167 valid_loss=1.093763\n",
      "Epoch: 200: loss=0.839085 valid_loss=0.750737\n",
      "Epoch: 300: loss=0.456880 valid_loss=0.538272\n",
      "Epoch: 400: loss=0.294722 valid_loss=0.219063\n",
      "Epoch: 500: loss=0.210984 valid_loss=0.128500\n",
      "Epoch: 600: loss=0.181257 valid_loss=0.097918\n",
      "Epoch: 700: loss=0.180615 valid_loss=0.107599\n",
      "Epoch: 800: loss=0.159457 valid_loss=0.081521\n",
      "Epoch: 900: loss=0.147728 valid_loss=0.061468\n",
      "Epoch: 1000: loss=2.157159 valid_loss=2.551970\n",
      "Accuracy = 60.0%\n",
      "60.0%, 6/10 features\n",
      "Epoch: 100: loss=1.165963 valid_loss=1.096415\n",
      "Epoch: 200: loss=0.710532 valid_loss=0.683915\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 300: loss=0.409049 valid_loss=0.325306\n",
      "Epoch: 400: loss=0.315858 valid_loss=0.181559\n",
      "Epoch: 500: loss=0.197627 valid_loss=0.143344\n",
      "Epoch: 600: loss=0.174196 valid_loss=0.093072\n",
      "Epoch: 700: loss=0.200612 valid_loss=0.109775\n",
      "Epoch: 800: loss=0.150635 valid_loss=0.067939\n",
      "Epoch: 900: loss=0.156191 valid_loss=0.060892\n",
      "Epoch: 1000: loss=0.135156 valid_loss=0.049827\n",
      "Accuracy = 100.0%\n",
      "100.0%, 10/12 features\n",
      "Epoch: 100: loss=1.167019 valid_loss=1.114585\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 200: loss=1.059216 valid_loss=1.032328\n",
      "Epoch: 300: loss=0.639846 valid_loss=0.590016\n",
      "Epoch: 400: loss=0.308522 valid_loss=0.257028\n",
      "Epoch: 500: loss=0.213660 valid_loss=0.141754\n",
      "Epoch: 600: loss=0.184521 valid_loss=0.094786\n",
      "Epoch: 700: loss=0.172380 valid_loss=0.066500\n",
      "Epoch: 800: loss=0.174204 valid_loss=0.061200\n",
      "Epoch: 900: loss=0.192803 valid_loss=0.102353\n",
      "Epoch: 1000: loss=0.145844 valid_loss=0.044282\n",
      "Accuracy = 100.0%\n",
      "100.0%, 14/14 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100: loss=1.165322 valid_loss=1.109341\n",
      "Epoch: 200: loss=1.004289 valid_loss=0.939965\n",
      "Epoch: 300: loss=0.503145 valid_loss=0.455821\n",
      "Epoch: 400: loss=0.312742 valid_loss=0.304960\n",
      "Epoch: 500: loss=0.291853 valid_loss=0.197136\n",
      "Epoch: 600: loss=0.205329 valid_loss=0.130391\n",
      "Epoch: 700: loss=0.173487 valid_loss=0.095847\n",
      "Epoch: 800: loss=0.158784 valid_loss=0.070648\n",
      "Epoch: 900: loss=0.157458 valid_loss=0.062664\n",
      "Epoch: 1000: loss=0.151661 valid_loss=0.059167\n",
      "Accuracy = 100.0%\n",
      "100.0%, 15/16 features\n",
      "Epoch: 100: loss=1.140144 valid_loss=1.078102\n",
      "Epoch: 200: loss=0.780945 valid_loss=0.587527\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 300: loss=0.305403 valid_loss=0.262931\n",
      "Epoch: 400: loss=0.662499 valid_loss=0.165749\n",
      "Epoch: 500: loss=0.183144 valid_loss=0.165874\n",
      "Epoch: 600: loss=0.167425 valid_loss=0.130457\n",
      "Epoch: 700: loss=0.219262 valid_loss=0.097660\n",
      "Epoch: 800: loss=0.156828 valid_loss=0.087285\n",
      "Epoch: 900: loss=0.156599 valid_loss=0.096668\n",
      "Epoch: 1000: loss=0.145217 valid_loss=0.068429\n",
      "Accuracy = 96.66666666666667%\n",
      "96.66666666666667%, 19/19 features\n",
      "Epoch: 100: loss=1.171155 valid_loss=1.117764\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 200: loss=1.048699 valid_loss=1.022218\n",
      "Epoch: 300: loss=0.755614 valid_loss=0.730382\n",
      "Epoch: 400: loss=0.389988 valid_loss=0.414563\n",
      "Epoch: 500: loss=0.294897 valid_loss=0.216359\n",
      "Epoch: 600: loss=0.219556 valid_loss=0.125225\n",
      "Epoch: 700: loss=0.222854 valid_loss=0.165463\n",
      "Epoch: 800: loss=0.216166 valid_loss=0.303195\n",
      "Epoch: 900: loss=1.553940 valid_loss=0.083196\n",
      "Epoch: 1000: loss=0.153647 valid_loss=0.075163\n",
      "Accuracy = 96.66666666666667%\n",
      "96.66666666666667%, 21/21 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100: loss=1.173499 valid_loss=1.119209\n",
      "Epoch: 200: loss=1.117157 valid_loss=1.085693\n",
      "Epoch: 300: loss=0.655063 valid_loss=0.612892\n",
      "Epoch: 400: loss=0.381863 valid_loss=0.292643\n",
      "Epoch: 500: loss=0.396355 valid_loss=0.207630\n",
      "Epoch: 600: loss=0.193900 valid_loss=0.163572\n",
      "Epoch: 700: loss=0.193471 valid_loss=0.098118\n",
      "Epoch: 800: loss=0.165954 valid_loss=0.077156\n",
      "Epoch: 900: loss=0.168577 valid_loss=0.074990\n",
      "Epoch: 1000: loss=0.154777 valid_loss=0.105560\n",
      "Accuracy = 100.0%\n",
      "100.0%, 23/23 features\n",
      "Epoch: 100: loss=1.162681 valid_loss=1.110934\n",
      "Epoch: 200: loss=0.947778 valid_loss=0.910926\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 300: loss=0.488566 valid_loss=0.427352\n",
      "Epoch: 400: loss=0.376351 valid_loss=0.503708\n",
      "Epoch: 500: loss=0.225121 valid_loss=0.154151\n",
      "Epoch: 600: loss=0.185887 valid_loss=0.116634\n",
      "Epoch: 700: loss=0.150086 valid_loss=0.084099\n",
      "Epoch: 800: loss=0.255489 valid_loss=0.182424\n",
      "Epoch: 900: loss=0.130415 valid_loss=0.111863\n",
      "Epoch: 1000: loss=0.165705 valid_loss=0.111918\n",
      "Accuracy = 93.33333333333333%\n",
      "93.33333333333333%, 25/25 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100: loss=1.168435 valid_loss=1.118116\n",
      "Epoch: 200: loss=1.129304 valid_loss=1.067860\n",
      "Epoch: 300: loss=0.561054 valid_loss=0.509553\n",
      "Epoch: 400: loss=0.313218 valid_loss=0.238927\n",
      "Epoch: 500: loss=0.280736 valid_loss=0.173715\n",
      "Epoch: 600: loss=0.398178 valid_loss=0.496139\n",
      "Epoch: 700: loss=0.182348 valid_loss=0.090001\n",
      "Epoch: 800: loss=0.144468 valid_loss=0.064176\n",
      "Epoch: 900: loss=0.182716 valid_loss=0.082128\n",
      "Epoch: 1000: loss=0.129904 valid_loss=0.061566\n",
      "Accuracy = 93.33333333333333%\n",
      "93.33333333333333%, 27/27 features\n",
      "Epoch: 100: loss=1.158105 valid_loss=1.094881\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 200: loss=0.643085 valid_loss=0.701529\n",
      "Epoch: 300: loss=0.368933 valid_loss=0.286819\n",
      "Epoch: 400: loss=0.265437 valid_loss=0.203357\n",
      "Epoch: 500: loss=0.270681 valid_loss=0.149519\n",
      "Epoch: 600: loss=0.197268 valid_loss=0.117122\n",
      "Epoch: 700: loss=0.181106 valid_loss=0.100959\n",
      "Epoch: 800: loss=0.162901 valid_loss=0.091581\n",
      "Epoch: 900: loss=0.144634 valid_loss=0.089725\n",
      "Epoch: 1000: loss=0.179656 valid_loss=0.085039\n",
      "Accuracy = 93.33333333333333%\n",
      "93.33333333333333%, 30/30 features\n",
      "Epoch: 100: loss=1.163942 valid_loss=1.115988\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 200: loss=1.046796 valid_loss=1.016015\n",
      "Epoch: 300: loss=0.487677 valid_loss=0.488300\n",
      "Epoch: 400: loss=0.309702 valid_loss=0.285982\n",
      "Epoch: 500: loss=0.282773 valid_loss=0.197976\n",
      "Epoch: 600: loss=0.175085 valid_loss=0.169000\n",
      "Epoch: 700: loss=0.198648 valid_loss=0.154775\n",
      "Epoch: 800: loss=0.176032 valid_loss=0.108704\n",
      "Epoch: 900: loss=0.146129 valid_loss=0.087287\n",
      "Epoch: 1000: loss=0.146412 valid_loss=0.095796\n",
      "Accuracy = 96.66666666666667%\n",
      "96.66666666666667%, 32/32 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100: loss=1.172920 valid_loss=1.117908\n",
      "Epoch: 200: loss=1.131474 valid_loss=1.089718\n",
      "Epoch: 300: loss=0.605805 valid_loss=0.623069\n",
      "Epoch: 400: loss=0.426359 valid_loss=0.298849\n",
      "Epoch: 500: loss=0.327587 valid_loss=0.401837\n",
      "Epoch: 600: loss=0.203291 valid_loss=0.153163\n",
      "Epoch: 700: loss=0.262265 valid_loss=0.160841\n",
      "Epoch: 800: loss=0.171506 valid_loss=0.120056\n",
      "Epoch: 900: loss=0.201938 valid_loss=0.138208\n",
      "Epoch: 1000: loss=0.185890 valid_loss=0.179872\n",
      "Accuracy = 93.33333333333333%\n",
      "93.33333333333333%, 34/34 features\n",
      "Epoch: 100: loss=1.174687 valid_loss=1.120345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 200: loss=1.125189 valid_loss=1.098667\n",
      "Epoch: 300: loss=0.727889 valid_loss=0.751233\n",
      "Epoch: 400: loss=0.481618 valid_loss=0.496473\n",
      "Epoch: 500: loss=0.340454 valid_loss=0.317037\n",
      "Epoch: 600: loss=0.347355 valid_loss=0.206660\n",
      "Epoch: 700: loss=0.221898 valid_loss=0.178793\n",
      "Epoch: 800: loss=0.185921 valid_loss=0.133459\n",
      "Epoch: 900: loss=0.166886 valid_loss=0.156707\n",
      "Epoch: 1000: loss=0.160964 valid_loss=0.160455\n",
      "Accuracy = 93.33333333333333%\n",
      "93.33333333333333%, 36/36 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100: loss=1.176099 valid_loss=1.120617\n",
      "Epoch: 200: loss=1.148951 valid_loss=1.103613\n",
      "Epoch: 300: loss=0.739814 valid_loss=0.680759\n",
      "Epoch: 400: loss=0.349854 valid_loss=0.372657\n",
      "Epoch: 500: loss=0.283910 valid_loss=0.173102\n",
      "Epoch: 600: loss=0.279353 valid_loss=0.129835\n",
      "Epoch: 700: loss=0.159622 valid_loss=0.115797\n",
      "Epoch: 800: loss=0.155457 valid_loss=0.075495\n",
      "Epoch: 900: loss=0.148735 valid_loss=0.073031\n",
      "Epoch: 1000: loss=0.129761 valid_loss=0.070100\n",
      "Accuracy = 96.66666666666667%\n",
      "96.66666666666667%, 38/38 features\n",
      "Epoch: 100: loss=1.169555 valid_loss=1.115185\n",
      "Epoch: 200: loss=1.147798 valid_loss=1.047318\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 300: loss=0.521481 valid_loss=0.503050\n",
      "Epoch: 400: loss=0.279611 valid_loss=0.205559\n",
      "Epoch: 500: loss=0.218705 valid_loss=0.105179\n",
      "Epoch: 600: loss=0.169139 valid_loss=0.077680\n",
      "Epoch: 700: loss=0.156881 valid_loss=0.050544\n",
      "Epoch: 800: loss=0.143802 valid_loss=0.048289\n",
      "Epoch: 900: loss=0.134291 valid_loss=0.034763\n",
      "Epoch: 1000: loss=0.146462 valid_loss=0.030862\n",
      "Accuracy = 100.0%\n",
      "100.0%, 5/8 features\n",
      "Epoch: 100: loss=1.069401 valid_loss=1.052119\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 200: loss=0.584617 valid_loss=0.581806\n",
      "Epoch: 300: loss=0.389837 valid_loss=0.383842\n",
      "Epoch: 400: loss=0.278160 valid_loss=0.216984\n",
      "Epoch: 500: loss=0.220963 valid_loss=0.147178\n",
      "Epoch: 600: loss=0.278460 valid_loss=0.237264\n",
      "Epoch: 700: loss=0.185214 valid_loss=0.098268\n",
      "Epoch: 800: loss=0.210933 valid_loss=0.146282\n",
      "Epoch: 900: loss=0.159922 valid_loss=0.072618\n",
      "Epoch: 1000: loss=0.167167 valid_loss=0.078592\n",
      "Accuracy = 96.66666666666667%\n",
      "96.66666666666667%, 7/10 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100: loss=1.156170 valid_loss=1.108309\n",
      "Epoch: 200: loss=0.894124 valid_loss=0.869007\n",
      "Epoch: 300: loss=0.405195 valid_loss=0.380056\n",
      "Epoch: 400: loss=0.308984 valid_loss=0.211765\n",
      "Epoch: 500: loss=0.212080 valid_loss=0.128201\n",
      "Epoch: 600: loss=0.183721 valid_loss=0.095971\n",
      "Epoch: 700: loss=0.161888 valid_loss=0.116575\n",
      "Epoch: 800: loss=0.149305 valid_loss=0.073169\n",
      "Epoch: 900: loss=0.180467 valid_loss=0.108635\n",
      "Epoch: 1000: loss=0.139588 valid_loss=0.058405\n",
      "Accuracy = 96.66666666666667%\n",
      "96.66666666666667%, 11/12 features\n",
      "Epoch: 100: loss=1.167174 valid_loss=1.111791\n",
      "Epoch: 200: loss=1.080060 valid_loss=1.012485\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 300: loss=0.511051 valid_loss=0.528922\n",
      "Epoch: 400: loss=0.319022 valid_loss=0.263754\n",
      "Epoch: 500: loss=0.215758 valid_loss=0.141773\n",
      "Epoch: 600: loss=0.540280 valid_loss=0.099762\n",
      "Epoch: 700: loss=0.156805 valid_loss=0.073158\n",
      "Epoch: 800: loss=0.148854 valid_loss=0.058008\n",
      "Epoch: 900: loss=0.170132 valid_loss=0.093222\n",
      "Epoch: 1000: loss=0.146389 valid_loss=0.046072\n",
      "Accuracy = 100.0%\n",
      "100.0%, 14/14 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100: loss=1.164472 valid_loss=1.107386\n",
      "Epoch: 200: loss=1.073228 valid_loss=0.906655\n",
      "Epoch: 300: loss=0.595317 valid_loss=0.637489\n",
      "Epoch: 400: loss=0.458719 valid_loss=0.436710\n",
      "Epoch: 500: loss=0.360538 valid_loss=0.311753\n",
      "Epoch: 600: loss=0.282676 valid_loss=0.237939\n",
      "Epoch: 700: loss=0.225324 valid_loss=0.221176\n",
      "Epoch: 800: loss=0.212827 valid_loss=0.185667\n",
      "Epoch: 900: loss=0.171003 valid_loss=0.103978\n",
      "Epoch: 1000: loss=0.206102 valid_loss=0.104750\n",
      "Accuracy = 96.66666666666667%\n",
      "96.66666666666667%, 16/16 features\n",
      "Epoch: 100: loss=1.176140 valid_loss=1.121531\n",
      "Epoch: 200: loss=1.137649 valid_loss=1.105298\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 300: loss=0.817779 valid_loss=0.678314\n",
      "Epoch: 400: loss=0.556673 valid_loss=0.585217\n",
      "Epoch: 500: loss=0.233358 valid_loss=0.194054\n",
      "Epoch: 600: loss=0.201952 valid_loss=0.157747\n",
      "Epoch: 700: loss=0.155609 valid_loss=0.105157\n",
      "Epoch: 800: loss=0.269701 valid_loss=0.224392\n",
      "Epoch: 900: loss=0.149770 valid_loss=0.088866\n",
      "Epoch: 1000: loss=0.161847 valid_loss=0.088755\n",
      "Accuracy = 96.66666666666667%\n",
      "96.66666666666667%, 19/19 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100: loss=1.174456 valid_loss=1.118863\n",
      "Epoch: 200: loss=1.045991 valid_loss=1.036049\n",
      "Epoch: 300: loss=0.539384 valid_loss=0.483748\n",
      "Epoch: 400: loss=0.441553 valid_loss=0.268034\n",
      "Epoch: 500: loss=0.234632 valid_loss=0.176690\n",
      "Epoch: 600: loss=0.197021 valid_loss=0.165065\n",
      "Epoch: 700: loss=0.176387 valid_loss=0.117438\n",
      "Epoch: 800: loss=0.210313 valid_loss=0.100573\n",
      "Epoch: 900: loss=0.277849 valid_loss=0.115535\n",
      "Epoch: 1000: loss=0.145222 valid_loss=0.080844\n",
      "Accuracy = 100.0%\n",
      "100.0%, 21/21 features\n",
      "Epoch: 100: loss=1.161779 valid_loss=1.113433\n",
      "Epoch: 200: loss=1.006723 valid_loss=1.000734\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 300: loss=0.659762 valid_loss=0.605635\n",
      "Epoch: 400: loss=0.309638 valid_loss=0.307460\n",
      "Epoch: 500: loss=0.205476 valid_loss=0.234444\n",
      "Epoch: 600: loss=0.298343 valid_loss=0.131098\n",
      "Epoch: 700: loss=0.151909 valid_loss=0.113492\n",
      "Epoch: 800: loss=0.212588 valid_loss=0.129716\n",
      "Epoch: 900: loss=0.128701 valid_loss=0.093442\n",
      "Epoch: 1000: loss=0.145597 valid_loss=0.096319\n",
      "Accuracy = 93.33333333333333%\n",
      "93.33333333333333%, 23/23 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100: loss=1.170083 valid_loss=1.118259\n",
      "Epoch: 200: loss=1.066223 valid_loss=1.060603\n",
      "Epoch: 300: loss=0.565265 valid_loss=0.537049\n",
      "Epoch: 400: loss=0.450968 valid_loss=0.223156\n",
      "Epoch: 500: loss=0.223766 valid_loss=0.146209\n",
      "Epoch: 600: loss=0.184677 valid_loss=0.118224\n",
      "Epoch: 700: loss=0.185854 valid_loss=0.095253\n",
      "Epoch: 800: loss=0.361098 valid_loss=0.105482\n",
      "Epoch: 900: loss=0.166400 valid_loss=0.089981\n",
      "Epoch: 1000: loss=0.155461 valid_loss=0.077582\n",
      "Accuracy = 96.66666666666667%\n",
      "96.66666666666667%, 25/25 features\n",
      "Epoch: 100: loss=1.170001 valid_loss=1.116146\n",
      "Epoch: 200: loss=1.121440 valid_loss=1.057801\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 300: loss=0.725779 valid_loss=0.718732\n",
      "Epoch: 400: loss=0.308672 valid_loss=0.234769\n",
      "Epoch: 500: loss=0.253224 valid_loss=0.162965\n",
      "Epoch: 600: loss=0.198033 valid_loss=0.115504\n",
      "Epoch: 700: loss=0.174028 valid_loss=0.088417\n",
      "Epoch: 800: loss=0.191321 valid_loss=0.110160\n",
      "Epoch: 900: loss=0.156760 valid_loss=0.064685\n",
      "Epoch: 1000: loss=0.157322 valid_loss=0.061625\n",
      "Accuracy = 100.0%\n",
      "100.0%, 27/27 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100: loss=1.166897 valid_loss=1.109314\n",
      "Epoch: 200: loss=1.039302 valid_loss=0.972983\n",
      "Epoch: 300: loss=0.480884 valid_loss=0.360582\n",
      "Epoch: 400: loss=0.314146 valid_loss=0.190315\n",
      "Epoch: 500: loss=0.217958 valid_loss=0.121482\n",
      "Epoch: 600: loss=0.182103 valid_loss=0.090867\n",
      "Epoch: 700: loss=0.172464 valid_loss=0.077643\n",
      "Epoch: 800: loss=0.160289 valid_loss=0.123753\n",
      "Epoch: 900: loss=0.160512 valid_loss=0.060094\n",
      "Epoch: 1000: loss=0.166749 valid_loss=0.056929\n",
      "Accuracy = 100.0%\n",
      "100.0%, 30/30 features\n",
      "Epoch: 100: loss=1.173870 valid_loss=1.117808\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 200: loss=1.096894 valid_loss=1.034320\n",
      "Epoch: 300: loss=0.473607 valid_loss=0.506851\n",
      "Epoch: 400: loss=0.495654 valid_loss=0.438176\n",
      "Epoch: 500: loss=0.229042 valid_loss=0.198787\n",
      "Epoch: 600: loss=0.194434 valid_loss=0.117481\n",
      "Epoch: 700: loss=0.173199 valid_loss=0.131977\n",
      "Epoch: 800: loss=0.254879 valid_loss=0.184522\n",
      "Epoch: 900: loss=0.153012 valid_loss=0.090333\n",
      "Epoch: 1000: loss=0.172441 valid_loss=0.089329\n",
      "Accuracy = 96.66666666666667%\n",
      "96.66666666666667%, 32/32 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100: loss=1.160471 valid_loss=1.106913\n",
      "Epoch: 200: loss=0.913791 valid_loss=0.810964\n",
      "Epoch: 300: loss=0.524705 valid_loss=0.583165\n",
      "Epoch: 400: loss=0.286973 valid_loss=0.257782\n",
      "Epoch: 500: loss=0.408646 valid_loss=0.153631\n",
      "Epoch: 600: loss=0.180486 valid_loss=0.106649\n",
      "Epoch: 700: loss=0.244918 valid_loss=0.090926\n",
      "Epoch: 800: loss=0.163023 valid_loss=0.086243\n",
      "Epoch: 900: loss=0.157955 valid_loss=0.083173\n",
      "Epoch: 1000: loss=0.178807 valid_loss=0.059218\n",
      "Accuracy = 100.0%\n",
      "100.0%, 34/34 features\n",
      "Epoch: 100: loss=1.174212 valid_loss=1.117002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 200: loss=1.124040 valid_loss=1.083617\n",
      "Epoch: 300: loss=0.701339 valid_loss=0.711308\n",
      "Epoch: 400: loss=0.459710 valid_loss=0.435862\n",
      "Epoch: 500: loss=0.307314 valid_loss=0.364830\n",
      "Epoch: 600: loss=0.348540 valid_loss=0.696449\n",
      "Epoch: 700: loss=0.178227 valid_loss=0.215713\n",
      "Epoch: 800: loss=0.306175 valid_loss=0.116054\n",
      "Epoch: 900: loss=0.140679 valid_loss=0.119748\n",
      "Epoch: 1000: loss=0.145322 valid_loss=0.099492\n",
      "Accuracy = 96.66666666666667%\n",
      "96.66666666666667%, 36/36 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100: loss=1.167399 valid_loss=1.105135\n",
      "Epoch: 200: loss=0.944813 valid_loss=0.880294\n",
      "Epoch: 300: loss=0.412732 valid_loss=0.378354\n",
      "Epoch: 400: loss=0.286050 valid_loss=0.212519\n",
      "Epoch: 500: loss=0.265956 valid_loss=0.133251\n",
      "Epoch: 600: loss=0.263379 valid_loss=0.477987\n",
      "Epoch: 700: loss=0.184525 valid_loss=0.134569\n",
      "Epoch: 800: loss=0.181607 valid_loss=0.089543\n",
      "Epoch: 900: loss=0.151359 valid_loss=0.104187\n",
      "Epoch: 1000: loss=0.169532 valid_loss=0.063543\n",
      "Accuracy = 100.0%\n",
      "100.0%, 38/38 features\n"
     ]
    }
   ],
   "source": [
    "from stg import STG\n",
    "for t in range(10):\n",
    "    accs = []\n",
    "    for i in range(len(num_normal)): \n",
    "        X_train, X_test, Y_train, Y_test = insert_feature_noise(X, y, num_random_noise=num_normal[i],\n",
    "            num_overwhelemed=num_overwhelmed[i], num_shortcut=num_shortcut[i])\n",
    "        X_train = X_train.to_numpy().astype(np.float32)\n",
    "        X_test = X_test.to_numpy().astype(np.float32)\n",
    "        clf = STG(task_type='classification',\n",
    "                input_dim=X_train.shape[1],\n",
    "                output_dim=3,\n",
    "                hidden_dims=(16, 16),\n",
    "                activation='ReLU', optimizer='SGD', learning_rate=0.1,\n",
    "                batch_size=X_train.shape[0], feature_selection=True, \n",
    "                sigma=0.5, lam=0.1, random_state=1, device=\"cpu\")\n",
    "        clf.fit(X_train, Y_train, nr_epochs=1000,\n",
    "            print_interval=100, \n",
    "            valid_X=X_test, \n",
    "            valid_y=Y_test)\n",
    "        acc = print_accuracy(clf.predict, X_test, Y_test)\n",
    "        stg_results[\"accuracy\"].append(acc)\n",
    "        total_feature = 2*num_normal[i] + num_overwhelmed[i] + num_shortcut[i]\n",
    "        stg_results['# features'].append(total_feature)\n",
    "        num_gate = np.count_nonzero(clf.get_gates(mode='prob'))\n",
    "        print(f\"{acc}%, {num_gate}/{total_feature + 4} features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 100.0%\n",
      "100.0%, 4 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 100.0%\n",
      "100.0%, 6 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 96.66666666666667%\n",
      "96.66666666666667%, 8 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 96.66666666666667%\n",
      "96.66666666666667%, 10 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 100.0%\n",
      "100.0%, 12 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 96.66666666666667%\n",
      "96.66666666666667%, 15 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 100.0%\n",
      "100.0%, 17 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 96.66666666666667%\n",
      "96.66666666666667%, 19 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 96.66666666666667%\n",
      "96.66666666666667%, 21 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 96.66666666666667%\n",
      "96.66666666666667%, 23 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 93.33333333333333%\n",
      "93.33333333333333%, 26 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 100.0%\n",
      "100.0%, 28 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 96.66666666666667%\n",
      "96.66666666666667%, 30 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 90.0%\n",
      "90.0%, 32 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 90.0%\n",
      "90.0%, 34 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 100.0%\n",
      "100.0%, 4 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 100.0%\n",
      "100.0%, 6 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 100.0%\n",
      "100.0%, 8 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 100.0%\n",
      "100.0%, 10 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 96.66666666666667%\n",
      "96.66666666666667%, 12 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 100.0%\n",
      "100.0%, 15 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 100.0%\n",
      "100.0%, 17 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 93.33333333333333%\n",
      "93.33333333333333%, 19 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 93.33333333333333%\n",
      "93.33333333333333%, 21 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 96.66666666666667%\n",
      "96.66666666666667%, 23 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 93.33333333333333%\n",
      "93.33333333333333%, 26 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 100.0%\n",
      "100.0%, 28 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 96.66666666666667%\n",
      "96.66666666666667%, 30 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 83.33333333333333%\n",
      "83.33333333333333%, 32 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 100.0%\n",
      "100.0%, 34 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 100.0%\n",
      "100.0%, 4 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 100.0%\n",
      "100.0%, 6 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 100.0%\n",
      "100.0%, 8 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 100.0%\n",
      "100.0%, 10 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 100.0%\n",
      "100.0%, 12 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 93.33333333333333%\n",
      "93.33333333333333%, 15 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 100.0%\n",
      "100.0%, 17 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 100.0%\n",
      "100.0%, 19 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 96.66666666666667%\n",
      "96.66666666666667%, 21 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 100.0%\n",
      "100.0%, 23 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 90.0%\n",
      "90.0%, 26 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 96.66666666666667%\n",
      "96.66666666666667%, 28 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 96.66666666666667%\n",
      "96.66666666666667%, 30 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 90.0%\n",
      "90.0%, 32 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 86.66666666666667%\n",
      "86.66666666666667%, 34 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 100.0%\n",
      "100.0%, 4 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 100.0%\n",
      "100.0%, 6 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 100.0%\n",
      "100.0%, 8 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 100.0%\n",
      "100.0%, 10 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 100.0%\n",
      "100.0%, 12 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 93.33333333333333%\n",
      "93.33333333333333%, 15 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 96.66666666666667%\n",
      "96.66666666666667%, 17 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 96.66666666666667%\n",
      "96.66666666666667%, 19 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 96.66666666666667%\n",
      "96.66666666666667%, 21 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 96.66666666666667%\n",
      "96.66666666666667%, 23 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 93.33333333333333%\n",
      "93.33333333333333%, 26 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 96.66666666666667%\n",
      "96.66666666666667%, 28 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 96.66666666666667%\n",
      "96.66666666666667%, 30 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 100.0%\n",
      "100.0%, 32 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 86.66666666666667%\n",
      "86.66666666666667%, 34 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 100.0%\n",
      "100.0%, 4 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 100.0%\n",
      "100.0%, 6 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 96.66666666666667%\n",
      "96.66666666666667%, 8 features\n",
      "Accuracy = 100.0%\n",
      "100.0%, 10 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 100.0%\n",
      "100.0%, 12 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 100.0%\n",
      "100.0%, 15 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 100.0%\n",
      "100.0%, 17 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 100.0%\n",
      "100.0%, 19 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 96.66666666666667%\n",
      "96.66666666666667%, 21 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 96.66666666666667%\n",
      "96.66666666666667%, 23 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 96.66666666666667%\n",
      "96.66666666666667%, 26 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 96.66666666666667%\n",
      "96.66666666666667%, 28 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 96.66666666666667%\n",
      "96.66666666666667%, 30 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 83.33333333333333%\n",
      "83.33333333333333%, 32 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 100.0%\n",
      "100.0%, 34 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 100.0%\n",
      "100.0%, 4 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 100.0%\n",
      "100.0%, 6 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 100.0%\n",
      "100.0%, 8 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 100.0%\n",
      "100.0%, 10 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 100.0%\n",
      "100.0%, 12 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 96.66666666666667%\n",
      "96.66666666666667%, 15 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 100.0%\n",
      "100.0%, 17 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 100.0%\n",
      "100.0%, 19 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 96.66666666666667%\n",
      "96.66666666666667%, 21 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 90.0%\n",
      "90.0%, 23 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 93.33333333333333%\n",
      "93.33333333333333%, 26 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 93.33333333333333%\n",
      "93.33333333333333%, 28 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 93.33333333333333%\n",
      "93.33333333333333%, 30 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 96.66666666666667%\n",
      "96.66666666666667%, 32 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 90.0%\n",
      "90.0%, 34 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 100.0%\n",
      "100.0%, 4 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 100.0%\n",
      "100.0%, 6 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 100.0%\n",
      "100.0%, 8 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 100.0%\n",
      "100.0%, 10 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 100.0%\n",
      "100.0%, 12 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 93.33333333333333%\n",
      "93.33333333333333%, 15 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 100.0%\n",
      "100.0%, 17 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 100.0%\n",
      "100.0%, 19 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 93.33333333333333%\n",
      "93.33333333333333%, 21 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 96.66666666666667%\n",
      "96.66666666666667%, 23 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 100.0%\n",
      "100.0%, 26 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 93.33333333333333%\n",
      "93.33333333333333%, 28 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 90.0%\n",
      "90.0%, 30 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 93.33333333333333%\n",
      "93.33333333333333%, 32 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 90.0%\n",
      "90.0%, 34 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 100.0%\n",
      "100.0%, 4 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 100.0%\n",
      "100.0%, 6 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 96.66666666666667%\n",
      "96.66666666666667%, 8 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 96.66666666666667%\n",
      "96.66666666666667%, 10 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 100.0%\n",
      "100.0%, 12 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 100.0%\n",
      "100.0%, 15 features\n",
      "Accuracy = 100.0%\n",
      "100.0%, 17 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 100.0%\n",
      "100.0%, 19 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 96.66666666666667%\n",
      "96.66666666666667%, 21 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 96.66666666666667%\n",
      "96.66666666666667%, 23 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 96.66666666666667%\n",
      "96.66666666666667%, 26 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 93.33333333333333%\n",
      "93.33333333333333%, 28 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 90.0%\n",
      "90.0%, 30 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 93.33333333333333%\n",
      "93.33333333333333%, 32 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 100.0%\n",
      "100.0%, 34 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 100.0%\n",
      "100.0%, 4 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 100.0%\n",
      "100.0%, 6 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 100.0%\n",
      "100.0%, 8 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 100.0%\n",
      "100.0%, 10 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 100.0%\n",
      "100.0%, 12 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 96.66666666666667%\n",
      "96.66666666666667%, 15 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 100.0%\n",
      "100.0%, 17 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 96.66666666666667%\n",
      "96.66666666666667%, 19 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 93.33333333333333%\n",
      "93.33333333333333%, 21 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 93.33333333333333%\n",
      "93.33333333333333%, 23 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 96.66666666666667%\n",
      "96.66666666666667%, 26 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 96.66666666666667%\n",
      "96.66666666666667%, 28 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 90.0%\n",
      "90.0%, 30 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 96.66666666666667%\n",
      "96.66666666666667%, 32 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 93.33333333333333%\n",
      "93.33333333333333%, 34 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 100.0%\n",
      "100.0%, 4 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 100.0%\n",
      "100.0%, 6 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 100.0%\n",
      "100.0%, 8 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 100.0%\n",
      "100.0%, 10 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 100.0%\n",
      "100.0%, 12 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 96.66666666666667%\n",
      "96.66666666666667%, 15 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 93.33333333333333%\n",
      "93.33333333333333%, 17 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 100.0%\n",
      "100.0%, 19 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 96.66666666666667%\n",
      "96.66666666666667%, 21 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 100.0%\n",
      "100.0%, 23 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 93.33333333333333%\n",
      "93.33333333333333%, 26 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 93.33333333333333%\n",
      "93.33333333333333%, 28 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 86.66666666666667%\n",
      "86.66666666666667%, 30 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 90.0%\n",
      "90.0%, 32 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 96.66666666666667%\n",
      "96.66666666666667%, 34 features\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from Gini import gini_filter\n",
    "gini_half_results = {\n",
    "    '# features': [],\n",
    "    \"accuracy\": []\n",
    "}\n",
    "for t in range(10):\n",
    "    accs = []\n",
    "    for i in range(len(num_normal)):\n",
    "        clf = MLPClassifier(\n",
    "                hidden_layer_sizes=(16, 16),\n",
    "                activation='relu',\n",
    "                solver='adam',\n",
    "                max_iter=1000)\n",
    "        X_train, X_test, Y_train, Y_test = insert_feature_noise(X, y, num_random_noise=num_normal[i],\n",
    "            num_overwhelemed=num_overwhelmed[i], num_shortcut=num_shortcut[i])\n",
    "        \n",
    "        X_train, X_test, Y_train, Y_test = gini_filter(torch.tensor(X_train.values), torch.tensor(X_test.values), \n",
    "            torch.tensor(Y_train, dtype=torch.int64), \n",
    "            torch.tensor(Y_test, dtype=torch.int64), \n",
    "            left=0.5)\n",
    "        X_train, X_test, Y_train, Y_test = X_train.numpy(), X_test.numpy(), Y_train.numpy(), Y_test.numpy()\n",
    "        # print(X_train.shape)\n",
    "        \n",
    "        clf.fit(X_train, Y_train)\n",
    "        acc = print_accuracy(clf.predict, X_test, Y_test)\n",
    "        gini_half_results[\"accuracy\"].append(acc)\n",
    "        total_feature = 2*num_normal[i] + num_overwhelmed[i] + num_shortcut[i]\n",
    "        gini_half_results['# features'].append(total_feature)\n",
    "        print(f\"{acc}%, {total_feature} features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 96.66666666666667%\n",
      "96.66666666666667%, 8 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 100.0%\n",
      "100.0%, 10 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 96.66666666666667%\n",
      "96.66666666666667%, 12 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 100.0%\n",
      "100.0%, 14 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 100.0%\n",
      "100.0%, 16 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 93.33333333333333%\n",
      "93.33333333333333%, 19 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 96.66666666666667%\n",
      "96.66666666666667%, 21 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 96.66666666666667%\n",
      "96.66666666666667%, 23 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 93.33333333333333%\n",
      "93.33333333333333%, 25 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 93.33333333333333%\n",
      "93.33333333333333%, 27 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 83.33333333333333%\n",
      "83.33333333333333%, 30 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 90.0%\n",
      "90.0%, 32 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 86.66666666666667%\n",
      "86.66666666666667%, 34 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 80.0%\n",
      "80.0%, 36 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 73.33333333333333%\n",
      "73.33333333333333%, 38 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 96.66666666666667%\n",
      "96.66666666666667%, 8 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 100.0%\n",
      "100.0%, 10 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 100.0%\n",
      "100.0%, 12 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 100.0%\n",
      "100.0%, 14 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 93.33333333333333%\n",
      "93.33333333333333%, 16 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 93.33333333333333%\n",
      "93.33333333333333%, 19 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 90.0%\n",
      "90.0%, 21 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 93.33333333333333%\n",
      "93.33333333333333%, 23 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 93.33333333333333%\n",
      "93.33333333333333%, 25 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 90.0%\n",
      "90.0%, 27 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 93.33333333333333%\n",
      "93.33333333333333%, 30 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 80.0%\n",
      "80.0%, 32 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 83.33333333333333%\n",
      "83.33333333333333%, 34 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 76.66666666666667%\n",
      "76.66666666666667%, 36 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 83.33333333333333%\n",
      "83.33333333333333%, 38 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 96.66666666666667%\n",
      "96.66666666666667%, 8 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 100.0%\n",
      "100.0%, 10 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 96.66666666666667%\n",
      "96.66666666666667%, 12 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 100.0%\n",
      "100.0%, 14 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 100.0%\n",
      "100.0%, 16 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 93.33333333333333%\n",
      "93.33333333333333%, 19 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 86.66666666666667%\n",
      "86.66666666666667%, 21 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 90.0%\n",
      "90.0%, 23 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 83.33333333333333%\n",
      "83.33333333333333%, 25 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 90.0%\n",
      "90.0%, 27 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 93.33333333333333%\n",
      "93.33333333333333%, 30 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 86.66666666666667%\n",
      "86.66666666666667%, 32 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 73.33333333333333%\n",
      "73.33333333333333%, 34 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 76.66666666666667%\n",
      "76.66666666666667%, 36 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 80.0%\n",
      "80.0%, 38 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 96.66666666666667%\n",
      "96.66666666666667%, 8 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 100.0%\n",
      "100.0%, 10 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 93.33333333333333%\n",
      "93.33333333333333%, 12 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 96.66666666666667%\n",
      "96.66666666666667%, 14 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 96.66666666666667%\n",
      "96.66666666666667%, 16 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 93.33333333333333%\n",
      "93.33333333333333%, 19 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 100.0%\n",
      "100.0%, 21 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 90.0%\n",
      "90.0%, 23 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 90.0%\n",
      "90.0%, 25 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 90.0%\n",
      "90.0%, 27 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 80.0%\n",
      "80.0%, 30 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 93.33333333333333%\n",
      "93.33333333333333%, 32 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 100.0%\n",
      "100.0%, 34 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 83.33333333333333%\n",
      "83.33333333333333%, 36 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 73.33333333333333%\n",
      "73.33333333333333%, 38 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 96.66666666666667%\n",
      "96.66666666666667%, 8 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 100.0%\n",
      "100.0%, 10 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 96.66666666666667%\n",
      "96.66666666666667%, 12 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 100.0%\n",
      "100.0%, 14 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 96.66666666666667%\n",
      "96.66666666666667%, 16 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 96.66666666666667%\n",
      "96.66666666666667%, 19 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 96.66666666666667%\n",
      "96.66666666666667%, 21 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 96.66666666666667%\n",
      "96.66666666666667%, 23 features\n",
      "Accuracy = 86.66666666666667%\n",
      "86.66666666666667%, 25 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 100.0%\n",
      "100.0%, 27 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 93.33333333333333%\n",
      "93.33333333333333%, 30 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 90.0%\n",
      "90.0%, 32 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 90.0%\n",
      "90.0%, 34 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 83.33333333333333%\n",
      "83.33333333333333%, 36 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 80.0%\n",
      "80.0%, 38 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 96.66666666666667%\n",
      "96.66666666666667%, 8 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 96.66666666666667%\n",
      "96.66666666666667%, 10 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 93.33333333333333%\n",
      "93.33333333333333%, 12 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 100.0%\n",
      "100.0%, 14 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 96.66666666666667%\n",
      "96.66666666666667%, 16 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 96.66666666666667%\n",
      "96.66666666666667%, 19 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 100.0%\n",
      "100.0%, 21 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 90.0%\n",
      "90.0%, 23 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 96.66666666666667%\n",
      "96.66666666666667%, 25 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 83.33333333333333%\n",
      "83.33333333333333%, 27 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 90.0%\n",
      "90.0%, 30 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 86.66666666666667%\n",
      "86.66666666666667%, 32 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 93.33333333333333%\n",
      "93.33333333333333%, 34 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 96.66666666666667%\n",
      "96.66666666666667%, 36 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 86.66666666666667%\n",
      "86.66666666666667%, 38 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 96.66666666666667%\n",
      "96.66666666666667%, 8 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 100.0%\n",
      "100.0%, 10 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 100.0%\n",
      "100.0%, 12 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 100.0%\n",
      "100.0%, 14 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 100.0%\n",
      "100.0%, 16 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 93.33333333333333%\n",
      "93.33333333333333%, 19 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 100.0%\n",
      "100.0%, 21 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 100.0%\n",
      "100.0%, 23 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 96.66666666666667%\n",
      "96.66666666666667%, 25 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 86.66666666666667%\n",
      "86.66666666666667%, 27 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 93.33333333333333%\n",
      "93.33333333333333%, 30 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 83.33333333333333%\n",
      "83.33333333333333%, 32 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 83.33333333333333%\n",
      "83.33333333333333%, 34 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 83.33333333333333%\n",
      "83.33333333333333%, 36 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 80.0%\n",
      "80.0%, 38 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 96.66666666666667%\n",
      "96.66666666666667%, 8 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 100.0%\n",
      "100.0%, 10 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 96.66666666666667%\n",
      "96.66666666666667%, 12 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 93.33333333333333%\n",
      "93.33333333333333%, 14 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 96.66666666666667%\n",
      "96.66666666666667%, 16 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 100.0%\n",
      "100.0%, 19 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 100.0%\n",
      "100.0%, 21 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 93.33333333333333%\n",
      "93.33333333333333%, 23 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 93.33333333333333%\n",
      "93.33333333333333%, 25 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 100.0%\n",
      "100.0%, 27 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 90.0%\n",
      "90.0%, 30 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 93.33333333333333%\n",
      "93.33333333333333%, 32 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 86.66666666666667%\n",
      "86.66666666666667%, 34 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 83.33333333333333%\n",
      "83.33333333333333%, 36 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 83.33333333333333%\n",
      "83.33333333333333%, 38 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 96.66666666666667%\n",
      "96.66666666666667%, 8 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 100.0%\n",
      "100.0%, 10 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 100.0%\n",
      "100.0%, 12 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 96.66666666666667%\n",
      "96.66666666666667%, 14 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 100.0%\n",
      "100.0%, 16 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 100.0%\n",
      "100.0%, 19 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 93.33333333333333%\n",
      "93.33333333333333%, 21 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 100.0%\n",
      "100.0%, 23 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 93.33333333333333%\n",
      "93.33333333333333%, 25 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 86.66666666666667%\n",
      "86.66666666666667%, 27 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 86.66666666666667%\n",
      "86.66666666666667%, 30 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 83.33333333333333%\n",
      "83.33333333333333%, 32 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 83.33333333333333%\n",
      "83.33333333333333%, 34 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 86.66666666666667%\n",
      "86.66666666666667%, 36 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 83.33333333333333%\n",
      "83.33333333333333%, 38 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 96.66666666666667%\n",
      "96.66666666666667%, 8 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 100.0%\n",
      "100.0%, 10 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 93.33333333333333%\n",
      "93.33333333333333%, 12 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 96.66666666666667%\n",
      "96.66666666666667%, 14 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 100.0%\n",
      "100.0%, 16 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 96.66666666666667%\n",
      "96.66666666666667%, 19 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 90.0%\n",
      "90.0%, 21 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 86.66666666666667%\n",
      "86.66666666666667%, 23 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 93.33333333333333%\n",
      "93.33333333333333%, 25 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 93.33333333333333%\n",
      "93.33333333333333%, 27 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 73.33333333333333%\n",
      "73.33333333333333%, 30 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 90.0%\n",
      "90.0%, 32 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 93.33333333333333%\n",
      "93.33333333333333%, 34 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 80.0%\n",
      "80.0%, 36 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 83.33333333333333%\n",
      "83.33333333333333%, 38 features\n"
     ]
    }
   ],
   "source": [
    "nofs_results = {\n",
    "    '# features': [],\n",
    "    \"accuracy\": []\n",
    "}\n",
    "for t in range(10):\n",
    "    accs = []\n",
    "    for i in range(len(num_normal)):\n",
    "        clf = MLPClassifier(\n",
    "                hidden_layer_sizes=(16, 16),\n",
    "                activation='relu',\n",
    "                solver='adam',\n",
    "                max_iter=1000)\n",
    "        X_train, X_test, Y_train, Y_test = insert_feature_noise(X, y, num_random_noise=num_normal[i],\n",
    "            num_overwhelemed=num_overwhelmed[i], num_shortcut=num_shortcut[i])\n",
    "        clf.fit(X_train, Y_train)\n",
    "        acc = print_accuracy(clf.predict, X_test, Y_test)\n",
    "        nofs_results[\"accuracy\"].append(acc)\n",
    "        total_feature = 2*num_normal[i] + num_overwhelmed[i] + num_shortcut[i]\n",
    "        nofs_results['# features'].append(total_feature)\n",
    "        print(f\"{acc}%, {total_feature+4} features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='# features', ylabel='accuracy'>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\r\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\r\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\r\n<svg height=\"262.19625pt\" version=\"1.1\" viewBox=\"0 0 390.255398 262.19625\" width=\"390.255398pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\r\n <metadata>\r\n  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\r\n   <cc:Work>\r\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\r\n    <dc:date>2022-03-29T18:42:23.081247</dc:date>\r\n    <dc:format>image/svg+xml</dc:format>\r\n    <dc:creator>\r\n     <cc:Agent>\r\n      <dc:title>Matplotlib v3.4.3, https://matplotlib.org/</dc:title>\r\n     </cc:Agent>\r\n    </dc:creator>\r\n   </cc:Work>\r\n  </rdf:RDF>\r\n </metadata>\r\n <defs>\r\n  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\r\n </defs>\r\n <g id=\"figure_1\">\r\n  <g id=\"patch_1\">\r\n   <path d=\"M 0 262.19625 \r\nL 390.255398 262.19625 \r\nL 390.255398 0 \r\nL 0 0 \r\nz\r\n\" style=\"fill:none;\"/>\r\n  </g>\r\n  <g id=\"axes_1\">\r\n   <g id=\"patch_2\">\r\n    <path d=\"M 46.965625 224.64 \r\nL 381.765625 224.64 \r\nL 381.765625 7.2 \r\nL 46.965625 7.2 \r\nz\r\n\" style=\"fill:#ffffff;\"/>\r\n   </g>\r\n   <g id=\"PolyCollection_1\">\r\n    <defs>\r\n     <path d=\"M 62.183807 -215.609222 \r\nL 62.183807 -215.609222 \r\nL 82.474716 -236.261596 \r\nL 102.765625 -200.857525 \r\nL 123.056534 -218.559561 \r\nL 143.347443 -212.658882 \r\nL 173.783807 -194.956847 \r\nL 194.074716 -177.254812 \r\nL 214.365625 -165.453455 \r\nL 234.656534 -150.701759 \r\nL 254.947443 -141.850741 \r\nL 285.383807 -100.472234 \r\nL 305.674716 -112.347349 \r\nL 325.965625 -94.645314 \r\nL 346.256534 -68.092261 \r\nL 366.547443 -47.439886 \r\nL 366.547443 -94.645314 \r\nL 366.547443 -94.645314 \r\nL 346.256534 -130.049384 \r\nL 325.965625 -165.453455 \r\nL 305.674716 -159.552776 \r\nL 285.383807 -168.403794 \r\nL 254.947443 -197.907186 \r\nL 234.656534 -194.956847 \r\nL 214.365625 -212.658882 \r\nL 194.074716 -227.410578 \r\nL 173.783807 -221.5099 \r\nL 143.347443 -239.211935 \r\nL 123.056534 -242.162274 \r\nL 102.765625 -230.360918 \r\nL 82.474716 -245.112614 \r\nL 62.183807 -215.609222 \r\nz\r\n\" id=\"m85a00fd999\" style=\"stroke:#1f77b4;stroke-opacity:0.2;\"/>\r\n    </defs>\r\n    <g clip-path=\"url(#pcf677a4c23)\">\r\n     <use style=\"fill:#1f77b4;fill-opacity:0.2;stroke:#1f77b4;stroke-opacity:0.2;\" x=\"0\" xlink:href=\"#m85a00fd999\" y=\"262.19625\"/>\r\n    </g>\r\n   </g>\r\n   <g id=\"PolyCollection_2\">\r\n    <defs>\r\n     <path d=\"M 62.183807 -245.112614 \r\nL 62.183807 -245.112614 \r\nL 82.474716 -245.112614 \r\nL 102.765625 -227.410578 \r\nL 123.056534 -230.360918 \r\nL 143.347443 -236.261596 \r\nL 173.783807 -200.857525 \r\nL 194.074716 -221.5099 \r\nL 214.365625 -218.559561 \r\nL 234.656534 -197.907186 \r\nL 254.947443 -194.956847 \r\nL 285.383807 -183.15549 \r\nL 305.674716 -197.833428 \r\nL 325.965625 -165.453455 \r\nL 346.256534 -141.850741 \r\nL 366.547443 -159.552776 \r\nL 366.547443 -212.658882 \r\nL 366.547443 -212.658882 \r\nL 346.256534 -197.980945 \r\nL 325.965625 -203.807865 \r\nL 305.674716 -224.460239 \r\nL 285.383807 -212.658882 \r\nL 254.947443 -227.410578 \r\nL 234.656534 -215.609222 \r\nL 214.365625 -242.162274 \r\nL 194.074716 -245.112614 \r\nL 173.783807 -230.360918 \r\nL 143.347443 -245.112614 \r\nL 123.056534 -245.112614 \r\nL 102.765625 -245.112614 \r\nL 82.474716 -245.112614 \r\nL 62.183807 -245.112614 \r\nz\r\n\" id=\"m18424855bc\" style=\"stroke:#ff7f0e;stroke-opacity:0.2;\"/>\r\n    </defs>\r\n    <g clip-path=\"url(#pcf677a4c23)\">\r\n     <use style=\"fill:#ff7f0e;fill-opacity:0.2;stroke:#ff7f0e;stroke-opacity:0.2;\" x=\"0\" xlink:href=\"#m18424855bc\" y=\"262.19625\"/>\r\n    </g>\r\n   </g>\r\n   <g id=\"PolyCollection_3\">\r\n    <defs>\r\n     <path d=\"M 62.183807 -245.112614 \r\nL 62.183807 -230.360918 \r\nL 82.474716 -124.148706 \r\nL 102.765625 -186.105829 \r\nL 123.056534 -224.460239 \r\nL 143.347443 -174.304473 \r\nL 173.783807 -203.734106 \r\nL 194.074716 -215.609222 \r\nL 214.365625 -200.857525 \r\nL 234.656534 -203.807865 \r\nL 254.947443 -203.807865 \r\nL 285.383807 -209.708543 \r\nL 305.674716 -212.658882 \r\nL 325.965625 -189.056169 \r\nL 346.256534 -209.708543 \r\nL 366.547443 -221.5099 \r\nL 366.547443 -239.211935 \r\nL 366.547443 -239.211935 \r\nL 346.256534 -242.162274 \r\nL 325.965625 -224.460239 \r\nL 305.674716 -236.261596 \r\nL 285.383807 -236.335354 \r\nL 254.947443 -233.311257 \r\nL 234.656534 -221.5099 \r\nL 214.365625 -227.410578 \r\nL 194.074716 -239.211935 \r\nL 173.783807 -221.5099 \r\nL 143.347443 -242.162274 \r\nL 123.056534 -242.162274 \r\nL 102.765625 -236.261596 \r\nL 82.474716 -230.360918 \r\nL 62.183807 -245.112614 \r\nz\r\n\" id=\"m19119b793f\" style=\"stroke:#2ca02c;stroke-opacity:0.2;\"/>\r\n    </defs>\r\n    <g clip-path=\"url(#pcf677a4c23)\">\r\n     <use style=\"fill:#2ca02c;fill-opacity:0.2;stroke:#2ca02c;stroke-opacity:0.2;\" x=\"0\" xlink:href=\"#m19119b793f\" y=\"262.19625\"/>\r\n    </g>\r\n   </g>\r\n   <g id=\"matplotlib.axis_1\">\r\n    <g id=\"xtick_1\">\r\n     <g id=\"line2d_1\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL 0 3.5 \r\n\" id=\"mff4a112ec4\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"72.329261\" xlink:href=\"#mff4a112ec4\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_1\">\r\n      <!-- 5 -->\r\n      <g transform=\"translate(69.148011 239.238437)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 691 4666 \r\nL 3169 4666 \r\nL 3169 4134 \r\nL 1269 4134 \r\nL 1269 2991 \r\nQ 1406 3038 1543 3061 \r\nQ 1681 3084 1819 3084 \r\nQ 2600 3084 3056 2656 \r\nQ 3513 2228 3513 1497 \r\nQ 3513 744 3044 326 \r\nQ 2575 -91 1722 -91 \r\nQ 1428 -91 1123 -41 \r\nQ 819 9 494 109 \r\nL 494 744 \r\nQ 775 591 1075 516 \r\nQ 1375 441 1709 441 \r\nQ 2250 441 2565 725 \r\nQ 2881 1009 2881 1497 \r\nQ 2881 1984 2565 2268 \r\nQ 2250 2553 1709 2553 \r\nQ 1456 2553 1204 2497 \r\nQ 953 2441 691 2322 \r\nL 691 4666 \r\nz\r\n\" id=\"DejaVuSans-35\" transform=\"scale(0.015625)\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-35\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_2\">\r\n     <g id=\"line2d_2\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"123.056534\" xlink:href=\"#mff4a112ec4\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_2\">\r\n      <!-- 10 -->\r\n      <g transform=\"translate(116.694034 239.238437)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 794 531 \r\nL 1825 531 \r\nL 1825 4091 \r\nL 703 3866 \r\nL 703 4441 \r\nL 1819 4666 \r\nL 2450 4666 \r\nL 2450 531 \r\nL 3481 531 \r\nL 3481 0 \r\nL 794 0 \r\nL 794 531 \r\nz\r\n\" id=\"DejaVuSans-31\" transform=\"scale(0.015625)\"/>\r\n        <path d=\"M 2034 4250 \r\nQ 1547 4250 1301 3770 \r\nQ 1056 3291 1056 2328 \r\nQ 1056 1369 1301 889 \r\nQ 1547 409 2034 409 \r\nQ 2525 409 2770 889 \r\nQ 3016 1369 3016 2328 \r\nQ 3016 3291 2770 3770 \r\nQ 2525 4250 2034 4250 \r\nz\r\nM 2034 4750 \r\nQ 2819 4750 3233 4129 \r\nQ 3647 3509 3647 2328 \r\nQ 3647 1150 3233 529 \r\nQ 2819 -91 2034 -91 \r\nQ 1250 -91 836 529 \r\nQ 422 1150 422 2328 \r\nQ 422 3509 836 4129 \r\nQ 1250 4750 2034 4750 \r\nz\r\n\" id=\"DejaVuSans-30\" transform=\"scale(0.015625)\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-31\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_3\">\r\n     <g id=\"line2d_3\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"173.783807\" xlink:href=\"#mff4a112ec4\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_3\">\r\n      <!-- 15 -->\r\n      <g transform=\"translate(167.421307 239.238437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-31\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-35\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_4\">\r\n     <g id=\"line2d_4\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"224.51108\" xlink:href=\"#mff4a112ec4\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_4\">\r\n      <!-- 20 -->\r\n      <g transform=\"translate(218.14858 239.238437)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 1228 531 \r\nL 3431 531 \r\nL 3431 0 \r\nL 469 0 \r\nL 469 531 \r\nQ 828 903 1448 1529 \r\nQ 2069 2156 2228 2338 \r\nQ 2531 2678 2651 2914 \r\nQ 2772 3150 2772 3378 \r\nQ 2772 3750 2511 3984 \r\nQ 2250 4219 1831 4219 \r\nQ 1534 4219 1204 4116 \r\nQ 875 4013 500 3803 \r\nL 500 4441 \r\nQ 881 4594 1212 4672 \r\nQ 1544 4750 1819 4750 \r\nQ 2544 4750 2975 4387 \r\nQ 3406 4025 3406 3419 \r\nQ 3406 3131 3298 2873 \r\nQ 3191 2616 2906 2266 \r\nQ 2828 2175 2409 1742 \r\nQ 1991 1309 1228 531 \r\nz\r\n\" id=\"DejaVuSans-32\" transform=\"scale(0.015625)\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-32\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_5\">\r\n     <g id=\"line2d_5\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"275.238352\" xlink:href=\"#mff4a112ec4\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_5\">\r\n      <!-- 25 -->\r\n      <g transform=\"translate(268.875852 239.238437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-32\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-35\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_6\">\r\n     <g id=\"line2d_6\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"325.965625\" xlink:href=\"#mff4a112ec4\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_6\">\r\n      <!-- 30 -->\r\n      <g transform=\"translate(319.603125 239.238437)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 2597 2516 \r\nQ 3050 2419 3304 2112 \r\nQ 3559 1806 3559 1356 \r\nQ 3559 666 3084 287 \r\nQ 2609 -91 1734 -91 \r\nQ 1441 -91 1130 -33 \r\nQ 819 25 488 141 \r\nL 488 750 \r\nQ 750 597 1062 519 \r\nQ 1375 441 1716 441 \r\nQ 2309 441 2620 675 \r\nQ 2931 909 2931 1356 \r\nQ 2931 1769 2642 2001 \r\nQ 2353 2234 1838 2234 \r\nL 1294 2234 \r\nL 1294 2753 \r\nL 1863 2753 \r\nQ 2328 2753 2575 2939 \r\nQ 2822 3125 2822 3475 \r\nQ 2822 3834 2567 4026 \r\nQ 2313 4219 1838 4219 \r\nQ 1578 4219 1281 4162 \r\nQ 984 4106 628 3988 \r\nL 628 4550 \r\nQ 988 4650 1302 4700 \r\nQ 1616 4750 1894 4750 \r\nQ 2613 4750 3031 4423 \r\nQ 3450 4097 3450 3541 \r\nQ 3450 3153 3228 2886 \r\nQ 3006 2619 2597 2516 \r\nz\r\n\" id=\"DejaVuSans-33\" transform=\"scale(0.015625)\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-33\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_7\">\r\n     <g id=\"line2d_7\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"376.692898\" xlink:href=\"#mff4a112ec4\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_7\">\r\n      <!-- 35 -->\r\n      <g transform=\"translate(370.330398 239.238437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-33\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-35\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"text_8\">\r\n     <!-- # features -->\r\n     <g transform=\"translate(187.932813 252.916562)scale(0.1 -0.1)\">\r\n      <defs>\r\n       <path d=\"M 3272 2816 \r\nL 2363 2816 \r\nL 2100 1772 \r\nL 3016 1772 \r\nL 3272 2816 \r\nz\r\nM 2803 4594 \r\nL 2478 3297 \r\nL 3391 3297 \r\nL 3719 4594 \r\nL 4219 4594 \r\nL 3897 3297 \r\nL 4872 3297 \r\nL 4872 2816 \r\nL 3775 2816 \r\nL 3519 1772 \r\nL 4513 1772 \r\nL 4513 1294 \r\nL 3397 1294 \r\nL 3072 0 \r\nL 2572 0 \r\nL 2894 1294 \r\nL 1978 1294 \r\nL 1656 0 \r\nL 1153 0 \r\nL 1478 1294 \r\nL 494 1294 \r\nL 494 1772 \r\nL 1594 1772 \r\nL 1856 2816 \r\nL 850 2816 \r\nL 850 3297 \r\nL 1978 3297 \r\nL 2297 4594 \r\nL 2803 4594 \r\nz\r\n\" id=\"DejaVuSans-23\" transform=\"scale(0.015625)\"/>\r\n       <path id=\"DejaVuSans-20\" transform=\"scale(0.015625)\"/>\r\n       <path d=\"M 2375 4863 \r\nL 2375 4384 \r\nL 1825 4384 \r\nQ 1516 4384 1395 4259 \r\nQ 1275 4134 1275 3809 \r\nL 1275 3500 \r\nL 2222 3500 \r\nL 2222 3053 \r\nL 1275 3053 \r\nL 1275 0 \r\nL 697 0 \r\nL 697 3053 \r\nL 147 3053 \r\nL 147 3500 \r\nL 697 3500 \r\nL 697 3744 \r\nQ 697 4328 969 4595 \r\nQ 1241 4863 1831 4863 \r\nL 2375 4863 \r\nz\r\n\" id=\"DejaVuSans-66\" transform=\"scale(0.015625)\"/>\r\n       <path d=\"M 3597 1894 \r\nL 3597 1613 \r\nL 953 1613 \r\nQ 991 1019 1311 708 \r\nQ 1631 397 2203 397 \r\nQ 2534 397 2845 478 \r\nQ 3156 559 3463 722 \r\nL 3463 178 \r\nQ 3153 47 2828 -22 \r\nQ 2503 -91 2169 -91 \r\nQ 1331 -91 842 396 \r\nQ 353 884 353 1716 \r\nQ 353 2575 817 3079 \r\nQ 1281 3584 2069 3584 \r\nQ 2775 3584 3186 3129 \r\nQ 3597 2675 3597 1894 \r\nz\r\nM 3022 2063 \r\nQ 3016 2534 2758 2815 \r\nQ 2500 3097 2075 3097 \r\nQ 1594 3097 1305 2825 \r\nQ 1016 2553 972 2059 \r\nL 3022 2063 \r\nz\r\n\" id=\"DejaVuSans-65\" transform=\"scale(0.015625)\"/>\r\n       <path d=\"M 2194 1759 \r\nQ 1497 1759 1228 1600 \r\nQ 959 1441 959 1056 \r\nQ 959 750 1161 570 \r\nQ 1363 391 1709 391 \r\nQ 2188 391 2477 730 \r\nQ 2766 1069 2766 1631 \r\nL 2766 1759 \r\nL 2194 1759 \r\nz\r\nM 3341 1997 \r\nL 3341 0 \r\nL 2766 0 \r\nL 2766 531 \r\nQ 2569 213 2275 61 \r\nQ 1981 -91 1556 -91 \r\nQ 1019 -91 701 211 \r\nQ 384 513 384 1019 \r\nQ 384 1609 779 1909 \r\nQ 1175 2209 1959 2209 \r\nL 2766 2209 \r\nL 2766 2266 \r\nQ 2766 2663 2505 2880 \r\nQ 2244 3097 1772 3097 \r\nQ 1472 3097 1187 3025 \r\nQ 903 2953 641 2809 \r\nL 641 3341 \r\nQ 956 3463 1253 3523 \r\nQ 1550 3584 1831 3584 \r\nQ 2591 3584 2966 3190 \r\nQ 3341 2797 3341 1997 \r\nz\r\n\" id=\"DejaVuSans-61\" transform=\"scale(0.015625)\"/>\r\n       <path d=\"M 1172 4494 \r\nL 1172 3500 \r\nL 2356 3500 \r\nL 2356 3053 \r\nL 1172 3053 \r\nL 1172 1153 \r\nQ 1172 725 1289 603 \r\nQ 1406 481 1766 481 \r\nL 2356 481 \r\nL 2356 0 \r\nL 1766 0 \r\nQ 1100 0 847 248 \r\nQ 594 497 594 1153 \r\nL 594 3053 \r\nL 172 3053 \r\nL 172 3500 \r\nL 594 3500 \r\nL 594 4494 \r\nL 1172 4494 \r\nz\r\n\" id=\"DejaVuSans-74\" transform=\"scale(0.015625)\"/>\r\n       <path d=\"M 544 1381 \r\nL 544 3500 \r\nL 1119 3500 \r\nL 1119 1403 \r\nQ 1119 906 1312 657 \r\nQ 1506 409 1894 409 \r\nQ 2359 409 2629 706 \r\nQ 2900 1003 2900 1516 \r\nL 2900 3500 \r\nL 3475 3500 \r\nL 3475 0 \r\nL 2900 0 \r\nL 2900 538 \r\nQ 2691 219 2414 64 \r\nQ 2138 -91 1772 -91 \r\nQ 1169 -91 856 284 \r\nQ 544 659 544 1381 \r\nz\r\nM 1991 3584 \r\nL 1991 3584 \r\nz\r\n\" id=\"DejaVuSans-75\" transform=\"scale(0.015625)\"/>\r\n       <path d=\"M 2631 2963 \r\nQ 2534 3019 2420 3045 \r\nQ 2306 3072 2169 3072 \r\nQ 1681 3072 1420 2755 \r\nQ 1159 2438 1159 1844 \r\nL 1159 0 \r\nL 581 0 \r\nL 581 3500 \r\nL 1159 3500 \r\nL 1159 2956 \r\nQ 1341 3275 1631 3429 \r\nQ 1922 3584 2338 3584 \r\nQ 2397 3584 2469 3576 \r\nQ 2541 3569 2628 3553 \r\nL 2631 2963 \r\nz\r\n\" id=\"DejaVuSans-72\" transform=\"scale(0.015625)\"/>\r\n       <path d=\"M 2834 3397 \r\nL 2834 2853 \r\nQ 2591 2978 2328 3040 \r\nQ 2066 3103 1784 3103 \r\nQ 1356 3103 1142 2972 \r\nQ 928 2841 928 2578 \r\nQ 928 2378 1081 2264 \r\nQ 1234 2150 1697 2047 \r\nL 1894 2003 \r\nQ 2506 1872 2764 1633 \r\nQ 3022 1394 3022 966 \r\nQ 3022 478 2636 193 \r\nQ 2250 -91 1575 -91 \r\nQ 1294 -91 989 -36 \r\nQ 684 19 347 128 \r\nL 347 722 \r\nQ 666 556 975 473 \r\nQ 1284 391 1588 391 \r\nQ 1994 391 2212 530 \r\nQ 2431 669 2431 922 \r\nQ 2431 1156 2273 1281 \r\nQ 2116 1406 1581 1522 \r\nL 1381 1569 \r\nQ 847 1681 609 1914 \r\nQ 372 2147 372 2553 \r\nQ 372 3047 722 3315 \r\nQ 1072 3584 1716 3584 \r\nQ 2034 3584 2315 3537 \r\nQ 2597 3491 2834 3397 \r\nz\r\n\" id=\"DejaVuSans-73\" transform=\"scale(0.015625)\"/>\r\n      </defs>\r\n      <use xlink:href=\"#DejaVuSans-23\"/>\r\n      <use x=\"83.789062\" xlink:href=\"#DejaVuSans-20\"/>\r\n      <use x=\"115.576172\" xlink:href=\"#DejaVuSans-66\"/>\r\n      <use x=\"150.78125\" xlink:href=\"#DejaVuSans-65\"/>\r\n      <use x=\"212.304688\" xlink:href=\"#DejaVuSans-61\"/>\r\n      <use x=\"273.583984\" xlink:href=\"#DejaVuSans-74\"/>\r\n      <use x=\"312.792969\" xlink:href=\"#DejaVuSans-75\"/>\r\n      <use x=\"376.171875\" xlink:href=\"#DejaVuSans-72\"/>\r\n      <use x=\"415.035156\" xlink:href=\"#DejaVuSans-65\"/>\r\n      <use x=\"476.558594\" xlink:href=\"#DejaVuSans-73\"/>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"matplotlib.axis_2\">\r\n    <g id=\"ytick_1\">\r\n     <g id=\"line2d_8\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL -3.5 0 \r\n\" id=\"me4e6c58f20\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"46.965625\" xlink:href=\"#me4e6c58f20\" y=\"194.103989\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_9\">\r\n      <!-- 80 -->\r\n      <g transform=\"translate(27.240625 197.903208)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 2034 2216 \r\nQ 1584 2216 1326 1975 \r\nQ 1069 1734 1069 1313 \r\nQ 1069 891 1326 650 \r\nQ 1584 409 2034 409 \r\nQ 2484 409 2743 651 \r\nQ 3003 894 3003 1313 \r\nQ 3003 1734 2745 1975 \r\nQ 2488 2216 2034 2216 \r\nz\r\nM 1403 2484 \r\nQ 997 2584 770 2862 \r\nQ 544 3141 544 3541 \r\nQ 544 4100 942 4425 \r\nQ 1341 4750 2034 4750 \r\nQ 2731 4750 3128 4425 \r\nQ 3525 4100 3525 3541 \r\nQ 3525 3141 3298 2862 \r\nQ 3072 2584 2669 2484 \r\nQ 3125 2378 3379 2068 \r\nQ 3634 1759 3634 1313 \r\nQ 3634 634 3220 271 \r\nQ 2806 -91 2034 -91 \r\nQ 1263 -91 848 271 \r\nQ 434 634 434 1313 \r\nQ 434 1759 690 2068 \r\nQ 947 2378 1403 2484 \r\nz\r\nM 1172 3481 \r\nQ 1172 3119 1398 2916 \r\nQ 1625 2713 2034 2713 \r\nQ 2441 2713 2670 2916 \r\nQ 2900 3119 2900 3481 \r\nQ 2900 3844 2670 4047 \r\nQ 2441 4250 2034 4250 \r\nQ 1625 4250 1398 4047 \r\nQ 1172 3844 1172 3481 \r\nz\r\n\" id=\"DejaVuSans-38\" transform=\"scale(0.015625)\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-38\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_2\">\r\n     <g id=\"line2d_9\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"46.965625\" xlink:href=\"#me4e6c58f20\" y=\"149.848901\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_10\">\r\n      <!-- 85 -->\r\n      <g transform=\"translate(27.240625 153.64812)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-38\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-35\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_3\">\r\n     <g id=\"line2d_10\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"46.965625\" xlink:href=\"#me4e6c58f20\" y=\"105.593813\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_11\">\r\n      <!-- 90 -->\r\n      <g transform=\"translate(27.240625 109.393032)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 703 97 \r\nL 703 672 \r\nQ 941 559 1184 500 \r\nQ 1428 441 1663 441 \r\nQ 2288 441 2617 861 \r\nQ 2947 1281 2994 2138 \r\nQ 2813 1869 2534 1725 \r\nQ 2256 1581 1919 1581 \r\nQ 1219 1581 811 2004 \r\nQ 403 2428 403 3163 \r\nQ 403 3881 828 4315 \r\nQ 1253 4750 1959 4750 \r\nQ 2769 4750 3195 4129 \r\nQ 3622 3509 3622 2328 \r\nQ 3622 1225 3098 567 \r\nQ 2575 -91 1691 -91 \r\nQ 1453 -91 1209 -44 \r\nQ 966 3 703 97 \r\nz\r\nM 1959 2075 \r\nQ 2384 2075 2632 2365 \r\nQ 2881 2656 2881 3163 \r\nQ 2881 3666 2632 3958 \r\nQ 2384 4250 1959 4250 \r\nQ 1534 4250 1286 3958 \r\nQ 1038 3666 1038 3163 \r\nQ 1038 2656 1286 2365 \r\nQ 1534 2075 1959 2075 \r\nz\r\n\" id=\"DejaVuSans-39\" transform=\"scale(0.015625)\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-39\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_4\">\r\n     <g id=\"line2d_11\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"46.965625\" xlink:href=\"#me4e6c58f20\" y=\"61.338725\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_12\">\r\n      <!-- 95 -->\r\n      <g transform=\"translate(27.240625 65.137943)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-39\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-35\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_5\">\r\n     <g id=\"line2d_12\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"46.965625\" xlink:href=\"#me4e6c58f20\" y=\"17.083636\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_13\">\r\n      <!-- 100 -->\r\n      <g transform=\"translate(20.878125 20.882855)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-31\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"text_14\">\r\n     <!-- accuracy -->\r\n     <g transform=\"translate(14.798438 138.479375)rotate(-90)scale(0.1 -0.1)\">\r\n      <defs>\r\n       <path d=\"M 3122 3366 \r\nL 3122 2828 \r\nQ 2878 2963 2633 3030 \r\nQ 2388 3097 2138 3097 \r\nQ 1578 3097 1268 2742 \r\nQ 959 2388 959 1747 \r\nQ 959 1106 1268 751 \r\nQ 1578 397 2138 397 \r\nQ 2388 397 2633 464 \r\nQ 2878 531 3122 666 \r\nL 3122 134 \r\nQ 2881 22 2623 -34 \r\nQ 2366 -91 2075 -91 \r\nQ 1284 -91 818 406 \r\nQ 353 903 353 1747 \r\nQ 353 2603 823 3093 \r\nQ 1294 3584 2113 3584 \r\nQ 2378 3584 2631 3529 \r\nQ 2884 3475 3122 3366 \r\nz\r\n\" id=\"DejaVuSans-63\" transform=\"scale(0.015625)\"/>\r\n       <path d=\"M 2059 -325 \r\nQ 1816 -950 1584 -1140 \r\nQ 1353 -1331 966 -1331 \r\nL 506 -1331 \r\nL 506 -850 \r\nL 844 -850 \r\nQ 1081 -850 1212 -737 \r\nQ 1344 -625 1503 -206 \r\nL 1606 56 \r\nL 191 3500 \r\nL 800 3500 \r\nL 1894 763 \r\nL 2988 3500 \r\nL 3597 3500 \r\nL 2059 -325 \r\nz\r\n\" id=\"DejaVuSans-79\" transform=\"scale(0.015625)\"/>\r\n      </defs>\r\n      <use xlink:href=\"#DejaVuSans-61\"/>\r\n      <use x=\"61.279297\" xlink:href=\"#DejaVuSans-63\"/>\r\n      <use x=\"116.259766\" xlink:href=\"#DejaVuSans-63\"/>\r\n      <use x=\"171.240234\" xlink:href=\"#DejaVuSans-75\"/>\r\n      <use x=\"234.619141\" xlink:href=\"#DejaVuSans-72\"/>\r\n      <use x=\"275.732422\" xlink:href=\"#DejaVuSans-61\"/>\r\n      <use x=\"337.011719\" xlink:href=\"#DejaVuSans-63\"/>\r\n      <use x=\"391.992188\" xlink:href=\"#DejaVuSans-79\"/>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"line2d_13\">\r\n    <path clip-path=\"url(#pcf677a4c23)\" d=\"M 62.183807 46.587028 \r\nL 82.474716 20.033976 \r\nL 102.765625 46.587028 \r\nL 123.056534 31.835332 \r\nL 143.347443 34.785672 \r\nL 173.783807 55.438046 \r\nL 194.074716 58.388385 \r\nL 214.365625 73.140081 \r\nL 234.656534 87.891777 \r\nL 254.947443 93.792456 \r\nL 285.383807 126.246187 \r\nL 305.674716 126.246187 \r\nL 325.965625 129.196526 \r\nL 346.256534 167.550936 \r\nL 366.547443 188.203311 \r\n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\r\n    <defs>\r\n     <path d=\"M 0 3 \r\nC 0.795609 3 1.55874 2.683901 2.12132 2.12132 \r\nC 2.683901 1.55874 3 0.795609 3 0 \r\nC 3 -0.795609 2.683901 -1.55874 2.12132 -2.12132 \r\nC 1.55874 -2.683901 0.795609 -3 0 -3 \r\nC -0.795609 -3 -1.55874 -2.683901 -2.12132 -2.12132 \r\nC -2.683901 -1.55874 -3 -0.795609 -3 0 \r\nC -3 0.795609 -2.683901 1.55874 -2.12132 2.12132 \r\nC -1.55874 2.683901 -0.795609 3 0 3 \r\nz\r\n\" id=\"mb7746e3537\" style=\"stroke:#ffffff;stroke-width:0.75;\"/>\r\n    </defs>\r\n    <g clip-path=\"url(#pcf677a4c23)\">\r\n     <use style=\"fill:#1f77b4;stroke:#ffffff;stroke-width:0.75;\" x=\"62.183807\" xlink:href=\"#mb7746e3537\" y=\"46.587028\"/>\r\n     <use style=\"fill:#1f77b4;stroke:#ffffff;stroke-width:0.75;\" x=\"82.474716\" xlink:href=\"#mb7746e3537\" y=\"20.033976\"/>\r\n     <use style=\"fill:#1f77b4;stroke:#ffffff;stroke-width:0.75;\" x=\"102.765625\" xlink:href=\"#mb7746e3537\" y=\"46.587028\"/>\r\n     <use style=\"fill:#1f77b4;stroke:#ffffff;stroke-width:0.75;\" x=\"123.056534\" xlink:href=\"#mb7746e3537\" y=\"31.835332\"/>\r\n     <use style=\"fill:#1f77b4;stroke:#ffffff;stroke-width:0.75;\" x=\"143.347443\" xlink:href=\"#mb7746e3537\" y=\"34.785672\"/>\r\n     <use style=\"fill:#1f77b4;stroke:#ffffff;stroke-width:0.75;\" x=\"173.783807\" xlink:href=\"#mb7746e3537\" y=\"55.438046\"/>\r\n     <use style=\"fill:#1f77b4;stroke:#ffffff;stroke-width:0.75;\" x=\"194.074716\" xlink:href=\"#mb7746e3537\" y=\"58.388385\"/>\r\n     <use style=\"fill:#1f77b4;stroke:#ffffff;stroke-width:0.75;\" x=\"214.365625\" xlink:href=\"#mb7746e3537\" y=\"73.140081\"/>\r\n     <use style=\"fill:#1f77b4;stroke:#ffffff;stroke-width:0.75;\" x=\"234.656534\" xlink:href=\"#mb7746e3537\" y=\"87.891777\"/>\r\n     <use style=\"fill:#1f77b4;stroke:#ffffff;stroke-width:0.75;\" x=\"254.947443\" xlink:href=\"#mb7746e3537\" y=\"93.792456\"/>\r\n     <use style=\"fill:#1f77b4;stroke:#ffffff;stroke-width:0.75;\" x=\"285.383807\" xlink:href=\"#mb7746e3537\" y=\"126.246187\"/>\r\n     <use style=\"fill:#1f77b4;stroke:#ffffff;stroke-width:0.75;\" x=\"305.674716\" xlink:href=\"#mb7746e3537\" y=\"126.246187\"/>\r\n     <use style=\"fill:#1f77b4;stroke:#ffffff;stroke-width:0.75;\" x=\"325.965625\" xlink:href=\"#mb7746e3537\" y=\"129.196526\"/>\r\n     <use style=\"fill:#1f77b4;stroke:#ffffff;stroke-width:0.75;\" x=\"346.256534\" xlink:href=\"#mb7746e3537\" y=\"167.550936\"/>\r\n     <use style=\"fill:#1f77b4;stroke:#ffffff;stroke-width:0.75;\" x=\"366.547443\" xlink:href=\"#mb7746e3537\" y=\"188.203311\"/>\r\n    </g>\r\n   </g>\r\n   <g id=\"line2d_14\">\r\n    <path clip-path=\"url(#pcf677a4c23)\" d=\"M 62.183807 17.083636 \r\nL 82.474716 17.083636 \r\nL 102.765625 25.934654 \r\nL 123.056534 22.984315 \r\nL 143.347443 20.033976 \r\nL 173.783807 46.587028 \r\nL 194.074716 25.934654 \r\nL 214.365625 31.835332 \r\nL 234.656534 55.438046 \r\nL 254.947443 49.537368 \r\nL 285.383807 64.289064 \r\nL 305.674716 52.487707 \r\nL 325.965625 76.090421 \r\nL 346.256534 90.842117 \r\nL 366.547443 76.090421 \r\n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\r\n    <defs>\r\n     <path d=\"M 0 3 \r\nC 0.795609 3 1.55874 2.683901 2.12132 2.12132 \r\nC 2.683901 1.55874 3 0.795609 3 0 \r\nC 3 -0.795609 2.683901 -1.55874 2.12132 -2.12132 \r\nC 1.55874 -2.683901 0.795609 -3 0 -3 \r\nC -0.795609 -3 -1.55874 -2.683901 -2.12132 -2.12132 \r\nC -2.683901 -1.55874 -3 -0.795609 -3 0 \r\nC -3 0.795609 -2.683901 1.55874 -2.12132 2.12132 \r\nC -1.55874 2.683901 -0.795609 3 0 3 \r\nz\r\n\" id=\"m5c18cb85f8\" style=\"stroke:#ffffff;stroke-width:0.75;\"/>\r\n    </defs>\r\n    <g clip-path=\"url(#pcf677a4c23)\">\r\n     <use style=\"fill:#ff7f0e;stroke:#ffffff;stroke-width:0.75;\" x=\"62.183807\" xlink:href=\"#m5c18cb85f8\" y=\"17.083636\"/>\r\n     <use style=\"fill:#ff7f0e;stroke:#ffffff;stroke-width:0.75;\" x=\"82.474716\" xlink:href=\"#m5c18cb85f8\" y=\"17.083636\"/>\r\n     <use style=\"fill:#ff7f0e;stroke:#ffffff;stroke-width:0.75;\" x=\"102.765625\" xlink:href=\"#m5c18cb85f8\" y=\"25.934654\"/>\r\n     <use style=\"fill:#ff7f0e;stroke:#ffffff;stroke-width:0.75;\" x=\"123.056534\" xlink:href=\"#m5c18cb85f8\" y=\"22.984315\"/>\r\n     <use style=\"fill:#ff7f0e;stroke:#ffffff;stroke-width:0.75;\" x=\"143.347443\" xlink:href=\"#m5c18cb85f8\" y=\"20.033976\"/>\r\n     <use style=\"fill:#ff7f0e;stroke:#ffffff;stroke-width:0.75;\" x=\"173.783807\" xlink:href=\"#m5c18cb85f8\" y=\"46.587028\"/>\r\n     <use style=\"fill:#ff7f0e;stroke:#ffffff;stroke-width:0.75;\" x=\"194.074716\" xlink:href=\"#m5c18cb85f8\" y=\"25.934654\"/>\r\n     <use style=\"fill:#ff7f0e;stroke:#ffffff;stroke-width:0.75;\" x=\"214.365625\" xlink:href=\"#m5c18cb85f8\" y=\"31.835332\"/>\r\n     <use style=\"fill:#ff7f0e;stroke:#ffffff;stroke-width:0.75;\" x=\"234.656534\" xlink:href=\"#m5c18cb85f8\" y=\"55.438046\"/>\r\n     <use style=\"fill:#ff7f0e;stroke:#ffffff;stroke-width:0.75;\" x=\"254.947443\" xlink:href=\"#m5c18cb85f8\" y=\"49.537368\"/>\r\n     <use style=\"fill:#ff7f0e;stroke:#ffffff;stroke-width:0.75;\" x=\"285.383807\" xlink:href=\"#m5c18cb85f8\" y=\"64.289064\"/>\r\n     <use style=\"fill:#ff7f0e;stroke:#ffffff;stroke-width:0.75;\" x=\"305.674716\" xlink:href=\"#m5c18cb85f8\" y=\"52.487707\"/>\r\n     <use style=\"fill:#ff7f0e;stroke:#ffffff;stroke-width:0.75;\" x=\"325.965625\" xlink:href=\"#m5c18cb85f8\" y=\"76.090421\"/>\r\n     <use style=\"fill:#ff7f0e;stroke:#ffffff;stroke-width:0.75;\" x=\"346.256534\" xlink:href=\"#m5c18cb85f8\" y=\"90.842117\"/>\r\n     <use style=\"fill:#ff7f0e;stroke:#ffffff;stroke-width:0.75;\" x=\"366.547443\" xlink:href=\"#m5c18cb85f8\" y=\"76.090421\"/>\r\n    </g>\r\n   </g>\r\n   <g id=\"line2d_15\">\r\n    <path clip-path=\"url(#pcf677a4c23)\" d=\"M 62.183807 22.984315 \r\nL 82.474716 70.189742 \r\nL 102.765625 46.587028 \r\nL 123.056534 28.884993 \r\nL 143.347443 46.587028 \r\nL 173.783807 49.537368 \r\nL 194.074716 34.785672 \r\nL 214.365625 46.587028 \r\nL 234.656534 49.537368 \r\nL 254.947443 43.636689 \r\nL 285.383807 37.736011 \r\nL 305.674716 37.736011 \r\nL 325.965625 55.438046 \r\nL 346.256534 34.785672 \r\nL 366.547443 31.835332 \r\n\" style=\"fill:none;stroke:#2ca02c;stroke-linecap:square;stroke-width:1.5;\"/>\r\n    <defs>\r\n     <path d=\"M 0 3 \r\nC 0.795609 3 1.55874 2.683901 2.12132 2.12132 \r\nC 2.683901 1.55874 3 0.795609 3 0 \r\nC 3 -0.795609 2.683901 -1.55874 2.12132 -2.12132 \r\nC 1.55874 -2.683901 0.795609 -3 0 -3 \r\nC -0.795609 -3 -1.55874 -2.683901 -2.12132 -2.12132 \r\nC -2.683901 -1.55874 -3 -0.795609 -3 0 \r\nC -3 0.795609 -2.683901 1.55874 -2.12132 2.12132 \r\nC -1.55874 2.683901 -0.795609 3 0 3 \r\nz\r\n\" id=\"me4a373bc35\" style=\"stroke:#ffffff;stroke-width:0.75;\"/>\r\n    </defs>\r\n    <g clip-path=\"url(#pcf677a4c23)\">\r\n     <use style=\"fill:#2ca02c;stroke:#ffffff;stroke-width:0.75;\" x=\"62.183807\" xlink:href=\"#me4a373bc35\" y=\"22.984315\"/>\r\n     <use style=\"fill:#2ca02c;stroke:#ffffff;stroke-width:0.75;\" x=\"82.474716\" xlink:href=\"#me4a373bc35\" y=\"70.189742\"/>\r\n     <use style=\"fill:#2ca02c;stroke:#ffffff;stroke-width:0.75;\" x=\"102.765625\" xlink:href=\"#me4a373bc35\" y=\"46.587028\"/>\r\n     <use style=\"fill:#2ca02c;stroke:#ffffff;stroke-width:0.75;\" x=\"123.056534\" xlink:href=\"#me4a373bc35\" y=\"28.884993\"/>\r\n     <use style=\"fill:#2ca02c;stroke:#ffffff;stroke-width:0.75;\" x=\"143.347443\" xlink:href=\"#me4a373bc35\" y=\"46.587028\"/>\r\n     <use style=\"fill:#2ca02c;stroke:#ffffff;stroke-width:0.75;\" x=\"173.783807\" xlink:href=\"#me4a373bc35\" y=\"49.537368\"/>\r\n     <use style=\"fill:#2ca02c;stroke:#ffffff;stroke-width:0.75;\" x=\"194.074716\" xlink:href=\"#me4a373bc35\" y=\"34.785672\"/>\r\n     <use style=\"fill:#2ca02c;stroke:#ffffff;stroke-width:0.75;\" x=\"214.365625\" xlink:href=\"#me4a373bc35\" y=\"46.587028\"/>\r\n     <use style=\"fill:#2ca02c;stroke:#ffffff;stroke-width:0.75;\" x=\"234.656534\" xlink:href=\"#me4a373bc35\" y=\"49.537368\"/>\r\n     <use style=\"fill:#2ca02c;stroke:#ffffff;stroke-width:0.75;\" x=\"254.947443\" xlink:href=\"#me4a373bc35\" y=\"43.636689\"/>\r\n     <use style=\"fill:#2ca02c;stroke:#ffffff;stroke-width:0.75;\" x=\"285.383807\" xlink:href=\"#me4a373bc35\" y=\"37.736011\"/>\r\n     <use style=\"fill:#2ca02c;stroke:#ffffff;stroke-width:0.75;\" x=\"305.674716\" xlink:href=\"#me4a373bc35\" y=\"37.736011\"/>\r\n     <use style=\"fill:#2ca02c;stroke:#ffffff;stroke-width:0.75;\" x=\"325.965625\" xlink:href=\"#me4a373bc35\" y=\"55.438046\"/>\r\n     <use style=\"fill:#2ca02c;stroke:#ffffff;stroke-width:0.75;\" x=\"346.256534\" xlink:href=\"#me4a373bc35\" y=\"34.785672\"/>\r\n     <use style=\"fill:#2ca02c;stroke:#ffffff;stroke-width:0.75;\" x=\"366.547443\" xlink:href=\"#me4a373bc35\" y=\"31.835332\"/>\r\n    </g>\r\n   </g>\r\n   <g id=\"patch_3\">\r\n    <path d=\"M 46.965625 224.64 \r\nL 46.965625 7.2 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_4\">\r\n    <path d=\"M 381.765625 224.64 \r\nL 381.765625 7.2 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_5\">\r\n    <path d=\"M 46.965625 224.64 \r\nL 381.765625 224.64 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_6\">\r\n    <path d=\"M 46.965625 7.2 \r\nL 381.765625 7.2 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"legend_1\">\r\n    <g id=\"patch_7\">\r\n     <path d=\"M 53.965625 219.64 \r\nL 189.7875 219.64 \r\nQ 191.7875 219.64 191.7875 217.64 \r\nL 191.7875 174.605625 \r\nQ 191.7875 172.605625 189.7875 172.605625 \r\nL 53.965625 172.605625 \r\nQ 51.965625 172.605625 51.965625 174.605625 \r\nL 51.965625 217.64 \r\nQ 51.965625 219.64 53.965625 219.64 \r\nz\r\n\" style=\"fill:#ffffff;opacity:0.8;stroke:#cccccc;stroke-linejoin:miter;\"/>\r\n    </g>\r\n    <g id=\"line2d_16\">\r\n     <path d=\"M 55.965625 180.704062 \r\nL 75.965625 180.704062 \r\n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\r\n    </g>\r\n    <g id=\"line2d_17\">\r\n     <g>\r\n      <use style=\"fill:#1f77b4;stroke:#ffffff;stroke-width:0.75;\" x=\"65.965625\" xlink:href=\"#mb7746e3537\" y=\"180.704062\"/>\r\n     </g>\r\n    </g>\r\n    <g id=\"text_15\">\r\n     <!-- No Feature Selection -->\r\n     <g transform=\"translate(83.965625 184.204062)scale(0.1 -0.1)\">\r\n      <defs>\r\n       <path d=\"M 628 4666 \r\nL 1478 4666 \r\nL 3547 763 \r\nL 3547 4666 \r\nL 4159 4666 \r\nL 4159 0 \r\nL 3309 0 \r\nL 1241 3903 \r\nL 1241 0 \r\nL 628 0 \r\nL 628 4666 \r\nz\r\n\" id=\"DejaVuSans-4e\" transform=\"scale(0.015625)\"/>\r\n       <path d=\"M 1959 3097 \r\nQ 1497 3097 1228 2736 \r\nQ 959 2375 959 1747 \r\nQ 959 1119 1226 758 \r\nQ 1494 397 1959 397 \r\nQ 2419 397 2687 759 \r\nQ 2956 1122 2956 1747 \r\nQ 2956 2369 2687 2733 \r\nQ 2419 3097 1959 3097 \r\nz\r\nM 1959 3584 \r\nQ 2709 3584 3137 3096 \r\nQ 3566 2609 3566 1747 \r\nQ 3566 888 3137 398 \r\nQ 2709 -91 1959 -91 \r\nQ 1206 -91 779 398 \r\nQ 353 888 353 1747 \r\nQ 353 2609 779 3096 \r\nQ 1206 3584 1959 3584 \r\nz\r\n\" id=\"DejaVuSans-6f\" transform=\"scale(0.015625)\"/>\r\n       <path d=\"M 628 4666 \r\nL 3309 4666 \r\nL 3309 4134 \r\nL 1259 4134 \r\nL 1259 2759 \r\nL 3109 2759 \r\nL 3109 2228 \r\nL 1259 2228 \r\nL 1259 0 \r\nL 628 0 \r\nL 628 4666 \r\nz\r\n\" id=\"DejaVuSans-46\" transform=\"scale(0.015625)\"/>\r\n       <path d=\"M 3425 4513 \r\nL 3425 3897 \r\nQ 3066 4069 2747 4153 \r\nQ 2428 4238 2131 4238 \r\nQ 1616 4238 1336 4038 \r\nQ 1056 3838 1056 3469 \r\nQ 1056 3159 1242 3001 \r\nQ 1428 2844 1947 2747 \r\nL 2328 2669 \r\nQ 3034 2534 3370 2195 \r\nQ 3706 1856 3706 1288 \r\nQ 3706 609 3251 259 \r\nQ 2797 -91 1919 -91 \r\nQ 1588 -91 1214 -16 \r\nQ 841 59 441 206 \r\nL 441 856 \r\nQ 825 641 1194 531 \r\nQ 1563 422 1919 422 \r\nQ 2459 422 2753 634 \r\nQ 3047 847 3047 1241 \r\nQ 3047 1584 2836 1778 \r\nQ 2625 1972 2144 2069 \r\nL 1759 2144 \r\nQ 1053 2284 737 2584 \r\nQ 422 2884 422 3419 \r\nQ 422 4038 858 4394 \r\nQ 1294 4750 2059 4750 \r\nQ 2388 4750 2728 4690 \r\nQ 3069 4631 3425 4513 \r\nz\r\n\" id=\"DejaVuSans-53\" transform=\"scale(0.015625)\"/>\r\n       <path d=\"M 603 4863 \r\nL 1178 4863 \r\nL 1178 0 \r\nL 603 0 \r\nL 603 4863 \r\nz\r\n\" id=\"DejaVuSans-6c\" transform=\"scale(0.015625)\"/>\r\n       <path d=\"M 603 3500 \r\nL 1178 3500 \r\nL 1178 0 \r\nL 603 0 \r\nL 603 3500 \r\nz\r\nM 603 4863 \r\nL 1178 4863 \r\nL 1178 4134 \r\nL 603 4134 \r\nL 603 4863 \r\nz\r\n\" id=\"DejaVuSans-69\" transform=\"scale(0.015625)\"/>\r\n       <path d=\"M 3513 2113 \r\nL 3513 0 \r\nL 2938 0 \r\nL 2938 2094 \r\nQ 2938 2591 2744 2837 \r\nQ 2550 3084 2163 3084 \r\nQ 1697 3084 1428 2787 \r\nQ 1159 2491 1159 1978 \r\nL 1159 0 \r\nL 581 0 \r\nL 581 3500 \r\nL 1159 3500 \r\nL 1159 2956 \r\nQ 1366 3272 1645 3428 \r\nQ 1925 3584 2291 3584 \r\nQ 2894 3584 3203 3211 \r\nQ 3513 2838 3513 2113 \r\nz\r\n\" id=\"DejaVuSans-6e\" transform=\"scale(0.015625)\"/>\r\n      </defs>\r\n      <use xlink:href=\"#DejaVuSans-4e\"/>\r\n      <use x=\"74.804688\" xlink:href=\"#DejaVuSans-6f\"/>\r\n      <use x=\"135.986328\" xlink:href=\"#DejaVuSans-20\"/>\r\n      <use x=\"167.773438\" xlink:href=\"#DejaVuSans-46\"/>\r\n      <use x=\"219.792969\" xlink:href=\"#DejaVuSans-65\"/>\r\n      <use x=\"281.316406\" xlink:href=\"#DejaVuSans-61\"/>\r\n      <use x=\"342.595703\" xlink:href=\"#DejaVuSans-74\"/>\r\n      <use x=\"381.804688\" xlink:href=\"#DejaVuSans-75\"/>\r\n      <use x=\"445.183594\" xlink:href=\"#DejaVuSans-72\"/>\r\n      <use x=\"484.046875\" xlink:href=\"#DejaVuSans-65\"/>\r\n      <use x=\"545.570312\" xlink:href=\"#DejaVuSans-20\"/>\r\n      <use x=\"577.357422\" xlink:href=\"#DejaVuSans-53\"/>\r\n      <use x=\"640.833984\" xlink:href=\"#DejaVuSans-65\"/>\r\n      <use x=\"702.357422\" xlink:href=\"#DejaVuSans-6c\"/>\r\n      <use x=\"730.140625\" xlink:href=\"#DejaVuSans-65\"/>\r\n      <use x=\"791.664062\" xlink:href=\"#DejaVuSans-63\"/>\r\n      <use x=\"846.644531\" xlink:href=\"#DejaVuSans-74\"/>\r\n      <use x=\"885.853516\" xlink:href=\"#DejaVuSans-69\"/>\r\n      <use x=\"913.636719\" xlink:href=\"#DejaVuSans-6f\"/>\r\n      <use x=\"974.818359\" xlink:href=\"#DejaVuSans-6e\"/>\r\n     </g>\r\n    </g>\r\n    <g id=\"line2d_18\">\r\n     <path d=\"M 55.965625 195.382187 \r\nL 75.965625 195.382187 \r\n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\r\n    </g>\r\n    <g id=\"line2d_19\">\r\n     <g>\r\n      <use style=\"fill:#ff7f0e;stroke:#ffffff;stroke-width:0.75;\" x=\"65.965625\" xlink:href=\"#m5c18cb85f8\" y=\"195.382187\"/>\r\n     </g>\r\n    </g>\r\n    <g id=\"text_16\">\r\n     <!-- Gini-Filter Half 0.5 -->\r\n     <g transform=\"translate(83.965625 198.882187)scale(0.1 -0.1)\">\r\n      <defs>\r\n       <path d=\"M 3809 666 \r\nL 3809 1919 \r\nL 2778 1919 \r\nL 2778 2438 \r\nL 4434 2438 \r\nL 4434 434 \r\nQ 4069 175 3628 42 \r\nQ 3188 -91 2688 -91 \r\nQ 1594 -91 976 548 \r\nQ 359 1188 359 2328 \r\nQ 359 3472 976 4111 \r\nQ 1594 4750 2688 4750 \r\nQ 3144 4750 3555 4637 \r\nQ 3966 4525 4313 4306 \r\nL 4313 3634 \r\nQ 3963 3931 3569 4081 \r\nQ 3175 4231 2741 4231 \r\nQ 1884 4231 1454 3753 \r\nQ 1025 3275 1025 2328 \r\nQ 1025 1384 1454 906 \r\nQ 1884 428 2741 428 \r\nQ 3075 428 3337 486 \r\nQ 3600 544 3809 666 \r\nz\r\n\" id=\"DejaVuSans-47\" transform=\"scale(0.015625)\"/>\r\n       <path d=\"M 313 2009 \r\nL 1997 2009 \r\nL 1997 1497 \r\nL 313 1497 \r\nL 313 2009 \r\nz\r\n\" id=\"DejaVuSans-2d\" transform=\"scale(0.015625)\"/>\r\n       <path d=\"M 628 4666 \r\nL 1259 4666 \r\nL 1259 2753 \r\nL 3553 2753 \r\nL 3553 4666 \r\nL 4184 4666 \r\nL 4184 0 \r\nL 3553 0 \r\nL 3553 2222 \r\nL 1259 2222 \r\nL 1259 0 \r\nL 628 0 \r\nL 628 4666 \r\nz\r\n\" id=\"DejaVuSans-48\" transform=\"scale(0.015625)\"/>\r\n       <path d=\"M 684 794 \r\nL 1344 794 \r\nL 1344 0 \r\nL 684 0 \r\nL 684 794 \r\nz\r\n\" id=\"DejaVuSans-2e\" transform=\"scale(0.015625)\"/>\r\n      </defs>\r\n      <use xlink:href=\"#DejaVuSans-47\"/>\r\n      <use x=\"77.490234\" xlink:href=\"#DejaVuSans-69\"/>\r\n      <use x=\"105.273438\" xlink:href=\"#DejaVuSans-6e\"/>\r\n      <use x=\"168.652344\" xlink:href=\"#DejaVuSans-69\"/>\r\n      <use x=\"196.435547\" xlink:href=\"#DejaVuSans-2d\"/>\r\n      <use x=\"232.519531\" xlink:href=\"#DejaVuSans-46\"/>\r\n      <use x=\"282.789062\" xlink:href=\"#DejaVuSans-69\"/>\r\n      <use x=\"310.572266\" xlink:href=\"#DejaVuSans-6c\"/>\r\n      <use x=\"338.355469\" xlink:href=\"#DejaVuSans-74\"/>\r\n      <use x=\"377.564453\" xlink:href=\"#DejaVuSans-65\"/>\r\n      <use x=\"439.087891\" xlink:href=\"#DejaVuSans-72\"/>\r\n      <use x=\"480.201172\" xlink:href=\"#DejaVuSans-20\"/>\r\n      <use x=\"511.988281\" xlink:href=\"#DejaVuSans-48\"/>\r\n      <use x=\"587.183594\" xlink:href=\"#DejaVuSans-61\"/>\r\n      <use x=\"648.462891\" xlink:href=\"#DejaVuSans-6c\"/>\r\n      <use x=\"676.246094\" xlink:href=\"#DejaVuSans-66\"/>\r\n      <use x=\"711.451172\" xlink:href=\"#DejaVuSans-20\"/>\r\n      <use x=\"743.238281\" xlink:href=\"#DejaVuSans-30\"/>\r\n      <use x=\"806.861328\" xlink:href=\"#DejaVuSans-2e\"/>\r\n      <use x=\"838.648438\" xlink:href=\"#DejaVuSans-35\"/>\r\n     </g>\r\n    </g>\r\n    <g id=\"line2d_20\">\r\n     <path d=\"M 55.965625 210.060312 \r\nL 75.965625 210.060312 \r\n\" style=\"fill:none;stroke:#2ca02c;stroke-linecap:square;stroke-width:1.5;\"/>\r\n    </g>\r\n    <g id=\"line2d_21\">\r\n     <g>\r\n      <use style=\"fill:#2ca02c;stroke:#ffffff;stroke-width:0.75;\" x=\"65.965625\" xlink:href=\"#me4a373bc35\" y=\"210.060312\"/>\r\n     </g>\r\n    </g>\r\n    <g id=\"text_17\">\r\n     <!-- STG -->\r\n     <g transform=\"translate(83.965625 213.560312)scale(0.1 -0.1)\">\r\n      <defs>\r\n       <path d=\"M -19 4666 \r\nL 3928 4666 \r\nL 3928 4134 \r\nL 2272 4134 \r\nL 2272 0 \r\nL 1638 0 \r\nL 1638 4134 \r\nL -19 4134 \r\nL -19 4666 \r\nz\r\n\" id=\"DejaVuSans-54\" transform=\"scale(0.015625)\"/>\r\n      </defs>\r\n      <use xlink:href=\"#DejaVuSans-53\"/>\r\n      <use x=\"63.476562\" xlink:href=\"#DejaVuSans-54\"/>\r\n      <use x=\"124.560547\" xlink:href=\"#DejaVuSans-47\"/>\r\n     </g>\r\n    </g>\r\n   </g>\r\n  </g>\r\n </g>\r\n <defs>\r\n  <clipPath id=\"pcf677a4c23\">\r\n   <rect height=\"217.44\" width=\"334.8\" x=\"46.965625\" y=\"7.2\"/>\r\n  </clipPath>\r\n </defs>\r\n</svg>\r\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "sns.lineplot(data=nofs_results, x=\"# features\", y=\"accuracy\", \n",
    "label='No Feature Selection', marker='o')\n",
    "sns.lineplot(data=gini_half_results, x=\"# features\", y=\"accuracy\",\n",
    "label='Gini-Filter Half 0.5', marker='o')\n",
    "sns.lineplot(data=stg_results, x=\"# features\", y=\"accuracy\", \n",
    "label='STG', marker='o')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100: loss=1.141661 valid_loss=1.082312\n",
      "Epoch: 200: loss=0.632584 valid_loss=0.663396\n",
      "Epoch: 300: loss=0.366413 valid_loss=0.330151\n",
      "Epoch: 400: loss=0.318675 valid_loss=0.280543\n",
      "Epoch: 500: loss=0.268238 valid_loss=0.097086\n",
      "Epoch: 600: loss=0.211092 valid_loss=0.066301\n",
      "Epoch: 700: loss=0.163787 valid_loss=0.052567\n",
      "Epoch: 800: loss=0.156890 valid_loss=0.043926\n",
      "Epoch: 900: loss=0.148733 valid_loss=0.039008\n",
      "Epoch: 1000: loss=0.144540 valid_loss=0.035064\n",
      "Accuracy = 100.0%\n",
      "100.0%, 3/8 features\n",
      "Epoch: 100: loss=1.136601 valid_loss=1.103121\n",
      "Epoch: 200: loss=0.838451 valid_loss=0.798046\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 300: loss=0.458451 valid_loss=0.428312\n",
      "Epoch: 400: loss=0.302189 valid_loss=0.247834\n",
      "Epoch: 500: loss=0.359134 valid_loss=0.375282\n",
      "Epoch: 600: loss=0.200712 valid_loss=0.130219\n",
      "Epoch: 700: loss=0.179418 valid_loss=0.111047\n",
      "Epoch: 800: loss=0.167611 valid_loss=0.100289\n",
      "Epoch: 900: loss=0.159524 valid_loss=0.091248\n",
      "Epoch: 1000: loss=0.156049 valid_loss=0.085459\n",
      "Accuracy = 96.66666666666667%\n",
      "96.66666666666667%, 3/10 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100: loss=1.165314 valid_loss=1.105360\n",
      "Epoch: 200: loss=0.990139 valid_loss=0.936128\n",
      "Epoch: 300: loss=0.516025 valid_loss=0.490155\n",
      "Epoch: 400: loss=0.417144 valid_loss=0.300483\n",
      "Epoch: 500: loss=0.264698 valid_loss=0.177584\n",
      "Epoch: 600: loss=0.195673 valid_loss=0.128410\n",
      "Epoch: 700: loss=0.175315 valid_loss=0.101007\n",
      "Epoch: 800: loss=0.158317 valid_loss=0.085804\n",
      "Epoch: 900: loss=0.148803 valid_loss=0.076134\n",
      "Epoch: 1000: loss=0.140717 valid_loss=0.070057\n",
      "Accuracy = 100.0%\n",
      "100.0%, 3/12 features\n",
      "Epoch: 100: loss=1.169827 valid_loss=1.115293\n",
      "Epoch: 200: loss=1.153981 valid_loss=1.073532\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 300: loss=0.678958 valid_loss=0.669510\n",
      "Epoch: 400: loss=0.390109 valid_loss=0.351228\n",
      "Epoch: 500: loss=0.285467 valid_loss=0.243468\n",
      "Epoch: 600: loss=0.217596 valid_loss=0.171624\n",
      "Epoch: 700: loss=0.242491 valid_loss=0.196763\n",
      "Epoch: 800: loss=0.164518 valid_loss=0.100964\n",
      "Epoch: 900: loss=0.153973 valid_loss=0.091622\n",
      "Epoch: 1000: loss=0.145396 valid_loss=0.084162\n",
      "Accuracy = 96.66666666666667%\n",
      "96.66666666666667%, 3/14 features\n",
      "Epoch: 100: loss=1.150971 valid_loss=1.090176\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 200: loss=0.640512 valid_loss=0.653050\n",
      "Epoch: 300: loss=0.425651 valid_loss=0.448277\n",
      "Epoch: 400: loss=0.352951 valid_loss=0.235550\n",
      "Epoch: 500: loss=0.228491 valid_loss=0.166421\n",
      "Epoch: 600: loss=0.195857 valid_loss=0.121913\n",
      "Epoch: 700: loss=0.221882 valid_loss=0.094213\n",
      "Epoch: 800: loss=0.262822 valid_loss=0.076712\n",
      "Epoch: 900: loss=0.159349 valid_loss=0.076389\n",
      "Epoch: 1000: loss=0.158700 valid_loss=0.060352\n",
      "Accuracy = 100.0%\n",
      "100.0%, 5/16 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100: loss=1.130576 valid_loss=1.094030\n",
      "Epoch: 200: loss=0.689021 valid_loss=0.620083\n",
      "Epoch: 300: loss=0.372948 valid_loss=0.285194\n",
      "Epoch: 400: loss=0.248812 valid_loss=0.153546\n",
      "Epoch: 500: loss=0.221350 valid_loss=0.116643\n",
      "Epoch: 600: loss=0.230428 valid_loss=0.095769\n",
      "Epoch: 700: loss=0.382148 valid_loss=0.090872\n",
      "Epoch: 800: loss=0.198326 valid_loss=0.085192\n",
      "Epoch: 900: loss=0.208123 valid_loss=0.079467\n",
      "Epoch: 1000: loss=0.186154 valid_loss=0.073160\n",
      "Accuracy = 96.66666666666667%\n",
      "96.66666666666667%, 7/19 features\n",
      "Epoch: 100: loss=1.169577 valid_loss=1.116583\n",
      "Epoch: 200: loss=1.109650 valid_loss=1.062959\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 300: loss=0.638135 valid_loss=0.606804\n",
      "Epoch: 400: loss=0.489571 valid_loss=0.562990\n",
      "Epoch: 500: loss=0.330166 valid_loss=0.415490\n",
      "Epoch: 600: loss=0.227275 valid_loss=0.211852\n",
      "Epoch: 700: loss=0.189455 valid_loss=0.125415\n",
      "Epoch: 800: loss=0.187332 valid_loss=0.098414\n",
      "Epoch: 900: loss=0.167332 valid_loss=0.080594\n",
      "Epoch: 1000: loss=0.162225 valid_loss=0.069942\n",
      "Accuracy = 100.0%\n",
      "100.0%, 8/21 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100: loss=1.171480 valid_loss=1.110558\n",
      "Epoch: 200: loss=1.006621 valid_loss=0.999333\n",
      "Epoch: 300: loss=0.497457 valid_loss=0.472022\n",
      "Epoch: 400: loss=0.333285 valid_loss=0.343329\n",
      "Epoch: 500: loss=0.260330 valid_loss=0.200705\n",
      "Epoch: 600: loss=0.194871 valid_loss=0.122708\n",
      "Epoch: 700: loss=0.175468 valid_loss=0.102553\n",
      "Epoch: 800: loss=0.162342 valid_loss=0.093094\n",
      "Epoch: 900: loss=0.148546 valid_loss=0.082230\n",
      "Epoch: 1000: loss=0.140133 valid_loss=0.078220\n",
      "Accuracy = 96.66666666666667%\n",
      "96.66666666666667%, 4/23 features\n",
      "Epoch: 100: loss=1.137998 valid_loss=1.081760\n",
      "Epoch: 200: loss=0.850783 valid_loss=0.816167\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 300: loss=0.273570 valid_loss=0.280371\n",
      "Epoch: 400: loss=0.215681 valid_loss=0.142325\n",
      "Epoch: 500: loss=0.188988 valid_loss=0.103477\n",
      "Epoch: 600: loss=0.202077 valid_loss=0.097718\n",
      "Epoch: 700: loss=0.162598 valid_loss=0.068020\n",
      "Epoch: 800: loss=0.170697 valid_loss=0.058920\n",
      "Epoch: 900: loss=0.142034 valid_loss=0.091948\n",
      "Epoch: 1000: loss=0.128000 valid_loss=0.051981\n",
      "Accuracy = 96.66666666666667%\n",
      "96.66666666666667%, 9/25 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100: loss=1.165965 valid_loss=1.112613\n",
      "Epoch: 200: loss=0.998463 valid_loss=0.990318\n",
      "Epoch: 300: loss=0.631511 valid_loss=0.518496\n",
      "Epoch: 400: loss=0.274078 valid_loss=0.183483\n",
      "Epoch: 500: loss=0.204799 valid_loss=0.113300\n",
      "Epoch: 600: loss=0.188662 valid_loss=0.091018\n",
      "Epoch: 700: loss=0.169249 valid_loss=0.077572\n",
      "Epoch: 800: loss=0.159889 valid_loss=0.068419\n",
      "Epoch: 900: loss=0.151020 valid_loss=0.070704\n",
      "Epoch: 1000: loss=0.141978 valid_loss=0.054749\n",
      "Accuracy = 100.0%\n",
      "100.0%, 12/27 features\n",
      "Epoch: 100: loss=1.117110 valid_loss=1.085020\n",
      "Epoch: 200: loss=0.611509 valid_loss=0.592014\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 300: loss=0.350688 valid_loss=0.334303\n",
      "Epoch: 400: loss=0.237180 valid_loss=0.210265\n",
      "Epoch: 500: loss=0.205686 valid_loss=0.116060\n",
      "Epoch: 600: loss=0.175416 valid_loss=0.092289\n",
      "Epoch: 700: loss=0.166075 valid_loss=0.078447\n",
      "Epoch: 800: loss=0.165022 valid_loss=0.086598\n",
      "Epoch: 900: loss=0.174718 valid_loss=0.055997\n",
      "Epoch: 1000: loss=0.140430 valid_loss=0.054061\n",
      "Accuracy = 96.66666666666667%\n",
      "96.66666666666667%, 15/30 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100: loss=1.169032 valid_loss=1.119179\n",
      "Epoch: 200: loss=1.083634 valid_loss=1.070352\n",
      "Epoch: 300: loss=0.621093 valid_loss=0.638922\n",
      "Epoch: 400: loss=0.407322 valid_loss=0.482113\n",
      "Epoch: 500: loss=0.285461 valid_loss=0.265911\n",
      "Epoch: 600: loss=0.222762 valid_loss=0.140285\n",
      "Epoch: 700: loss=0.192392 valid_loss=0.112927\n",
      "Epoch: 800: loss=0.178621 valid_loss=0.091344\n",
      "Epoch: 900: loss=0.188011 valid_loss=0.081081\n",
      "Epoch: 1000: loss=0.167711 valid_loss=0.075753\n",
      "Accuracy = 100.0%\n",
      "100.0%, 16/32 features\n",
      "Epoch: 100: loss=1.146063 valid_loss=1.099857\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 200: loss=0.857365 valid_loss=0.761503\n",
      "Epoch: 300: loss=0.403870 valid_loss=0.366529\n",
      "Epoch: 400: loss=0.286551 valid_loss=0.231761\n",
      "Epoch: 500: loss=0.220680 valid_loss=0.183391\n",
      "Epoch: 600: loss=0.206609 valid_loss=0.148925\n",
      "Epoch: 700: loss=0.173733 valid_loss=0.129407\n",
      "Epoch: 800: loss=0.164895 valid_loss=0.110614\n",
      "Epoch: 900: loss=0.165796 valid_loss=0.139927\n",
      "Epoch: 1000: loss=0.151012 valid_loss=0.105392\n",
      "Accuracy = 96.66666666666667%\n",
      "96.66666666666667%, 17/34 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100: loss=1.156221 valid_loss=1.100620\n",
      "Epoch: 200: loss=0.738645 valid_loss=0.709273\n",
      "Epoch: 300: loss=0.380807 valid_loss=0.317276\n",
      "Epoch: 400: loss=0.248177 valid_loss=0.202875\n",
      "Epoch: 500: loss=0.212619 valid_loss=0.150964\n",
      "Epoch: 600: loss=0.302202 valid_loss=0.091716\n",
      "Epoch: 700: loss=0.164985 valid_loss=0.070844\n",
      "Epoch: 800: loss=0.219451 valid_loss=0.075686\n",
      "Epoch: 900: loss=0.206373 valid_loss=0.057743\n",
      "Epoch: 1000: loss=0.146010 valid_loss=0.064335\n",
      "Accuracy = 96.66666666666667%\n",
      "96.66666666666667%, 18/36 features\n",
      "Epoch: 100: loss=1.160379 valid_loss=1.094978\n",
      "Epoch: 200: loss=0.662421 valid_loss=0.656187\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 300: loss=0.376207 valid_loss=0.398203\n",
      "Epoch: 400: loss=0.252687 valid_loss=0.188046\n",
      "Epoch: 500: loss=0.206033 valid_loss=0.145444\n",
      "Epoch: 600: loss=0.169796 valid_loss=0.104535\n",
      "Epoch: 700: loss=0.158248 valid_loss=0.072114\n",
      "Epoch: 800: loss=0.451356 valid_loss=0.116503\n",
      "Epoch: 900: loss=0.214949 valid_loss=0.084378\n",
      "Epoch: 1000: loss=0.137710 valid_loss=0.051429\n",
      "Accuracy = 100.0%\n",
      "100.0%, 19/38 features\n",
      "Epoch: 100: loss=1.148741 valid_loss=1.101250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 200: loss=0.913612 valid_loss=0.879566\n",
      "Epoch: 300: loss=0.475055 valid_loss=0.410747\n",
      "Epoch: 400: loss=0.278070 valid_loss=0.171482\n",
      "Epoch: 500: loss=0.223769 valid_loss=0.110451\n",
      "Epoch: 600: loss=0.296587 valid_loss=0.201464\n",
      "Epoch: 700: loss=0.191048 valid_loss=0.060976\n",
      "Epoch: 800: loss=0.173869 valid_loss=0.049419\n",
      "Epoch: 900: loss=0.172012 valid_loss=0.045972\n",
      "Epoch: 1000: loss=0.164514 valid_loss=0.038047\n",
      "Accuracy = 96.66666666666667%\n",
      "96.66666666666667%, 4/8 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100: loss=1.160322 valid_loss=1.114595\n",
      "Epoch: 200: loss=1.109950 valid_loss=1.060278\n",
      "Epoch: 300: loss=0.656910 valid_loss=0.654696\n",
      "Epoch: 400: loss=0.526628 valid_loss=0.453993\n",
      "Epoch: 500: loss=0.400509 valid_loss=0.365419\n",
      "Epoch: 600: loss=0.305823 valid_loss=0.284503\n",
      "Epoch: 700: loss=0.237866 valid_loss=0.184064\n",
      "Epoch: 800: loss=0.205415 valid_loss=0.141083\n",
      "Epoch: 900: loss=0.175804 valid_loss=0.113531\n",
      "Epoch: 1000: loss=0.163021 valid_loss=0.099553\n",
      "Accuracy = 100.0%\n",
      "100.0%, 3/10 features\n",
      "Epoch: 100: loss=1.174802 valid_loss=1.120909\n",
      "Epoch: 200: loss=1.161477 valid_loss=1.118574\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 300: loss=1.116183 valid_loss=1.100645\n",
      "Epoch: 400: loss=0.849061 valid_loss=0.866133\n",
      "Epoch: 500: loss=0.614923 valid_loss=0.580472\n",
      "Epoch: 600: loss=0.458507 valid_loss=0.437027\n",
      "Epoch: 700: loss=0.386281 valid_loss=0.356578\n",
      "Epoch: 800: loss=0.458858 valid_loss=0.373094\n",
      "Epoch: 900: loss=0.228023 valid_loss=0.198721\n",
      "Epoch: 1000: loss=0.183765 valid_loss=0.143205\n",
      "Accuracy = 100.0%\n",
      "100.0%, 2/12 features\n",
      "Epoch: 100: loss=1.163721 valid_loss=1.114870\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 200: loss=1.094966 valid_loss=1.046445\n",
      "Epoch: 300: loss=0.557871 valid_loss=0.572552\n",
      "Epoch: 400: loss=0.375430 valid_loss=0.356369\n",
      "Epoch: 500: loss=0.257094 valid_loss=0.201417\n",
      "Epoch: 600: loss=0.216240 valid_loss=0.184453\n",
      "Epoch: 700: loss=0.188735 valid_loss=0.099421\n",
      "Epoch: 800: loss=0.178510 valid_loss=0.087376\n",
      "Epoch: 900: loss=0.166980 valid_loss=0.085818\n",
      "Epoch: 1000: loss=0.174697 valid_loss=0.081640\n",
      "Accuracy = 96.66666666666667%\n",
      "96.66666666666667%, 6/14 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100: loss=1.167820 valid_loss=1.116716\n",
      "Epoch: 200: loss=1.076776 valid_loss=1.064340\n",
      "Epoch: 300: loss=0.719552 valid_loss=0.735394\n",
      "Epoch: 400: loss=0.516193 valid_loss=0.506030\n",
      "Epoch: 500: loss=0.437418 valid_loss=0.416502\n",
      "Epoch: 600: loss=0.379006 valid_loss=0.369386\n",
      "Epoch: 700: loss=0.385812 valid_loss=0.363476\n",
      "Epoch: 800: loss=0.291066 valid_loss=0.268394\n",
      "Epoch: 900: loss=0.254862 valid_loss=0.194281\n",
      "Epoch: 1000: loss=0.202917 valid_loss=0.157159\n",
      "Accuracy = 96.66666666666667%\n",
      "96.66666666666667%, 4/16 features\n",
      "Epoch: 100: loss=1.108747 valid_loss=1.069617\n",
      "Epoch: 200: loss=0.584946 valid_loss=0.535617\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 300: loss=0.306111 valid_loss=0.219209\n",
      "Epoch: 400: loss=0.230994 valid_loss=0.145492\n",
      "Epoch: 500: loss=0.210112 valid_loss=0.101272\n",
      "Epoch: 600: loss=0.231492 valid_loss=0.093823\n",
      "Epoch: 700: loss=0.189954 valid_loss=0.081794\n",
      "Epoch: 800: loss=0.184735 valid_loss=0.073372\n",
      "Epoch: 900: loss=0.168394 valid_loss=0.064120\n",
      "Epoch: 1000: loss=0.166029 valid_loss=0.068439\n",
      "Accuracy = 96.66666666666667%\n",
      "96.66666666666667%, 6/19 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100: loss=1.121075 valid_loss=1.078900\n",
      "Epoch: 200: loss=0.653992 valid_loss=0.659939\n",
      "Epoch: 300: loss=0.473108 valid_loss=0.465872\n",
      "Epoch: 400: loss=0.361902 valid_loss=0.393222\n",
      "Epoch: 500: loss=0.340313 valid_loss=0.178132\n",
      "Epoch: 600: loss=0.213503 valid_loss=0.171381\n",
      "Epoch: 700: loss=0.267497 valid_loss=0.183505\n",
      "Epoch: 800: loss=0.167249 valid_loss=0.094138\n",
      "Epoch: 900: loss=0.172863 valid_loss=0.072665\n",
      "Epoch: 1000: loss=0.151797 valid_loss=0.079101\n",
      "Accuracy = 100.0%\n",
      "100.0%, 7/21 features\n",
      "Epoch: 100: loss=1.171235 valid_loss=1.113944\n",
      "Epoch: 200: loss=1.141493 valid_loss=1.067620\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 300: loss=0.632746 valid_loss=0.537339\n",
      "Epoch: 400: loss=0.341548 valid_loss=0.288745\n",
      "Epoch: 500: loss=0.223964 valid_loss=0.144865\n",
      "Epoch: 600: loss=0.182130 valid_loss=0.109983\n",
      "Epoch: 700: loss=0.160782 valid_loss=0.079912\n",
      "Epoch: 800: loss=0.157662 valid_loss=0.072598\n",
      "Epoch: 900: loss=0.170357 valid_loss=0.074463\n",
      "Epoch: 1000: loss=0.149620 valid_loss=0.054503\n",
      "Accuracy = 100.0%\n",
      "100.0%, 9/23 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100: loss=1.134837 valid_loss=1.093909\n",
      "Epoch: 200: loss=0.823029 valid_loss=0.834697\n",
      "Epoch: 300: loss=0.578117 valid_loss=0.562641\n",
      "Epoch: 400: loss=0.282256 valid_loss=0.247962\n",
      "Epoch: 500: loss=0.200249 valid_loss=0.139169\n",
      "Epoch: 600: loss=0.167296 valid_loss=0.114259\n",
      "Epoch: 700: loss=0.169429 valid_loss=0.099222\n",
      "Epoch: 800: loss=0.162904 valid_loss=0.095758\n",
      "Epoch: 900: loss=0.138559 valid_loss=0.081281\n",
      "Epoch: 1000: loss=0.183547 valid_loss=0.094645\n",
      "Accuracy = 96.66666666666667%\n",
      "96.66666666666667%, 8/25 features\n",
      "Epoch: 100: loss=1.126223 valid_loss=1.095082\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 200: loss=0.733321 valid_loss=0.716907\n",
      "Epoch: 300: loss=0.399318 valid_loss=0.369597\n",
      "Epoch: 400: loss=0.278561 valid_loss=0.184625\n",
      "Epoch: 500: loss=0.218515 valid_loss=0.161763\n",
      "Epoch: 600: loss=0.210035 valid_loss=0.114110\n",
      "Epoch: 700: loss=0.176556 valid_loss=0.074432\n",
      "Epoch: 800: loss=0.157685 valid_loss=0.076252\n",
      "Epoch: 900: loss=0.153204 valid_loss=0.055426\n",
      "Epoch: 1000: loss=0.157170 valid_loss=0.081250\n",
      "Accuracy = 93.33333333333333%\n",
      "93.33333333333333%, 11/27 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100: loss=1.164597 valid_loss=1.119551\n",
      "Epoch: 200: loss=1.103758 valid_loss=1.062078\n",
      "Epoch: 300: loss=0.588154 valid_loss=0.621518\n",
      "Epoch: 400: loss=0.359994 valid_loss=0.312330\n",
      "Epoch: 500: loss=0.246998 valid_loss=0.198827\n",
      "Epoch: 600: loss=0.207442 valid_loss=0.136514\n",
      "Epoch: 700: loss=0.212707 valid_loss=0.170338\n",
      "Epoch: 800: loss=0.171084 valid_loss=0.099307\n",
      "Epoch: 900: loss=0.157602 valid_loss=0.092012\n",
      "Epoch: 1000: loss=0.173938 valid_loss=0.076856\n",
      "Accuracy = 96.66666666666667%\n",
      "96.66666666666667%, 15/30 features\n",
      "Epoch: 100: loss=1.148337 valid_loss=1.101490\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 200: loss=0.806762 valid_loss=0.800404\n",
      "Epoch: 300: loss=0.459360 valid_loss=0.436876\n",
      "Epoch: 400: loss=0.314849 valid_loss=0.284264\n",
      "Epoch: 500: loss=0.219786 valid_loss=0.150595\n",
      "Epoch: 600: loss=0.188985 valid_loss=0.108842\n",
      "Epoch: 700: loss=0.169103 valid_loss=0.096839\n",
      "Epoch: 800: loss=0.161721 valid_loss=0.095813\n",
      "Epoch: 900: loss=0.188281 valid_loss=0.090335\n",
      "Epoch: 1000: loss=0.145988 valid_loss=0.073263\n",
      "Accuracy = 96.66666666666667%\n",
      "96.66666666666667%, 16/32 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100: loss=1.168846 valid_loss=1.115656\n",
      "Epoch: 200: loss=1.121580 valid_loss=1.054385\n",
      "Epoch: 300: loss=0.934989 valid_loss=0.705117\n",
      "Epoch: 400: loss=0.340834 valid_loss=0.289427\n",
      "Epoch: 500: loss=0.235630 valid_loss=0.167650\n",
      "Epoch: 600: loss=0.210853 valid_loss=0.082010\n",
      "Epoch: 700: loss=0.198576 valid_loss=0.141575\n",
      "Epoch: 800: loss=0.199221 valid_loss=0.060871\n",
      "Epoch: 900: loss=0.169831 valid_loss=0.053535\n",
      "Epoch: 1000: loss=0.160026 valid_loss=0.050120\n",
      "Accuracy = 100.0%\n",
      "100.0%, 17/34 features\n",
      "Epoch: 100: loss=1.172033 valid_loss=1.118897\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 200: loss=1.073449 valid_loss=1.073672\n",
      "Epoch: 300: loss=0.615623 valid_loss=0.675252\n",
      "Epoch: 400: loss=0.426189 valid_loss=0.524419\n",
      "Epoch: 500: loss=0.262001 valid_loss=0.212761\n",
      "Epoch: 600: loss=0.241701 valid_loss=0.148880\n",
      "Epoch: 700: loss=0.179036 valid_loss=0.112834\n",
      "Epoch: 800: loss=0.166578 valid_loss=0.110756\n",
      "Epoch: 900: loss=0.210972 valid_loss=0.081438\n",
      "Epoch: 1000: loss=0.179807 valid_loss=0.103957\n",
      "Accuracy = 100.0%\n",
      "100.0%, 18/36 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100: loss=1.168910 valid_loss=1.106216\n",
      "Epoch: 200: loss=0.852542 valid_loss=0.824338\n",
      "Epoch: 300: loss=0.446274 valid_loss=0.383174\n",
      "Epoch: 400: loss=0.308811 valid_loss=0.211131\n",
      "Epoch: 500: loss=0.227336 valid_loss=0.202616\n",
      "Epoch: 600: loss=0.202316 valid_loss=0.099828\n",
      "Epoch: 700: loss=0.205436 valid_loss=0.111018\n",
      "Epoch: 800: loss=0.177806 valid_loss=0.078948\n",
      "Epoch: 900: loss=0.147479 valid_loss=0.059984\n",
      "Epoch: 1000: loss=0.159612 valid_loss=0.055001\n",
      "Accuracy = 96.66666666666667%\n",
      "96.66666666666667%, 19/38 features\n",
      "Epoch: 100: loss=1.160202 valid_loss=1.112351\n",
      "Epoch: 200: loss=1.119735 valid_loss=1.064991\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 300: loss=1.057397 valid_loss=0.709828\n",
      "Epoch: 400: loss=0.392937 valid_loss=0.399677\n",
      "Epoch: 500: loss=0.291221 valid_loss=0.144118\n",
      "Epoch: 600: loss=0.231583 valid_loss=0.081486\n",
      "Epoch: 700: loss=0.489085 valid_loss=0.104517\n",
      "Epoch: 800: loss=0.185936 valid_loss=0.051243\n",
      "Epoch: 900: loss=0.190293 valid_loss=0.051474\n",
      "Epoch: 1000: loss=0.295321 valid_loss=0.037555\n",
      "Accuracy = 100.0%\n",
      "100.0%, 4/8 features\n",
      "Epoch: 100: loss=1.173272 valid_loss=1.114903\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 200: loss=1.145976 valid_loss=1.076791\n",
      "Epoch: 300: loss=0.614311 valid_loss=0.603547\n",
      "Epoch: 400: loss=0.382711 valid_loss=0.342447\n",
      "Epoch: 500: loss=0.257040 valid_loss=0.223497\n",
      "Epoch: 600: loss=0.951070 valid_loss=0.319475\n",
      "Epoch: 700: loss=0.181442 valid_loss=0.115589\n",
      "Epoch: 800: loss=0.166975 valid_loss=0.094461\n",
      "Epoch: 900: loss=0.157816 valid_loss=0.082009\n",
      "Epoch: 1000: loss=0.152504 valid_loss=0.073770\n",
      "Accuracy = 96.66666666666667%\n",
      "96.66666666666667%, 3/10 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100: loss=1.111070 valid_loss=1.084597\n",
      "Epoch: 200: loss=0.637055 valid_loss=0.616876\n",
      "Epoch: 300: loss=0.585986 valid_loss=0.595440\n",
      "Epoch: 400: loss=0.307277 valid_loss=0.249220\n",
      "Epoch: 500: loss=0.243405 valid_loss=0.197440\n",
      "Epoch: 600: loss=0.204590 valid_loss=0.138474\n",
      "Epoch: 700: loss=0.173051 valid_loss=0.101628\n",
      "Epoch: 800: loss=0.160005 valid_loss=0.084573\n",
      "Epoch: 900: loss=0.152865 valid_loss=0.079934\n",
      "Epoch: 1000: loss=0.145049 valid_loss=0.075839\n",
      "Accuracy = 100.0%\n",
      "100.0%, 3/12 features\n",
      "Epoch: 100: loss=1.161204 valid_loss=1.099842\n",
      "Epoch: 200: loss=0.884818 valid_loss=0.846971\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 300: loss=0.847512 valid_loss=0.558645\n",
      "Epoch: 400: loss=0.261597 valid_loss=0.201480\n",
      "Epoch: 500: loss=0.203033 valid_loss=0.108506\n",
      "Epoch: 600: loss=0.175239 valid_loss=0.084618\n",
      "Epoch: 700: loss=0.174589 valid_loss=0.104513\n",
      "Epoch: 800: loss=0.150599 valid_loss=0.061647\n",
      "Epoch: 900: loss=0.141215 valid_loss=0.051095\n",
      "Epoch: 1000: loss=0.183908 valid_loss=0.045307\n",
      "Accuracy = 100.0%\n",
      "100.0%, 4/14 features\n",
      "Epoch: 100: loss=1.167265 valid_loss=1.113887\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 200: loss=1.111834 valid_loss=1.051617\n",
      "Epoch: 300: loss=0.578426 valid_loss=0.594724\n",
      "Epoch: 400: loss=0.370826 valid_loss=0.320928\n",
      "Epoch: 500: loss=0.263630 valid_loss=0.199316\n",
      "Epoch: 600: loss=0.214739 valid_loss=0.151785\n",
      "Epoch: 700: loss=0.207640 valid_loss=0.228112\n",
      "Epoch: 800: loss=0.170359 valid_loss=0.103417\n",
      "Epoch: 900: loss=0.161027 valid_loss=0.092916\n",
      "Epoch: 1000: loss=0.153239 valid_loss=0.084696\n",
      "Accuracy = 96.66666666666667%\n",
      "96.66666666666667%, 4/16 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100: loss=1.145529 valid_loss=1.091182\n",
      "Epoch: 200: loss=0.784868 valid_loss=0.701235\n",
      "Epoch: 300: loss=0.434669 valid_loss=0.379926\n",
      "Epoch: 400: loss=0.252601 valid_loss=0.152468\n",
      "Epoch: 500: loss=0.396719 valid_loss=0.110573\n",
      "Epoch: 600: loss=0.205800 valid_loss=0.081388\n",
      "Epoch: 700: loss=0.221668 valid_loss=0.071668\n",
      "Epoch: 800: loss=0.169236 valid_loss=0.075568\n",
      "Epoch: 900: loss=0.186424 valid_loss=0.100022\n",
      "Epoch: 1000: loss=0.159234 valid_loss=0.058084\n",
      "Accuracy = 100.0%\n",
      "100.0%, 6/19 features\n",
      "Epoch: 100: loss=1.135841 valid_loss=1.089145\n",
      "Epoch: 200: loss=0.712138 valid_loss=0.663302\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 300: loss=0.428556 valid_loss=0.383469\n",
      "Epoch: 400: loss=0.301425 valid_loss=0.235568\n",
      "Epoch: 500: loss=0.251774 valid_loss=0.149289\n",
      "Epoch: 600: loss=0.199644 valid_loss=0.108900\n",
      "Epoch: 700: loss=0.179124 valid_loss=0.089481\n",
      "Epoch: 800: loss=0.167261 valid_loss=0.077200\n",
      "Epoch: 900: loss=0.164981 valid_loss=0.071244\n",
      "Epoch: 1000: loss=0.152340 valid_loss=0.067839\n",
      "Accuracy = 96.66666666666667%\n",
      "96.66666666666667%, 6/21 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100: loss=1.171909 valid_loss=1.113985\n",
      "Epoch: 200: loss=1.150685 valid_loss=1.085340\n",
      "Epoch: 300: loss=0.639553 valid_loss=0.622143\n",
      "Epoch: 400: loss=0.329358 valid_loss=0.290926\n",
      "Epoch: 500: loss=0.217066 valid_loss=0.158289\n",
      "Epoch: 600: loss=0.179963 valid_loss=0.098507\n",
      "Epoch: 700: loss=0.172863 valid_loss=0.083385\n",
      "Epoch: 800: loss=0.168714 valid_loss=0.086438\n",
      "Epoch: 900: loss=0.154941 valid_loss=0.066432\n",
      "Epoch: 1000: loss=0.139312 valid_loss=0.052235\n",
      "Accuracy = 100.0%\n",
      "100.0%, 6/23 features\n",
      "Epoch: 100: loss=1.169280 valid_loss=1.116156\n",
      "Epoch: 200: loss=1.075057 valid_loss=1.043379\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 300: loss=0.577499 valid_loss=0.529209\n",
      "Epoch: 400: loss=0.360210 valid_loss=0.425911\n",
      "Epoch: 500: loss=0.249585 valid_loss=0.161815\n",
      "Epoch: 600: loss=0.190222 valid_loss=0.187177\n",
      "Epoch: 700: loss=0.252368 valid_loss=0.106928\n",
      "Epoch: 800: loss=0.166380 valid_loss=0.095422\n",
      "Epoch: 900: loss=0.166327 valid_loss=0.082129\n",
      "Epoch: 1000: loss=0.156697 valid_loss=0.091670\n",
      "Accuracy = 100.0%\n",
      "100.0%, 10/25 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100: loss=1.173878 valid_loss=1.115256\n",
      "Epoch: 200: loss=1.043750 valid_loss=1.013757\n",
      "Epoch: 300: loss=0.622447 valid_loss=0.763888\n",
      "Epoch: 400: loss=0.309049 valid_loss=0.261160\n",
      "Epoch: 500: loss=0.243996 valid_loss=0.189595\n",
      "Epoch: 600: loss=0.220024 valid_loss=0.129678\n",
      "Epoch: 700: loss=0.166810 valid_loss=0.136659\n",
      "Epoch: 800: loss=0.167610 valid_loss=0.108088\n",
      "Epoch: 900: loss=0.214892 valid_loss=0.356471\n",
      "Epoch: 1000: loss=0.147324 valid_loss=0.073283\n",
      "Accuracy = 96.66666666666667%\n",
      "96.66666666666667%, 13/27 features\n",
      "Epoch: 100: loss=1.161571 valid_loss=1.117607\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 200: loss=1.002481 valid_loss=1.005652\n",
      "Epoch: 300: loss=0.639620 valid_loss=0.618553\n",
      "Epoch: 400: loss=0.407964 valid_loss=0.395016\n",
      "Epoch: 500: loss=0.318754 valid_loss=0.305253\n",
      "Epoch: 600: loss=0.252050 valid_loss=0.177038\n",
      "Epoch: 700: loss=0.186290 valid_loss=0.140606\n",
      "Epoch: 800: loss=0.172445 valid_loss=0.108811\n",
      "Epoch: 900: loss=0.224039 valid_loss=0.088207\n",
      "Epoch: 1000: loss=0.154298 valid_loss=0.083869\n",
      "Accuracy = 100.0%\n",
      "100.0%, 15/30 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100: loss=1.173532 valid_loss=1.115160\n",
      "Epoch: 200: loss=1.147815 valid_loss=1.092620\n",
      "Epoch: 300: loss=0.840849 valid_loss=0.756924\n",
      "Epoch: 400: loss=0.466145 valid_loss=0.437247\n",
      "Epoch: 500: loss=0.291128 valid_loss=0.318741\n",
      "Epoch: 600: loss=0.225428 valid_loss=0.160058\n",
      "Epoch: 700: loss=0.196086 valid_loss=0.102338\n",
      "Epoch: 800: loss=0.174753 valid_loss=0.107778\n",
      "Epoch: 900: loss=0.146171 valid_loss=0.052832\n",
      "Epoch: 1000: loss=0.140598 valid_loss=0.048949\n",
      "Accuracy = 100.0%\n",
      "100.0%, 16/32 features\n",
      "Epoch: 100: loss=1.169475 valid_loss=1.112094\n",
      "Epoch: 200: loss=1.098866 valid_loss=1.017379\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 300: loss=0.527156 valid_loss=0.544651\n",
      "Epoch: 400: loss=0.367245 valid_loss=0.373637\n",
      "Epoch: 500: loss=0.243674 valid_loss=0.241877\n",
      "Epoch: 600: loss=0.264015 valid_loss=0.127850\n",
      "Epoch: 700: loss=0.324036 valid_loss=0.225547\n",
      "Epoch: 800: loss=0.186294 valid_loss=0.112734\n",
      "Epoch: 900: loss=0.179126 valid_loss=0.100069\n",
      "Epoch: 1000: loss=0.185348 valid_loss=0.117679\n",
      "Accuracy = 93.33333333333333%\n",
      "93.33333333333333%, 17/34 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100: loss=1.160568 valid_loss=1.102400\n",
      "Epoch: 200: loss=0.779350 valid_loss=0.788317\n",
      "Epoch: 300: loss=0.462742 valid_loss=0.447115\n",
      "Epoch: 400: loss=1.803307 valid_loss=1.681227\n",
      "Epoch: 500: loss=0.211991 valid_loss=0.241651\n",
      "Epoch: 600: loss=0.200916 valid_loss=0.115126\n",
      "Epoch: 700: loss=0.175146 valid_loss=0.072873\n",
      "Epoch: 800: loss=0.154309 valid_loss=0.063944\n",
      "Epoch: 900: loss=0.146335 valid_loss=0.056115\n",
      "Epoch: 1000: loss=0.156018 valid_loss=0.049566\n",
      "Accuracy = 100.0%\n",
      "100.0%, 18/36 features\n",
      "Epoch: 100: loss=1.172437 valid_loss=1.106363\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 200: loss=1.032197 valid_loss=0.874343\n",
      "Epoch: 300: loss=0.439825 valid_loss=0.331640\n",
      "Epoch: 400: loss=0.269022 valid_loss=0.281285\n",
      "Epoch: 500: loss=0.207550 valid_loss=0.148812\n",
      "Epoch: 600: loss=0.188781 valid_loss=0.097854\n",
      "Epoch: 700: loss=0.163674 valid_loss=0.095559\n",
      "Epoch: 800: loss=0.145928 valid_loss=0.080385\n",
      "Epoch: 900: loss=0.222393 valid_loss=0.074205\n",
      "Epoch: 1000: loss=0.139509 valid_loss=0.091990\n",
      "Accuracy = 93.33333333333333%\n",
      "93.33333333333333%, 19/38 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100: loss=1.158365 valid_loss=1.108061\n",
      "Epoch: 200: loss=1.101067 valid_loss=0.945666\n",
      "Epoch: 300: loss=0.687574 valid_loss=0.560849\n",
      "Epoch: 400: loss=0.312204 valid_loss=0.425253\n",
      "Epoch: 500: loss=0.223483 valid_loss=0.114914\n",
      "Epoch: 600: loss=0.193639 valid_loss=0.075885\n",
      "Epoch: 700: loss=0.177379 valid_loss=0.058329\n",
      "Epoch: 800: loss=0.169124 valid_loss=0.046231\n",
      "Epoch: 900: loss=0.162098 valid_loss=0.039417\n",
      "Epoch: 1000: loss=0.175004 valid_loss=0.049004\n",
      "Accuracy = 96.66666666666667%\n",
      "96.66666666666667%, 4/8 features\n",
      "Epoch: 100: loss=1.164311 valid_loss=1.117860\n",
      "Epoch: 200: loss=1.055163 valid_loss=1.068359\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 300: loss=0.732488 valid_loss=0.732533\n",
      "Epoch: 400: loss=0.600146 valid_loss=0.531196\n",
      "Epoch: 500: loss=0.517205 valid_loss=0.484410\n",
      "Epoch: 600: loss=0.362657 valid_loss=0.392751\n",
      "Epoch: 700: loss=0.301853 valid_loss=0.258948\n",
      "Epoch: 800: loss=0.234796 valid_loss=0.192697\n",
      "Epoch: 900: loss=0.205044 valid_loss=0.159340\n",
      "Epoch: 1000: loss=0.188412 valid_loss=0.140366\n",
      "Accuracy = 96.66666666666667%\n",
      "96.66666666666667%, 3/10 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100: loss=1.171749 valid_loss=1.120156\n",
      "Epoch: 200: loss=1.165177 valid_loss=1.117796\n",
      "Epoch: 300: loss=1.149576 valid_loss=1.108509\n",
      "Epoch: 400: loss=0.962916 valid_loss=0.982395\n",
      "Epoch: 500: loss=0.530524 valid_loss=0.514042\n",
      "Epoch: 600: loss=0.354725 valid_loss=0.307096\n",
      "Epoch: 700: loss=0.258010 valid_loss=0.215052\n",
      "Epoch: 800: loss=0.199296 valid_loss=0.155405\n",
      "Epoch: 900: loss=0.175538 valid_loss=0.134735\n",
      "Epoch: 1000: loss=0.174619 valid_loss=0.112935\n",
      "Accuracy = 93.33333333333333%\n",
      "93.33333333333333%, 2/12 features\n",
      "Epoch: 100: loss=1.167660 valid_loss=1.105702\n",
      "Epoch: 200: loss=1.075932 valid_loss=1.000893\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 300: loss=0.630846 valid_loss=0.546695\n",
      "Epoch: 400: loss=0.682284 valid_loss=0.697021\n",
      "Epoch: 500: loss=0.230774 valid_loss=0.161371\n",
      "Epoch: 600: loss=0.190363 valid_loss=0.105843\n",
      "Epoch: 700: loss=0.161807 valid_loss=0.074401\n",
      "Epoch: 800: loss=0.150797 valid_loss=0.058615\n",
      "Epoch: 900: loss=0.137006 valid_loss=0.047961\n",
      "Epoch: 1000: loss=0.132543 valid_loss=0.047988\n",
      "Accuracy = 100.0%\n",
      "100.0%, 3/14 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100: loss=1.168180 valid_loss=1.113378\n",
      "Epoch: 200: loss=1.101756 valid_loss=1.070274\n",
      "Epoch: 300: loss=1.086771 valid_loss=0.751861\n",
      "Epoch: 400: loss=0.397548 valid_loss=0.261176\n",
      "Epoch: 500: loss=0.285084 valid_loss=0.164925\n",
      "Epoch: 600: loss=0.193812 valid_loss=0.128039\n",
      "Epoch: 700: loss=0.177139 valid_loss=0.097062\n",
      "Epoch: 800: loss=0.163165 valid_loss=0.087829\n",
      "Epoch: 900: loss=0.154932 valid_loss=0.077385\n",
      "Epoch: 1000: loss=0.156179 valid_loss=0.070587\n",
      "Accuracy = 96.66666666666667%\n",
      "96.66666666666667%, 4/16 features\n",
      "Epoch: 100: loss=1.147527 valid_loss=1.100323\n",
      "Epoch: 200: loss=0.725812 valid_loss=0.769019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 300: loss=0.465448 valid_loss=0.327816\n",
      "Epoch: 400: loss=0.274504 valid_loss=0.165875\n",
      "Epoch: 500: loss=0.224704 valid_loss=0.115793\n",
      "Epoch: 600: loss=0.305985 valid_loss=0.102365\n",
      "Epoch: 700: loss=0.193737 valid_loss=0.110156\n",
      "Epoch: 800: loss=0.184394 valid_loss=0.075171\n",
      "Epoch: 900: loss=0.189095 valid_loss=0.077714\n",
      "Epoch: 1000: loss=0.174336 valid_loss=0.067858\n",
      "Accuracy = 100.0%\n",
      "100.0%, 6/19 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100: loss=1.169626 valid_loss=1.116627\n",
      "Epoch: 200: loss=1.032638 valid_loss=1.014017\n",
      "Epoch: 300: loss=0.542998 valid_loss=0.566593\n",
      "Epoch: 400: loss=0.492058 valid_loss=0.571047\n",
      "Epoch: 500: loss=0.378946 valid_loss=0.433488\n",
      "Epoch: 600: loss=0.282256 valid_loss=0.131471\n",
      "Epoch: 700: loss=0.202187 valid_loss=0.101638\n",
      "Epoch: 800: loss=0.168663 valid_loss=0.118989\n",
      "Epoch: 900: loss=0.163886 valid_loss=0.079747\n",
      "Epoch: 1000: loss=0.163509 valid_loss=0.069087\n",
      "Accuracy = 96.66666666666667%\n",
      "96.66666666666667%, 7/21 features\n",
      "Epoch: 100: loss=1.128518 valid_loss=1.087956\n",
      "Epoch: 200: loss=0.725975 valid_loss=0.715833\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 300: loss=0.435025 valid_loss=0.398075\n",
      "Epoch: 400: loss=0.305869 valid_loss=0.243894\n",
      "Epoch: 500: loss=0.287597 valid_loss=0.250323\n",
      "Epoch: 600: loss=0.181105 valid_loss=0.194553\n",
      "Epoch: 700: loss=0.164231 valid_loss=0.116042\n",
      "Epoch: 800: loss=0.198902 valid_loss=0.104406\n",
      "Epoch: 900: loss=0.150955 valid_loss=0.093288\n",
      "Epoch: 1000: loss=0.142198 valid_loss=0.087184\n",
      "Accuracy = 96.66666666666667%\n",
      "96.66666666666667%, 7/23 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100: loss=1.149310 valid_loss=1.105734\n",
      "Epoch: 200: loss=0.842419 valid_loss=0.824165\n",
      "Epoch: 300: loss=0.520222 valid_loss=0.418710\n",
      "Epoch: 400: loss=0.260847 valid_loss=0.188129\n",
      "Epoch: 500: loss=0.205810 valid_loss=0.137435\n",
      "Epoch: 600: loss=0.188064 valid_loss=0.124479\n",
      "Epoch: 700: loss=0.154672 valid_loss=0.085092\n",
      "Epoch: 800: loss=0.153928 valid_loss=0.085288\n",
      "Epoch: 900: loss=0.145977 valid_loss=0.073665\n",
      "Epoch: 1000: loss=0.184810 valid_loss=0.060676\n",
      "Accuracy = 96.66666666666667%\n",
      "96.66666666666667%, 10/25 features\n",
      "Epoch: 100: loss=1.150729 valid_loss=1.095946\n",
      "Epoch: 200: loss=0.857626 valid_loss=0.744811\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 300: loss=0.382858 valid_loss=0.335462\n",
      "Epoch: 400: loss=0.258033 valid_loss=0.176096\n",
      "Epoch: 500: loss=0.250343 valid_loss=0.118028\n",
      "Epoch: 600: loss=0.181771 valid_loss=0.091592\n",
      "Epoch: 700: loss=1.198259 valid_loss=0.271608\n",
      "Epoch: 800: loss=0.166779 valid_loss=0.091577\n",
      "Epoch: 900: loss=0.145695 valid_loss=0.058733\n",
      "Epoch: 1000: loss=0.160519 valid_loss=0.051756\n",
      "Accuracy = 100.0%\n",
      "100.0%, 11/27 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100: loss=1.171274 valid_loss=1.114805\n",
      "Epoch: 200: loss=1.149671 valid_loss=1.091015\n",
      "Epoch: 300: loss=0.709795 valid_loss=0.699904\n",
      "Epoch: 400: loss=0.515685 valid_loss=0.338779\n",
      "Epoch: 500: loss=0.264813 valid_loss=0.199489\n",
      "Epoch: 600: loss=0.218508 valid_loss=0.128931\n",
      "Epoch: 700: loss=0.166751 valid_loss=0.083397\n",
      "Epoch: 800: loss=0.190070 valid_loss=0.066806\n",
      "Epoch: 900: loss=0.154235 valid_loss=0.058387\n",
      "Epoch: 1000: loss=0.143358 valid_loss=0.048385\n",
      "Accuracy = 100.0%\n",
      "100.0%, 15/30 features\n",
      "Epoch: 100: loss=1.173810 valid_loss=1.112140\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 200: loss=1.045781 valid_loss=0.983555\n",
      "Epoch: 300: loss=0.575819 valid_loss=0.521362\n",
      "Epoch: 400: loss=0.338802 valid_loss=0.302211\n",
      "Epoch: 500: loss=0.248187 valid_loss=0.243256\n",
      "Epoch: 600: loss=0.214918 valid_loss=0.117904\n",
      "Epoch: 700: loss=0.171963 valid_loss=0.088430\n",
      "Epoch: 800: loss=0.162837 valid_loss=0.076658\n",
      "Epoch: 900: loss=0.154035 valid_loss=0.066095\n",
      "Epoch: 1000: loss=0.147230 valid_loss=0.057495\n",
      "Accuracy = 100.0%\n",
      "100.0%, 16/32 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100: loss=1.177389 valid_loss=1.117928\n",
      "Epoch: 200: loss=1.171264 valid_loss=1.109475\n",
      "Epoch: 300: loss=1.069149 valid_loss=1.019731\n",
      "Epoch: 400: loss=0.580525 valid_loss=0.494979\n",
      "Epoch: 500: loss=0.339254 valid_loss=0.257949\n",
      "Epoch: 600: loss=0.293762 valid_loss=0.217861\n",
      "Epoch: 700: loss=0.188491 valid_loss=0.100308\n",
      "Epoch: 800: loss=0.331054 valid_loss=0.107516\n",
      "Epoch: 900: loss=0.157431 valid_loss=0.063297\n",
      "Epoch: 1000: loss=0.169913 valid_loss=0.060760\n",
      "Accuracy = 100.0%\n",
      "100.0%, 17/34 features\n",
      "Epoch: 100: loss=1.127148 valid_loss=1.083141\n",
      "Epoch: 200: loss=0.750506 valid_loss=0.732951\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 300: loss=0.505491 valid_loss=0.483315\n",
      "Epoch: 400: loss=0.381472 valid_loss=0.333416\n",
      "Epoch: 500: loss=0.295447 valid_loss=0.247875\n",
      "Epoch: 600: loss=0.233184 valid_loss=0.163507\n",
      "Epoch: 700: loss=0.202791 valid_loss=0.128889\n",
      "Epoch: 800: loss=0.181600 valid_loss=0.098164\n",
      "Epoch: 900: loss=0.253471 valid_loss=0.137861\n",
      "Epoch: 1000: loss=0.147869 valid_loss=0.065772\n",
      "Accuracy = 100.0%\n",
      "100.0%, 18/36 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100: loss=1.155350 valid_loss=1.103094\n",
      "Epoch: 200: loss=0.753427 valid_loss=0.764261\n",
      "Epoch: 300: loss=0.459866 valid_loss=0.476262\n",
      "Epoch: 400: loss=0.563626 valid_loss=0.670018\n",
      "Epoch: 500: loss=0.290175 valid_loss=0.305618\n",
      "Epoch: 600: loss=0.224254 valid_loss=0.148324\n",
      "Epoch: 700: loss=0.435041 valid_loss=0.627469\n",
      "Epoch: 800: loss=0.185362 valid_loss=0.106521\n",
      "Epoch: 900: loss=0.172454 valid_loss=0.104311\n",
      "Epoch: 1000: loss=0.210748 valid_loss=0.119789\n",
      "Accuracy = 100.0%\n",
      "100.0%, 19/38 features\n",
      "Epoch: 100: loss=1.170905 valid_loss=1.113624\n",
      "Epoch: 200: loss=1.115546 valid_loss=1.068305\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 300: loss=0.855785 valid_loss=0.622750\n",
      "Epoch: 400: loss=0.325547 valid_loss=0.220492\n",
      "Epoch: 500: loss=0.262868 valid_loss=0.103268\n",
      "Epoch: 600: loss=0.228522 valid_loss=0.070358\n",
      "Epoch: 700: loss=0.179549 valid_loss=0.046630\n",
      "Epoch: 800: loss=0.173777 valid_loss=0.036958\n",
      "Epoch: 900: loss=0.162546 valid_loss=0.032662\n",
      "Epoch: 1000: loss=0.203013 valid_loss=0.029341\n",
      "Accuracy = 100.0%\n",
      "100.0%, 4/8 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100: loss=1.165048 valid_loss=1.111481\n",
      "Epoch: 200: loss=1.153022 valid_loss=1.059865\n",
      "Epoch: 300: loss=0.624550 valid_loss=0.591588\n",
      "Epoch: 400: loss=0.382030 valid_loss=0.387896\n",
      "Epoch: 500: loss=0.257138 valid_loss=0.257976\n",
      "Epoch: 600: loss=0.196181 valid_loss=0.130560\n",
      "Epoch: 700: loss=0.174725 valid_loss=0.099792\n",
      "Epoch: 800: loss=0.165228 valid_loss=0.089062\n",
      "Epoch: 900: loss=0.157025 valid_loss=0.079515\n",
      "Epoch: 1000: loss=0.151724 valid_loss=0.072418\n",
      "Accuracy = 96.66666666666667%\n",
      "96.66666666666667%, 3/10 features\n",
      "Epoch: 100: loss=1.172763 valid_loss=1.102650\n",
      "Epoch: 200: loss=0.871303 valid_loss=0.840070\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 300: loss=0.445084 valid_loss=0.399234\n",
      "Epoch: 400: loss=0.350848 valid_loss=0.257734\n",
      "Epoch: 500: loss=0.215572 valid_loss=0.144518\n",
      "Epoch: 600: loss=0.181023 valid_loss=0.107241\n",
      "Epoch: 700: loss=0.161690 valid_loss=0.089927\n",
      "Epoch: 800: loss=0.150626 valid_loss=0.078147\n",
      "Epoch: 900: loss=0.142737 valid_loss=0.071228\n",
      "Epoch: 1000: loss=0.136389 valid_loss=0.068294\n",
      "Accuracy = 100.0%\n",
      "100.0%, 2/12 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100: loss=1.134608 valid_loss=1.092474\n",
      "Epoch: 200: loss=0.787606 valid_loss=0.773064\n",
      "Epoch: 300: loss=1.273545 valid_loss=0.618637\n",
      "Epoch: 400: loss=0.506535 valid_loss=0.509439\n",
      "Epoch: 500: loss=0.346224 valid_loss=0.337507\n",
      "Epoch: 600: loss=0.266081 valid_loss=0.274312\n",
      "Epoch: 700: loss=0.220999 valid_loss=0.204189\n",
      "Epoch: 800: loss=0.194555 valid_loss=0.147632\n",
      "Epoch: 900: loss=0.191804 valid_loss=0.126034\n",
      "Epoch: 1000: loss=0.160729 valid_loss=0.109886\n",
      "Accuracy = 100.0%\n",
      "100.0%, 3/14 features\n",
      "Epoch: 100: loss=1.139843 valid_loss=1.089375\n",
      "Epoch: 200: loss=0.651586 valid_loss=0.685254\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 300: loss=0.400919 valid_loss=0.394019\n",
      "Epoch: 400: loss=0.282245 valid_loss=0.247657\n",
      "Epoch: 500: loss=0.232953 valid_loss=0.241512\n",
      "Epoch: 600: loss=0.193057 valid_loss=0.111458\n",
      "Epoch: 700: loss=0.176538 valid_loss=0.099654\n",
      "Epoch: 800: loss=0.219875 valid_loss=0.196699\n",
      "Epoch: 900: loss=0.153724 valid_loss=0.087604\n",
      "Epoch: 1000: loss=0.183928 valid_loss=0.067157\n",
      "Accuracy = 100.0%\n",
      "100.0%, 4/16 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100: loss=1.166609 valid_loss=1.114663\n",
      "Epoch: 200: loss=1.082491 valid_loss=1.054520\n",
      "Epoch: 300: loss=0.629698 valid_loss=0.626291\n",
      "Epoch: 400: loss=0.437334 valid_loss=0.436098\n",
      "Epoch: 500: loss=0.327366 valid_loss=0.242380\n",
      "Epoch: 600: loss=0.237475 valid_loss=0.168400\n",
      "Epoch: 700: loss=0.238036 valid_loss=0.119447\n",
      "Epoch: 800: loss=0.175952 valid_loss=0.093485\n",
      "Epoch: 900: loss=0.159995 valid_loss=0.083106\n",
      "Epoch: 1000: loss=0.252794 valid_loss=0.077176\n",
      "Accuracy = 96.66666666666667%\n",
      "96.66666666666667%, 5/19 features\n",
      "Epoch: 100: loss=1.160635 valid_loss=1.107738\n",
      "Epoch: 200: loss=0.979793 valid_loss=0.920540\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 300: loss=0.514083 valid_loss=0.456801\n",
      "Epoch: 400: loss=0.318228 valid_loss=0.297675\n",
      "Epoch: 500: loss=0.239702 valid_loss=0.168257\n",
      "Epoch: 600: loss=0.270588 valid_loss=0.131116\n",
      "Epoch: 700: loss=0.172153 valid_loss=0.096690\n",
      "Epoch: 800: loss=0.160094 valid_loss=0.075662\n",
      "Epoch: 900: loss=0.157767 valid_loss=0.070089\n",
      "Epoch: 1000: loss=0.150140 valid_loss=0.069088\n",
      "Accuracy = 100.0%\n",
      "100.0%, 7/21 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100: loss=1.164874 valid_loss=1.109923\n",
      "Epoch: 200: loss=1.015440 valid_loss=0.993306\n",
      "Epoch: 300: loss=0.529268 valid_loss=0.474532\n",
      "Epoch: 400: loss=0.292382 valid_loss=0.204479\n",
      "Epoch: 500: loss=0.208545 valid_loss=0.138589\n",
      "Epoch: 600: loss=0.179540 valid_loss=0.109316\n",
      "Epoch: 700: loss=0.170204 valid_loss=0.100679\n",
      "Epoch: 800: loss=0.159847 valid_loss=0.076478\n",
      "Epoch: 900: loss=0.149739 valid_loss=0.072731\n",
      "Epoch: 1000: loss=0.144819 valid_loss=0.070386\n",
      "Accuracy = 96.66666666666667%\n",
      "96.66666666666667%, 8/23 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100: loss=1.079845 valid_loss=1.038431\n",
      "Epoch: 200: loss=0.544844 valid_loss=0.459269\n",
      "Epoch: 300: loss=0.367047 valid_loss=0.202454\n",
      "Epoch: 400: loss=0.216239 valid_loss=0.127163\n",
      "Epoch: 500: loss=0.172120 valid_loss=0.098442\n",
      "Epoch: 600: loss=0.264092 valid_loss=0.115666\n",
      "Epoch: 700: loss=0.165732 valid_loss=0.069052\n",
      "Epoch: 800: loss=0.170417 valid_loss=0.075836\n",
      "Epoch: 900: loss=0.154637 valid_loss=0.060891\n",
      "Epoch: 1000: loss=0.135799 valid_loss=0.035767\n",
      "Accuracy = 100.0%\n",
      "100.0%, 10/25 features\n",
      "Epoch: 100: loss=1.171449 valid_loss=1.115630\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 200: loss=1.177076 valid_loss=1.075798\n",
      "Epoch: 300: loss=0.686622 valid_loss=0.601364\n",
      "Epoch: 400: loss=0.342431 valid_loss=0.257580\n",
      "Epoch: 500: loss=0.228141 valid_loss=0.178255\n",
      "Epoch: 600: loss=0.203606 valid_loss=0.119245\n",
      "Epoch: 700: loss=0.185958 valid_loss=0.087320\n",
      "Epoch: 800: loss=0.167728 valid_loss=0.074554\n",
      "Epoch: 900: loss=0.153308 valid_loss=0.062462\n",
      "Epoch: 1000: loss=0.156354 valid_loss=0.061766\n",
      "Accuracy = 100.0%\n",
      "100.0%, 13/27 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100: loss=1.154365 valid_loss=1.108065\n",
      "Epoch: 200: loss=0.904390 valid_loss=0.877045\n",
      "Epoch: 300: loss=0.520926 valid_loss=0.537959\n",
      "Epoch: 400: loss=0.358230 valid_loss=0.344305\n",
      "Epoch: 500: loss=0.312392 valid_loss=0.228197\n",
      "Epoch: 600: loss=0.218411 valid_loss=0.148532\n",
      "Epoch: 700: loss=0.173543 valid_loss=0.111104\n",
      "Epoch: 800: loss=0.193550 valid_loss=0.101979\n",
      "Epoch: 900: loss=0.167413 valid_loss=0.084299\n",
      "Epoch: 1000: loss=0.148818 valid_loss=0.079846\n",
      "Accuracy = 100.0%\n",
      "100.0%, 15/30 features\n",
      "Epoch: 100: loss=1.171433 valid_loss=1.109751\n",
      "Epoch: 200: loss=1.104826 valid_loss=1.006798\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 300: loss=0.474293 valid_loss=0.385524\n",
      "Epoch: 400: loss=0.411788 valid_loss=0.226496\n",
      "Epoch: 500: loss=0.199084 valid_loss=0.196292\n",
      "Epoch: 600: loss=0.275333 valid_loss=0.254760\n",
      "Epoch: 700: loss=0.181078 valid_loss=0.085828\n",
      "Epoch: 800: loss=0.149322 valid_loss=0.084351\n",
      "Epoch: 900: loss=0.138161 valid_loss=0.078594\n",
      "Epoch: 1000: loss=0.134312 valid_loss=0.091815\n",
      "Accuracy = 96.66666666666667%\n",
      "96.66666666666667%, 16/32 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100: loss=1.176477 valid_loss=1.119877\n",
      "Epoch: 200: loss=1.172306 valid_loss=1.112399\n",
      "Epoch: 300: loss=1.088994 valid_loss=0.995379\n",
      "Epoch: 400: loss=0.662297 valid_loss=0.560744\n",
      "Epoch: 500: loss=0.284140 valid_loss=0.278825\n",
      "Epoch: 600: loss=0.257954 valid_loss=0.160893\n",
      "Epoch: 700: loss=0.201881 valid_loss=0.131558\n",
      "Epoch: 800: loss=0.208945 valid_loss=0.100029\n",
      "Epoch: 900: loss=0.236156 valid_loss=0.089810\n",
      "Epoch: 1000: loss=0.189291 valid_loss=0.096020\n",
      "Accuracy = 100.0%\n",
      "100.0%, 17/34 features\n",
      "Epoch: 100: loss=1.177026 valid_loss=1.118798\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 200: loss=1.166377 valid_loss=1.104544\n",
      "Epoch: 300: loss=0.852312 valid_loss=0.826259\n",
      "Epoch: 400: loss=0.404935 valid_loss=0.361571\n",
      "Epoch: 500: loss=0.220666 valid_loss=0.241761\n",
      "Epoch: 600: loss=0.187054 valid_loss=0.113512\n",
      "Epoch: 700: loss=0.168175 valid_loss=0.084312\n",
      "Epoch: 800: loss=0.151513 valid_loss=0.063804\n",
      "Epoch: 900: loss=0.148650 valid_loss=0.058400\n",
      "Epoch: 1000: loss=0.158173 valid_loss=0.050679\n",
      "Accuracy = 100.0%\n",
      "100.0%, 18/36 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100: loss=1.172625 valid_loss=1.117428\n",
      "Epoch: 200: loss=1.150101 valid_loss=1.092042\n",
      "Epoch: 300: loss=0.706013 valid_loss=0.704561\n",
      "Epoch: 400: loss=0.399767 valid_loss=0.458358\n",
      "Epoch: 500: loss=0.355116 valid_loss=0.749483\n",
      "Epoch: 600: loss=0.210734 valid_loss=0.126261\n",
      "Epoch: 700: loss=0.217770 valid_loss=0.099805\n",
      "Epoch: 800: loss=0.203612 valid_loss=0.103988\n",
      "Epoch: 900: loss=0.167336 valid_loss=0.066835\n",
      "Epoch: 1000: loss=0.147855 valid_loss=0.100260\n",
      "Accuracy = 93.33333333333333%\n",
      "93.33333333333333%, 19/38 features\n",
      "Epoch: 100: loss=1.151021 valid_loss=1.093453\n",
      "Epoch: 200: loss=0.880004 valid_loss=0.794249\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 300: loss=0.403559 valid_loss=0.319821\n",
      "Epoch: 400: loss=0.309180 valid_loss=0.309856\n",
      "Epoch: 500: loss=0.190208 valid_loss=0.071442\n",
      "Epoch: 600: loss=0.173889 valid_loss=0.048245\n",
      "Epoch: 700: loss=0.546059 valid_loss=0.069825\n",
      "Epoch: 800: loss=0.166917 valid_loss=0.034075\n",
      "Epoch: 900: loss=0.160518 valid_loss=0.030273\n",
      "Epoch: 1000: loss=0.158016 valid_loss=0.027632\n",
      "Accuracy = 100.0%\n",
      "100.0%, 4/8 features\n",
      "Epoch: 100: loss=1.157967 valid_loss=1.102425\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 200: loss=0.833042 valid_loss=0.828506\n",
      "Epoch: 300: loss=0.451835 valid_loss=0.439570\n",
      "Epoch: 400: loss=0.294650 valid_loss=0.239816\n",
      "Epoch: 500: loss=0.223236 valid_loss=0.164740\n",
      "Epoch: 600: loss=0.191156 valid_loss=0.120213\n",
      "Epoch: 700: loss=0.172745 valid_loss=0.099600\n",
      "Epoch: 800: loss=0.166026 valid_loss=0.086265\n",
      "Epoch: 900: loss=0.155931 valid_loss=0.078525\n",
      "Epoch: 1000: loss=0.151526 valid_loss=0.073109\n",
      "Accuracy = 96.66666666666667%\n",
      "96.66666666666667%, 3/10 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100: loss=1.174147 valid_loss=1.118274\n",
      "Epoch: 200: loss=1.157824 valid_loss=1.111623\n",
      "Epoch: 300: loss=1.084707 valid_loss=1.057246\n",
      "Epoch: 400: loss=0.592458 valid_loss=0.600429\n",
      "Epoch: 500: loss=0.394580 valid_loss=0.293308\n",
      "Epoch: 600: loss=0.226705 valid_loss=0.162588\n",
      "Epoch: 700: loss=0.184015 valid_loss=0.124079\n",
      "Epoch: 800: loss=0.155379 valid_loss=0.100163\n",
      "Epoch: 900: loss=0.145784 valid_loss=0.082907\n",
      "Epoch: 1000: loss=0.138255 valid_loss=0.077198\n",
      "Accuracy = 96.66666666666667%\n",
      "96.66666666666667%, 2/12 features\n",
      "Epoch: 100: loss=1.141726 valid_loss=1.095952\n",
      "Epoch: 200: loss=0.787193 valid_loss=0.764572\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 300: loss=0.422241 valid_loss=0.384972\n",
      "Epoch: 400: loss=0.308117 valid_loss=0.267536\n",
      "Epoch: 500: loss=0.212648 valid_loss=0.143480\n",
      "Epoch: 600: loss=0.183201 valid_loss=0.108278\n",
      "Epoch: 700: loss=0.167367 valid_loss=0.089452\n",
      "Epoch: 800: loss=0.174293 valid_loss=0.076792\n",
      "Epoch: 900: loss=0.158164 valid_loss=0.079464\n",
      "Epoch: 1000: loss=0.148183 valid_loss=0.066250\n",
      "Accuracy = 96.66666666666667%\n",
      "96.66666666666667%, 4/14 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100: loss=1.163454 valid_loss=1.109482\n",
      "Epoch: 200: loss=1.067598 valid_loss=1.015144\n",
      "Epoch: 300: loss=0.552973 valid_loss=0.533504\n",
      "Epoch: 400: loss=0.803626 valid_loss=0.812614\n",
      "Epoch: 500: loss=0.231999 valid_loss=0.174760\n",
      "Epoch: 600: loss=0.183060 valid_loss=0.128085\n",
      "Epoch: 700: loss=0.160799 valid_loss=0.092123\n",
      "Epoch: 800: loss=0.161873 valid_loss=0.080908\n",
      "Epoch: 900: loss=0.153729 valid_loss=0.073215\n",
      "Epoch: 1000: loss=0.138283 valid_loss=0.064970\n",
      "Accuracy = 100.0%\n",
      "100.0%, 4/16 features\n",
      "Epoch: 100: loss=1.118710 valid_loss=1.084979\n",
      "Epoch: 200: loss=0.618843 valid_loss=0.617955\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 300: loss=0.444234 valid_loss=0.370201\n",
      "Epoch: 400: loss=0.267036 valid_loss=0.289831\n",
      "Epoch: 500: loss=0.247338 valid_loss=0.147214\n",
      "Epoch: 600: loss=0.187879 valid_loss=0.120556\n",
      "Epoch: 700: loss=0.176866 valid_loss=0.105603\n",
      "Epoch: 800: loss=0.174991 valid_loss=0.100471\n",
      "Epoch: 900: loss=0.155502 valid_loss=0.090619\n",
      "Epoch: 1000: loss=0.145628 valid_loss=0.081720\n",
      "Accuracy = 96.66666666666667%\n",
      "96.66666666666667%, 5/19 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100: loss=1.131457 valid_loss=1.093253\n",
      "Epoch: 200: loss=0.910540 valid_loss=0.732976\n",
      "Epoch: 300: loss=0.539538 valid_loss=0.529145\n",
      "Epoch: 400: loss=0.426337 valid_loss=0.440587\n",
      "Epoch: 500: loss=0.377958 valid_loss=0.376414\n",
      "Epoch: 600: loss=0.262846 valid_loss=0.186979\n",
      "Epoch: 700: loss=0.216475 valid_loss=0.144645\n",
      "Epoch: 800: loss=0.178888 valid_loss=0.122370\n",
      "Epoch: 900: loss=0.181699 valid_loss=0.103753\n",
      "Epoch: 1000: loss=0.166236 valid_loss=0.083376\n",
      "Accuracy = 96.66666666666667%\n",
      "96.66666666666667%, 7/21 features\n",
      "Epoch: 100: loss=1.174115 valid_loss=1.105342\n",
      "Epoch: 200: loss=0.847494 valid_loss=0.858654\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 300: loss=0.513383 valid_loss=0.475347\n",
      "Epoch: 400: loss=0.375306 valid_loss=0.347593\n",
      "Epoch: 500: loss=0.249122 valid_loss=0.237432\n",
      "Epoch: 600: loss=0.241583 valid_loss=0.128530\n",
      "Epoch: 700: loss=0.182707 valid_loss=0.098390\n",
      "Epoch: 800: loss=0.162612 valid_loss=0.081691\n",
      "Epoch: 900: loss=0.190732 valid_loss=0.120394\n",
      "Epoch: 1000: loss=0.142605 valid_loss=0.070587\n",
      "Accuracy = 100.0%\n",
      "100.0%, 7/23 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100: loss=1.163677 valid_loss=1.107645\n",
      "Epoch: 200: loss=1.060979 valid_loss=1.012899\n",
      "Epoch: 300: loss=0.585044 valid_loss=0.525984\n",
      "Epoch: 400: loss=0.337664 valid_loss=0.249520\n",
      "Epoch: 500: loss=0.248815 valid_loss=0.128859\n",
      "Epoch: 600: loss=0.195075 valid_loss=0.111915\n",
      "Epoch: 700: loss=0.175030 valid_loss=0.078743\n",
      "Epoch: 800: loss=0.153192 valid_loss=0.062241\n",
      "Epoch: 900: loss=0.193203 valid_loss=0.052347\n",
      "Epoch: 1000: loss=0.161178 valid_loss=0.073135\n",
      "Accuracy = 93.33333333333333%\n",
      "93.33333333333333%, 9/25 features\n",
      "Epoch: 100: loss=1.174795 valid_loss=1.113490\n",
      "Epoch: 200: loss=1.130274 valid_loss=1.080508\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 300: loss=0.783770 valid_loss=0.686609\n",
      "Epoch: 400: loss=0.425128 valid_loss=0.386717\n",
      "Epoch: 500: loss=0.249061 valid_loss=0.233815\n",
      "Epoch: 600: loss=0.197124 valid_loss=0.112743\n",
      "Epoch: 700: loss=0.167324 valid_loss=0.095540\n",
      "Epoch: 800: loss=0.165439 valid_loss=0.070694\n",
      "Epoch: 900: loss=0.142871 valid_loss=0.057572\n",
      "Epoch: 1000: loss=0.229093 valid_loss=0.051008\n",
      "Accuracy = 100.0%\n",
      "100.0%, 11/27 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100: loss=1.154726 valid_loss=1.106143\n",
      "Epoch: 200: loss=1.079257 valid_loss=0.888398\n",
      "Epoch: 300: loss=0.571368 valid_loss=0.556805\n",
      "Epoch: 400: loss=0.443521 valid_loss=0.453677\n",
      "Epoch: 500: loss=0.338048 valid_loss=0.301801\n",
      "Epoch: 600: loss=0.288916 valid_loss=0.245751\n",
      "Epoch: 700: loss=0.214123 valid_loss=0.148537\n",
      "Epoch: 800: loss=0.177316 valid_loss=0.127924\n",
      "Epoch: 900: loss=0.186435 valid_loss=0.123442\n",
      "Epoch: 1000: loss=0.154920 valid_loss=0.083147\n",
      "Accuracy = 100.0%\n",
      "100.0%, 15/30 features\n",
      "Epoch: 100: loss=1.176311 valid_loss=1.121412\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 200: loss=1.172951 valid_loss=1.118113\n",
      "Epoch: 300: loss=1.145638 valid_loss=1.099684\n",
      "Epoch: 400: loss=0.874269 valid_loss=0.838992\n",
      "Epoch: 500: loss=0.474612 valid_loss=0.416954\n",
      "Epoch: 600: loss=0.294092 valid_loss=0.276350\n",
      "Epoch: 700: loss=0.211628 valid_loss=0.165872\n",
      "Epoch: 800: loss=0.188798 valid_loss=0.099017\n",
      "Epoch: 900: loss=0.162278 valid_loss=0.077653\n",
      "Epoch: 1000: loss=0.154561 valid_loss=0.060225\n",
      "Accuracy = 100.0%\n",
      "100.0%, 16/32 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100: loss=1.177445 valid_loss=1.120436\n",
      "Epoch: 200: loss=1.150485 valid_loss=1.112999\n",
      "Epoch: 300: loss=0.848572 valid_loss=0.875688\n",
      "Epoch: 400: loss=0.387883 valid_loss=0.393918\n",
      "Epoch: 500: loss=0.272103 valid_loss=0.304757\n",
      "Epoch: 600: loss=0.256992 valid_loss=0.163472\n",
      "Epoch: 700: loss=0.362677 valid_loss=0.135177\n",
      "Epoch: 800: loss=0.174250 valid_loss=0.121611\n",
      "Epoch: 900: loss=0.156190 valid_loss=0.134231\n",
      "Epoch: 1000: loss=0.199535 valid_loss=0.104027\n",
      "Accuracy = 96.66666666666667%\n",
      "96.66666666666667%, 17/34 features\n",
      "Epoch: 100: loss=1.171095 valid_loss=1.118376\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 200: loss=1.112244 valid_loss=1.038034\n",
      "Epoch: 300: loss=0.738141 valid_loss=0.795340\n",
      "Epoch: 400: loss=0.446578 valid_loss=0.452516\n",
      "Epoch: 500: loss=0.358224 valid_loss=0.332764\n",
      "Epoch: 600: loss=0.263415 valid_loss=0.216092\n",
      "Epoch: 700: loss=0.198210 valid_loss=0.166783\n",
      "Epoch: 800: loss=0.182627 valid_loss=0.121214\n",
      "Epoch: 900: loss=0.191770 valid_loss=0.093459\n",
      "Epoch: 1000: loss=0.304905 valid_loss=0.084500\n",
      "Accuracy = 96.66666666666667%\n",
      "96.66666666666667%, 18/36 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100: loss=1.121289 valid_loss=1.081388\n",
      "Epoch: 200: loss=0.707976 valid_loss=0.624694\n",
      "Epoch: 300: loss=0.423250 valid_loss=0.372910\n",
      "Epoch: 400: loss=0.295125 valid_loss=0.243862\n",
      "Epoch: 500: loss=0.236419 valid_loss=0.143583\n",
      "Epoch: 600: loss=0.190176 valid_loss=0.120647\n",
      "Epoch: 700: loss=0.198043 valid_loss=0.099676\n",
      "Epoch: 800: loss=0.172273 valid_loss=0.083589\n",
      "Epoch: 900: loss=0.175036 valid_loss=0.078705\n",
      "Epoch: 1000: loss=0.163070 valid_loss=0.070701\n",
      "Accuracy = 100.0%\n",
      "100.0%, 19/38 features\n",
      "Epoch: 100: loss=1.167887 valid_loss=1.108018\n",
      "Epoch: 200: loss=1.110345 valid_loss=1.037773\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 300: loss=0.656452 valid_loss=0.605729\n",
      "Epoch: 400: loss=0.351107 valid_loss=0.235906\n",
      "Epoch: 500: loss=0.231637 valid_loss=0.092110\n",
      "Epoch: 600: loss=0.181334 valid_loss=0.058550\n",
      "Epoch: 700: loss=0.169761 valid_loss=0.042800\n",
      "Epoch: 800: loss=0.165871 valid_loss=0.037395\n",
      "Epoch: 900: loss=0.161620 valid_loss=0.032168\n",
      "Epoch: 1000: loss=0.241038 valid_loss=0.030272\n",
      "Accuracy = 100.0%\n",
      "100.0%, 4/8 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100: loss=1.170959 valid_loss=1.119647\n",
      "Epoch: 200: loss=1.149960 valid_loss=1.111190\n",
      "Epoch: 300: loss=1.052323 valid_loss=0.996358\n",
      "Epoch: 400: loss=0.634658 valid_loss=0.615759\n",
      "Epoch: 500: loss=0.505295 valid_loss=0.504724\n",
      "Epoch: 600: loss=0.391658 valid_loss=0.372148\n",
      "Epoch: 700: loss=0.320967 valid_loss=0.280978\n",
      "Epoch: 800: loss=0.266909 valid_loss=0.233914\n",
      "Epoch: 900: loss=0.289798 valid_loss=0.264844\n",
      "Epoch: 1000: loss=0.205942 valid_loss=0.155972\n",
      "Accuracy = 96.66666666666667%\n",
      "96.66666666666667%, 3/10 features\n",
      "Epoch: 100: loss=1.118714 valid_loss=1.073635\n",
      "Epoch: 200: loss=0.856635 valid_loss=0.703369\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 300: loss=0.463102 valid_loss=0.430350\n",
      "Epoch: 400: loss=0.336566 valid_loss=0.281086\n",
      "Epoch: 500: loss=0.263474 valid_loss=0.358907\n",
      "Epoch: 600: loss=0.204930 valid_loss=0.135906\n",
      "Epoch: 700: loss=0.181254 valid_loss=0.114255\n",
      "Epoch: 800: loss=0.166654 valid_loss=0.096092\n",
      "Epoch: 900: loss=0.157228 valid_loss=0.084910\n",
      "Epoch: 1000: loss=0.151283 valid_loss=0.077900\n",
      "Accuracy = 96.66666666666667%\n",
      "96.66666666666667%, 3/12 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100: loss=1.158888 valid_loss=1.106116\n",
      "Epoch: 200: loss=0.917600 valid_loss=0.920214\n",
      "Epoch: 300: loss=0.471272 valid_loss=0.549810\n",
      "Epoch: 400: loss=0.316266 valid_loss=0.293193\n",
      "Epoch: 500: loss=0.255105 valid_loss=0.193173\n",
      "Epoch: 600: loss=0.190750 valid_loss=0.133662\n",
      "Epoch: 700: loss=0.204694 valid_loss=0.102666\n",
      "Epoch: 800: loss=0.198543 valid_loss=0.094087\n",
      "Epoch: 900: loss=0.179848 valid_loss=0.100199\n",
      "Epoch: 1000: loss=0.151770 valid_loss=0.071944\n",
      "Accuracy = 96.66666666666667%\n",
      "96.66666666666667%, 5/14 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100: loss=1.161981 valid_loss=1.115536\n",
      "Epoch: 200: loss=0.936288 valid_loss=0.965335\n",
      "Epoch: 300: loss=0.504326 valid_loss=0.500173\n",
      "Epoch: 400: loss=0.485007 valid_loss=0.342070\n",
      "Epoch: 500: loss=0.229475 valid_loss=0.168304\n",
      "Epoch: 600: loss=0.197219 valid_loss=0.130418\n",
      "Epoch: 700: loss=0.173393 valid_loss=0.104354\n",
      "Epoch: 800: loss=0.165122 valid_loss=0.086794\n",
      "Epoch: 900: loss=0.180714 valid_loss=0.136518\n",
      "Epoch: 1000: loss=0.147151 valid_loss=0.074447\n",
      "Accuracy = 100.0%\n",
      "100.0%, 5/16 features\n",
      "Epoch: 100: loss=1.155210 valid_loss=1.101195\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 200: loss=0.797758 valid_loss=0.775150\n",
      "Epoch: 300: loss=0.439735 valid_loss=0.443767\n",
      "Epoch: 400: loss=0.265523 valid_loss=0.154424\n",
      "Epoch: 500: loss=0.233421 valid_loss=0.181892\n",
      "Epoch: 600: loss=0.286102 valid_loss=0.100790\n",
      "Epoch: 700: loss=0.198942 valid_loss=0.075881\n",
      "Epoch: 800: loss=0.191814 valid_loss=0.074312\n",
      "Epoch: 900: loss=0.183562 valid_loss=0.067144\n",
      "Epoch: 1000: loss=0.179029 valid_loss=0.064162\n",
      "Accuracy = 100.0%\n",
      "100.0%, 7/19 features\n",
      "Epoch: 100: loss=1.151268 valid_loss=1.098648\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 200: loss=0.760632 valid_loss=0.752752\n",
      "Epoch: 300: loss=0.510708 valid_loss=0.402839\n",
      "Epoch: 400: loss=0.277310 valid_loss=0.264708\n",
      "Epoch: 500: loss=0.236859 valid_loss=0.140405\n",
      "Epoch: 600: loss=0.197762 valid_loss=0.102903\n",
      "Epoch: 700: loss=0.197671 valid_loss=0.088554\n",
      "Epoch: 800: loss=0.177673 valid_loss=0.086736\n",
      "Epoch: 900: loss=0.179091 valid_loss=0.069062\n",
      "Epoch: 1000: loss=0.158450 valid_loss=0.055738\n",
      "Accuracy = 100.0%\n",
      "100.0%, 7/21 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100: loss=1.168702 valid_loss=1.115776\n",
      "Epoch: 200: loss=1.102728 valid_loss=1.043893\n",
      "Epoch: 300: loss=0.552960 valid_loss=0.530739\n",
      "Epoch: 400: loss=0.294355 valid_loss=0.247814\n",
      "Epoch: 500: loss=0.217441 valid_loss=0.173806\n",
      "Epoch: 600: loss=0.178830 valid_loss=0.114828\n",
      "Epoch: 700: loss=0.165355 valid_loss=0.094453\n",
      "Epoch: 800: loss=0.155645 valid_loss=0.086266\n",
      "Epoch: 900: loss=0.144464 valid_loss=0.083744\n",
      "Epoch: 1000: loss=0.153868 valid_loss=0.076910\n",
      "Accuracy = 96.66666666666667%\n",
      "96.66666666666667%, 6/23 features\n",
      "Epoch: 100: loss=1.174762 valid_loss=1.119430\n",
      "Epoch: 200: loss=1.152768 valid_loss=1.101564\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 300: loss=0.940149 valid_loss=0.837568\n",
      "Epoch: 400: loss=0.394701 valid_loss=0.320996\n",
      "Epoch: 500: loss=0.246036 valid_loss=0.166784\n",
      "Epoch: 600: loss=0.213437 valid_loss=0.129508\n",
      "Epoch: 700: loss=0.206639 valid_loss=0.116479\n",
      "Epoch: 800: loss=0.175967 valid_loss=0.091966\n",
      "Epoch: 900: loss=0.151680 valid_loss=0.080375\n",
      "Epoch: 1000: loss=0.164733 valid_loss=0.075887\n",
      "Accuracy = 96.66666666666667%\n",
      "96.66666666666667%, 10/25 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100: loss=1.173723 valid_loss=1.110121\n",
      "Epoch: 200: loss=1.123142 valid_loss=1.050917\n",
      "Epoch: 300: loss=0.595874 valid_loss=0.534899\n",
      "Epoch: 400: loss=0.406478 valid_loss=0.228856\n",
      "Epoch: 500: loss=0.211396 valid_loss=0.127159\n",
      "Epoch: 600: loss=0.306575 valid_loss=0.090681\n",
      "Epoch: 700: loss=0.172902 valid_loss=0.069367\n",
      "Epoch: 800: loss=0.754384 valid_loss=0.082194\n",
      "Epoch: 900: loss=0.157066 valid_loss=0.055241\n",
      "Epoch: 1000: loss=0.140712 valid_loss=0.045590\n",
      "Accuracy = 100.0%\n",
      "100.0%, 13/27 features\n",
      "Epoch: 100: loss=1.177146 valid_loss=1.112431\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 200: loss=1.091032 valid_loss=1.023413\n",
      "Epoch: 300: loss=0.479717 valid_loss=0.445849\n",
      "Epoch: 400: loss=0.290271 valid_loss=0.217249\n",
      "Epoch: 500: loss=0.220341 valid_loss=0.136008\n",
      "Epoch: 600: loss=0.199616 valid_loss=0.136706\n",
      "Epoch: 700: loss=0.160715 valid_loss=0.075596\n",
      "Epoch: 800: loss=0.155896 valid_loss=0.069943\n",
      "Epoch: 900: loss=0.162602 valid_loss=0.058905\n",
      "Epoch: 1000: loss=0.222505 valid_loss=0.051551\n",
      "Accuracy = 100.0%\n",
      "100.0%, 15/30 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100: loss=1.133120 valid_loss=1.087970\n",
      "Epoch: 200: loss=0.647190 valid_loss=0.581554\n",
      "Epoch: 300: loss=0.319087 valid_loss=0.319529\n",
      "Epoch: 400: loss=0.233074 valid_loss=0.203196\n",
      "Epoch: 500: loss=0.201827 valid_loss=0.115002\n",
      "Epoch: 600: loss=0.214379 valid_loss=0.087609\n",
      "Epoch: 700: loss=0.198969 valid_loss=0.079451\n",
      "Epoch: 800: loss=0.171179 valid_loss=0.096539\n",
      "Epoch: 900: loss=0.194759 valid_loss=0.058965\n",
      "Epoch: 1000: loss=0.182161 valid_loss=0.052441\n",
      "Accuracy = 96.66666666666667%\n",
      "96.66666666666667%, 16/32 features\n",
      "Epoch: 100: loss=1.169637 valid_loss=1.119824\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 200: loss=1.129167 valid_loss=1.083767\n",
      "Epoch: 300: loss=0.689096 valid_loss=0.641258\n",
      "Epoch: 400: loss=0.334602 valid_loss=0.275035\n",
      "Epoch: 500: loss=0.408025 valid_loss=0.195209\n",
      "Epoch: 600: loss=0.235320 valid_loss=0.158658\n",
      "Epoch: 700: loss=0.276405 valid_loss=0.152868\n",
      "Epoch: 800: loss=0.175018 valid_loss=0.112340\n",
      "Epoch: 900: loss=0.169716 valid_loss=0.096613\n",
      "Epoch: 1000: loss=0.162212 valid_loss=0.089008\n",
      "Accuracy = 96.66666666666667%\n",
      "96.66666666666667%, 17/34 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100: loss=1.091022 valid_loss=1.049179\n",
      "Epoch: 200: loss=0.527949 valid_loss=0.521891\n",
      "Epoch: 300: loss=0.308855 valid_loss=0.311844\n",
      "Epoch: 400: loss=0.243114 valid_loss=0.156080\n",
      "Epoch: 500: loss=0.224777 valid_loss=0.108796\n",
      "Epoch: 600: loss=0.173266 valid_loss=0.099786\n",
      "Epoch: 700: loss=0.158522 valid_loss=0.077156\n",
      "Epoch: 800: loss=0.183946 valid_loss=0.078275\n",
      "Epoch: 900: loss=0.162281 valid_loss=0.081382\n",
      "Epoch: 1000: loss=0.217208 valid_loss=0.077461\n",
      "Accuracy = 93.33333333333333%\n",
      "93.33333333333333%, 18/36 features\n",
      "Epoch: 100: loss=1.164022 valid_loss=1.098137\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 200: loss=0.737402 valid_loss=0.755090\n",
      "Epoch: 300: loss=0.524524 valid_loss=0.600047\n",
      "Epoch: 400: loss=0.374012 valid_loss=0.238180\n",
      "Epoch: 500: loss=0.224404 valid_loss=0.311197\n",
      "Epoch: 600: loss=0.219535 valid_loss=0.269167\n",
      "Epoch: 700: loss=0.174526 valid_loss=0.156329\n",
      "Epoch: 800: loss=0.152826 valid_loss=0.115013\n",
      "Epoch: 900: loss=0.211364 valid_loss=0.117099\n",
      "Epoch: 1000: loss=0.158176 valid_loss=0.104339\n",
      "Accuracy = 100.0%\n",
      "100.0%, 19/38 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100: loss=1.148776 valid_loss=1.090432\n",
      "Epoch: 200: loss=0.729832 valid_loss=0.664612\n",
      "Epoch: 300: loss=0.469387 valid_loss=0.294329\n",
      "Epoch: 400: loss=0.243939 valid_loss=0.173109\n",
      "Epoch: 500: loss=0.221861 valid_loss=0.098297\n",
      "Epoch: 600: loss=0.186348 valid_loss=0.063664\n",
      "Epoch: 700: loss=0.175845 valid_loss=0.049664\n",
      "Epoch: 800: loss=0.166901 valid_loss=0.041068\n",
      "Epoch: 900: loss=0.167536 valid_loss=0.039559\n",
      "Epoch: 1000: loss=0.162379 valid_loss=0.030933\n",
      "Accuracy = 100.0%\n",
      "100.0%, 4/8 features\n",
      "Epoch: 100: loss=1.164061 valid_loss=1.116474\n",
      "Epoch: 200: loss=1.137685 valid_loss=1.072321\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 300: loss=0.679398 valid_loss=0.671530\n",
      "Epoch: 400: loss=0.460501 valid_loss=0.424452\n",
      "Epoch: 500: loss=0.361039 valid_loss=0.316015\n",
      "Epoch: 600: loss=0.273833 valid_loss=0.244699\n",
      "Epoch: 700: loss=0.217898 valid_loss=0.179397\n",
      "Epoch: 800: loss=0.184486 valid_loss=0.144559\n",
      "Epoch: 900: loss=0.163392 valid_loss=0.119275\n",
      "Epoch: 1000: loss=0.152771 valid_loss=0.103269\n",
      "Accuracy = 96.66666666666667%\n",
      "96.66666666666667%, 2/10 features\n",
      "Epoch: 100: loss=1.166634 valid_loss=1.108230\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 200: loss=0.957396 valid_loss=0.950808\n",
      "Epoch: 300: loss=0.525284 valid_loss=0.503920\n",
      "Epoch: 400: loss=0.349718 valid_loss=0.291510\n",
      "Epoch: 500: loss=0.242919 valid_loss=0.190486\n",
      "Epoch: 600: loss=0.197217 valid_loss=0.141222\n",
      "Epoch: 700: loss=0.168522 valid_loss=0.109550\n",
      "Epoch: 800: loss=0.152380 valid_loss=0.094772\n",
      "Epoch: 900: loss=0.143077 valid_loss=0.084905\n",
      "Epoch: 1000: loss=0.135878 valid_loss=0.078667\n",
      "Accuracy = 96.66666666666667%\n",
      "96.66666666666667%, 2/12 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100: loss=1.142722 valid_loss=1.105240\n",
      "Epoch: 200: loss=0.954862 valid_loss=0.865782\n",
      "Epoch: 300: loss=0.513887 valid_loss=0.484390\n",
      "Epoch: 400: loss=0.370014 valid_loss=0.333046\n",
      "Epoch: 500: loss=0.304923 valid_loss=0.310817\n",
      "Epoch: 600: loss=0.231350 valid_loss=0.201391\n",
      "Epoch: 700: loss=0.189838 valid_loss=0.116957\n",
      "Epoch: 800: loss=0.175203 valid_loss=0.100351\n",
      "Epoch: 900: loss=0.163901 valid_loss=0.089560\n",
      "Epoch: 1000: loss=0.158733 valid_loss=0.081690\n",
      "Accuracy = 96.66666666666667%\n",
      "96.66666666666667%, 4/14 features\n",
      "Epoch: 100: loss=1.166082 valid_loss=1.109075\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 200: loss=0.992505 valid_loss=0.991081\n",
      "Epoch: 300: loss=0.576426 valid_loss=0.522431\n",
      "Epoch: 400: loss=0.338070 valid_loss=0.315480\n",
      "Epoch: 500: loss=0.242654 valid_loss=0.330622\n",
      "Epoch: 600: loss=0.207025 valid_loss=0.115175\n",
      "Epoch: 700: loss=0.187650 valid_loss=0.104137\n",
      "Epoch: 800: loss=0.177427 valid_loss=0.080649\n",
      "Epoch: 900: loss=0.160184 valid_loss=0.079323\n",
      "Epoch: 1000: loss=0.141945 valid_loss=0.066039\n",
      "Accuracy = 100.0%\n",
      "100.0%, 5/16 features\n",
      "Epoch: 100: loss=1.154233 valid_loss=1.098609\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 200: loss=0.923927 valid_loss=0.726081\n",
      "Epoch: 300: loss=0.436609 valid_loss=0.306406\n",
      "Epoch: 400: loss=0.269451 valid_loss=0.192136\n",
      "Epoch: 500: loss=0.228102 valid_loss=0.109056\n",
      "Epoch: 600: loss=0.226917 valid_loss=0.098189\n",
      "Epoch: 700: loss=0.191264 valid_loss=0.080414\n",
      "Epoch: 800: loss=0.180376 valid_loss=0.082682\n",
      "Epoch: 900: loss=0.178210 valid_loss=0.081735\n",
      "Epoch: 1000: loss=0.170388 valid_loss=0.075009\n",
      "Accuracy = 96.66666666666667%\n",
      "96.66666666666667%, 6/19 features\n",
      "Epoch: 100: loss=1.174692 valid_loss=1.112145\n",
      "Epoch: 200: loss=1.067605 valid_loss=1.031282\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 300: loss=0.537175 valid_loss=0.472753\n",
      "Epoch: 400: loss=0.339865 valid_loss=0.393401\n",
      "Epoch: 500: loss=0.301394 valid_loss=0.243820\n",
      "Epoch: 600: loss=0.204604 valid_loss=0.149135\n",
      "Epoch: 700: loss=0.184324 valid_loss=0.090704\n",
      "Epoch: 800: loss=0.163763 valid_loss=0.074271\n",
      "Epoch: 900: loss=0.165045 valid_loss=0.085772\n",
      "Epoch: 1000: loss=0.143434 valid_loss=0.063681\n",
      "Accuracy = 100.0%\n",
      "100.0%, 5/21 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100: loss=1.167822 valid_loss=1.077362\n",
      "Epoch: 200: loss=0.688137 valid_loss=0.704438\n",
      "Epoch: 300: loss=0.392823 valid_loss=0.316648\n",
      "Epoch: 400: loss=0.366471 valid_loss=0.333924\n",
      "Epoch: 500: loss=0.200054 valid_loss=0.131400\n",
      "Epoch: 600: loss=0.171606 valid_loss=0.091608\n",
      "Epoch: 700: loss=0.171955 valid_loss=0.083820\n",
      "Epoch: 800: loss=0.191885 valid_loss=0.080404\n",
      "Epoch: 900: loss=0.151718 valid_loss=0.053745\n",
      "Epoch: 1000: loss=0.148785 valid_loss=0.059793\n",
      "Accuracy = 96.66666666666667%\n",
      "96.66666666666667%, 5/23 features\n",
      "Epoch: 100: loss=1.155236 valid_loss=1.107379\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 200: loss=0.837401 valid_loss=0.843466\n",
      "Epoch: 300: loss=0.480109 valid_loss=0.447462\n",
      "Epoch: 400: loss=0.319902 valid_loss=0.241529\n",
      "Epoch: 500: loss=0.228684 valid_loss=0.149895\n",
      "Epoch: 600: loss=0.204697 valid_loss=0.117000\n",
      "Epoch: 700: loss=0.168880 valid_loss=0.104819\n",
      "Epoch: 800: loss=0.195039 valid_loss=0.091421\n",
      "Epoch: 900: loss=0.182864 valid_loss=0.074129\n",
      "Epoch: 1000: loss=0.169800 valid_loss=0.073590\n",
      "Accuracy = 96.66666666666667%\n",
      "96.66666666666667%, 10/25 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100: loss=1.157126 valid_loss=1.105599\n",
      "Epoch: 200: loss=1.060746 valid_loss=0.902679\n",
      "Epoch: 300: loss=0.419525 valid_loss=0.388890\n",
      "Epoch: 400: loss=0.292387 valid_loss=0.212703\n",
      "Epoch: 500: loss=0.237238 valid_loss=0.177610\n",
      "Epoch: 600: loss=0.181256 valid_loss=0.099718\n",
      "Epoch: 700: loss=0.172627 valid_loss=0.087059\n",
      "Epoch: 800: loss=0.155296 valid_loss=0.070913\n",
      "Epoch: 900: loss=0.153828 valid_loss=0.063782\n",
      "Epoch: 1000: loss=0.164333 valid_loss=0.071352\n",
      "Accuracy = 96.66666666666667%\n",
      "96.66666666666667%, 8/27 features\n",
      "Epoch: 100: loss=1.104680 valid_loss=1.038217\n",
      "Epoch: 200: loss=0.555026 valid_loss=0.550891\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 300: loss=0.306272 valid_loss=0.258093\n",
      "Epoch: 400: loss=0.259889 valid_loss=0.146826\n",
      "Epoch: 500: loss=0.228136 valid_loss=0.103738\n",
      "Epoch: 600: loss=0.169852 valid_loss=0.088618\n",
      "Epoch: 700: loss=0.153314 valid_loss=0.066568\n",
      "Epoch: 800: loss=0.144148 valid_loss=0.053883\n",
      "Epoch: 900: loss=0.152441 valid_loss=0.042125\n",
      "Epoch: 1000: loss=0.172957 valid_loss=0.049626\n",
      "Accuracy = 100.0%\n",
      "100.0%, 15/30 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100: loss=1.175012 valid_loss=1.116844\n",
      "Epoch: 200: loss=1.155338 valid_loss=1.102709\n",
      "Epoch: 300: loss=0.927224 valid_loss=0.909718\n",
      "Epoch: 400: loss=0.506926 valid_loss=0.415785\n",
      "Epoch: 500: loss=0.278585 valid_loss=0.181551\n",
      "Epoch: 600: loss=0.201357 valid_loss=0.172579\n",
      "Epoch: 700: loss=0.350449 valid_loss=0.378282\n",
      "Epoch: 800: loss=0.168605 valid_loss=0.083306\n",
      "Epoch: 900: loss=0.191474 valid_loss=0.095035\n",
      "Epoch: 1000: loss=0.183809 valid_loss=0.072866\n",
      "Accuracy = 96.66666666666667%\n",
      "96.66666666666667%, 16/32 features\n",
      "Epoch: 100: loss=1.118279 valid_loss=1.072720\n",
      "Epoch: 200: loss=0.616830 valid_loss=0.589888\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 300: loss=0.367393 valid_loss=0.307686\n",
      "Epoch: 400: loss=0.250513 valid_loss=0.188723\n",
      "Epoch: 500: loss=0.284923 valid_loss=0.138371\n",
      "Epoch: 600: loss=0.202368 valid_loss=0.101620\n",
      "Epoch: 700: loss=0.163085 valid_loss=0.068608\n",
      "Epoch: 800: loss=0.158825 valid_loss=0.056454\n",
      "Epoch: 900: loss=0.167177 valid_loss=0.131869\n",
      "Epoch: 1000: loss=0.163271 valid_loss=0.052610\n",
      "Accuracy = 100.0%\n",
      "100.0%, 17/34 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100: loss=1.138973 valid_loss=1.094694\n",
      "Epoch: 200: loss=0.758099 valid_loss=0.704675\n",
      "Epoch: 300: loss=0.427300 valid_loss=0.475311\n",
      "Epoch: 400: loss=0.334816 valid_loss=0.200033\n",
      "Epoch: 500: loss=0.207990 valid_loss=0.141718\n",
      "Epoch: 600: loss=0.301056 valid_loss=0.108265\n",
      "Epoch: 700: loss=0.181304 valid_loss=0.088572\n",
      "Epoch: 800: loss=0.151808 valid_loss=0.092944\n",
      "Epoch: 900: loss=0.195209 valid_loss=0.088361\n",
      "Epoch: 1000: loss=0.159029 valid_loss=0.075616\n",
      "Accuracy = 96.66666666666667%\n",
      "96.66666666666667%, 18/36 features\n",
      "Epoch: 100: loss=1.100314 valid_loss=1.070372\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 200: loss=0.586603 valid_loss=0.570887\n",
      "Epoch: 300: loss=0.373957 valid_loss=0.450305\n",
      "Epoch: 400: loss=0.312500 valid_loss=0.324077\n",
      "Epoch: 500: loss=0.200269 valid_loss=0.151366\n",
      "Epoch: 600: loss=0.174421 valid_loss=0.115467\n",
      "Epoch: 700: loss=0.159456 valid_loss=0.068404\n",
      "Epoch: 800: loss=0.146969 valid_loss=0.064534\n",
      "Epoch: 900: loss=0.150747 valid_loss=0.063809\n",
      "Epoch: 1000: loss=0.136743 valid_loss=0.048236\n",
      "Accuracy = 100.0%\n",
      "100.0%, 19/38 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100: loss=1.122956 valid_loss=1.096849\n",
      "Epoch: 200: loss=0.991843 valid_loss=0.822193\n",
      "Epoch: 300: loss=0.535311 valid_loss=0.438178\n",
      "Epoch: 400: loss=0.238358 valid_loss=0.193067\n",
      "Epoch: 500: loss=0.207772 valid_loss=0.111061\n",
      "Epoch: 600: loss=0.189815 valid_loss=0.075550\n",
      "Epoch: 700: loss=0.223749 valid_loss=0.070179\n",
      "Epoch: 800: loss=0.166184 valid_loss=0.057327\n",
      "Epoch: 900: loss=0.159626 valid_loss=0.050344\n",
      "Epoch: 1000: loss=0.156203 valid_loss=0.044877\n",
      "Accuracy = 100.0%\n",
      "100.0%, 3/8 features\n",
      "Epoch: 100: loss=1.152590 valid_loss=1.112423\n",
      "Epoch: 200: loss=1.035737 valid_loss=0.993218\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 300: loss=0.568569 valid_loss=0.574723\n",
      "Epoch: 400: loss=0.376705 valid_loss=0.330247\n",
      "Epoch: 500: loss=0.256993 valid_loss=0.184232\n",
      "Epoch: 600: loss=0.207376 valid_loss=0.143758\n",
      "Epoch: 700: loss=0.179220 valid_loss=0.105461\n",
      "Epoch: 800: loss=0.163759 valid_loss=0.088700\n",
      "Epoch: 900: loss=0.153083 valid_loss=0.078660\n",
      "Epoch: 1000: loss=0.145478 valid_loss=0.071879\n",
      "Accuracy = 96.66666666666667%\n",
      "96.66666666666667%, 3/10 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100: loss=1.167280 valid_loss=1.115455\n",
      "Epoch: 200: loss=1.122861 valid_loss=1.083014\n",
      "Epoch: 300: loss=0.707981 valid_loss=0.699617\n",
      "Epoch: 400: loss=0.402909 valid_loss=0.346617\n",
      "Epoch: 500: loss=0.284032 valid_loss=0.209280\n",
      "Epoch: 600: loss=0.201434 valid_loss=0.140970\n",
      "Epoch: 700: loss=0.174991 valid_loss=0.110039\n",
      "Epoch: 800: loss=0.161485 valid_loss=0.093400\n",
      "Epoch: 900: loss=0.151602 valid_loss=0.080971\n",
      "Epoch: 1000: loss=0.160095 valid_loss=0.089696\n",
      "Accuracy = 100.0%\n",
      "100.0%, 3/12 features\n",
      "Epoch: 100: loss=1.168441 valid_loss=1.109212\n",
      "Epoch: 200: loss=0.990047 valid_loss=0.972499\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 300: loss=0.755994 valid_loss=0.597534\n",
      "Epoch: 400: loss=0.303713 valid_loss=0.236091\n",
      "Epoch: 500: loss=0.222811 valid_loss=0.154166\n",
      "Epoch: 600: loss=0.188703 valid_loss=0.109301\n",
      "Epoch: 700: loss=0.176621 valid_loss=0.085930\n",
      "Epoch: 800: loss=0.158196 valid_loss=0.079290\n",
      "Epoch: 900: loss=0.148415 valid_loss=0.073833\n",
      "Epoch: 1000: loss=0.143658 valid_loss=0.066947\n",
      "Accuracy = 100.0%\n",
      "100.0%, 4/14 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100: loss=1.150493 valid_loss=1.108606\n",
      "Epoch: 200: loss=0.907212 valid_loss=0.910102\n",
      "Epoch: 300: loss=0.542693 valid_loss=0.545430\n",
      "Epoch: 400: loss=0.341552 valid_loss=0.274914\n",
      "Epoch: 500: loss=0.248924 valid_loss=0.179842\n",
      "Epoch: 600: loss=0.202388 valid_loss=0.123712\n",
      "Epoch: 700: loss=0.186305 valid_loss=0.109002\n",
      "Epoch: 800: loss=0.164370 valid_loss=0.090498\n",
      "Epoch: 900: loss=0.153599 valid_loss=0.080591\n",
      "Epoch: 1000: loss=0.145915 valid_loss=0.071093\n",
      "Accuracy = 100.0%\n",
      "100.0%, 3/16 features\n",
      "Epoch: 100: loss=1.173455 valid_loss=1.117132\n",
      "Epoch: 200: loss=1.105497 valid_loss=1.080469\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 300: loss=0.884866 valid_loss=0.720136\n",
      "Epoch: 400: loss=0.408213 valid_loss=0.375057\n",
      "Epoch: 500: loss=0.288880 valid_loss=0.193394\n",
      "Epoch: 600: loss=0.229255 valid_loss=0.196659\n",
      "Epoch: 700: loss=0.233285 valid_loss=0.125335\n",
      "Epoch: 800: loss=0.210983 valid_loss=0.101343\n",
      "Epoch: 900: loss=0.163766 valid_loss=0.087801\n",
      "Epoch: 1000: loss=0.157956 valid_loss=0.078265\n",
      "Accuracy = 96.66666666666667%\n",
      "96.66666666666667%, 5/19 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100: loss=1.121469 valid_loss=1.102285\n",
      "Epoch: 200: loss=0.756514 valid_loss=0.782725\n",
      "Epoch: 300: loss=0.481530 valid_loss=0.471195\n",
      "Epoch: 400: loss=0.337736 valid_loss=0.316274\n",
      "Epoch: 500: loss=0.242987 valid_loss=0.193747\n",
      "Epoch: 600: loss=0.225383 valid_loss=0.171156\n",
      "Epoch: 700: loss=0.245412 valid_loss=0.199956\n",
      "Epoch: 800: loss=0.180182 valid_loss=0.126450\n",
      "Epoch: 900: loss=0.177003 valid_loss=0.071983\n",
      "Epoch: 1000: loss=0.150219 valid_loss=0.066586\n",
      "Accuracy = 100.0%\n",
      "100.0%, 6/21 features\n",
      "Epoch: 100: loss=1.170432 valid_loss=1.117410\n",
      "Epoch: 200: loss=1.176789 valid_loss=1.090633\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 300: loss=0.804485 valid_loss=0.766169\n",
      "Epoch: 400: loss=0.437984 valid_loss=0.346987\n",
      "Epoch: 500: loss=0.296104 valid_loss=0.223205\n",
      "Epoch: 600: loss=0.203381 valid_loss=0.138948\n",
      "Epoch: 700: loss=0.242788 valid_loss=0.154875\n",
      "Epoch: 800: loss=0.155286 valid_loss=0.080191\n",
      "Epoch: 900: loss=0.144843 valid_loss=0.071710\n",
      "Epoch: 1000: loss=0.149839 valid_loss=0.065357\n",
      "Accuracy = 96.66666666666667%\n",
      "96.66666666666667%, 6/23 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100: loss=1.167454 valid_loss=1.107895\n",
      "Epoch: 200: loss=0.890894 valid_loss=0.856276\n",
      "Epoch: 300: loss=0.528273 valid_loss=0.460213\n",
      "Epoch: 400: loss=0.322116 valid_loss=0.202144\n",
      "Epoch: 500: loss=0.217011 valid_loss=0.140342\n",
      "Epoch: 600: loss=0.178041 valid_loss=0.115773\n",
      "Epoch: 700: loss=0.174260 valid_loss=0.085972\n",
      "Epoch: 800: loss=0.153266 valid_loss=0.073441\n",
      "Epoch: 900: loss=0.197254 valid_loss=0.073007\n",
      "Epoch: 1000: loss=0.139492 valid_loss=0.055751\n",
      "Accuracy = 100.0%\n",
      "100.0%, 10/25 features\n",
      "Epoch: 100: loss=1.168260 valid_loss=1.107732\n",
      "Epoch: 200: loss=1.019086 valid_loss=0.978065\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 300: loss=0.476047 valid_loss=0.442223\n",
      "Epoch: 400: loss=0.339638 valid_loss=0.318460\n",
      "Epoch: 500: loss=0.235933 valid_loss=0.137890\n",
      "Epoch: 600: loss=0.196310 valid_loss=0.099455\n",
      "Epoch: 700: loss=0.167703 valid_loss=0.076162\n",
      "Epoch: 800: loss=0.153823 valid_loss=0.060322\n",
      "Epoch: 900: loss=0.148770 valid_loss=0.054787\n",
      "Epoch: 1000: loss=0.140266 valid_loss=0.047654\n",
      "Accuracy = 100.0%\n",
      "100.0%, 11/27 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100: loss=1.167022 valid_loss=1.112533\n",
      "Epoch: 200: loss=1.026242 valid_loss=0.994811\n",
      "Epoch: 300: loss=0.630603 valid_loss=0.572624\n",
      "Epoch: 400: loss=0.296755 valid_loss=0.295016\n",
      "Epoch: 500: loss=0.219767 valid_loss=0.140782\n",
      "Epoch: 600: loss=0.207295 valid_loss=0.097161\n",
      "Epoch: 700: loss=0.198143 valid_loss=0.076713\n",
      "Epoch: 800: loss=0.168024 valid_loss=0.064813\n",
      "Epoch: 900: loss=0.148011 valid_loss=0.053534\n",
      "Epoch: 1000: loss=0.191292 valid_loss=0.058630\n",
      "Accuracy = 96.66666666666667%\n",
      "96.66666666666667%, 15/30 features\n",
      "Epoch: 100: loss=1.163478 valid_loss=1.104679\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 200: loss=0.914707 valid_loss=0.793991\n",
      "Epoch: 300: loss=0.442340 valid_loss=0.362516\n",
      "Epoch: 400: loss=0.307987 valid_loss=0.175271\n",
      "Epoch: 500: loss=0.208051 valid_loss=0.124407\n",
      "Epoch: 600: loss=0.177807 valid_loss=0.102970\n",
      "Epoch: 700: loss=0.164291 valid_loss=0.079571\n",
      "Epoch: 800: loss=0.163304 valid_loss=0.090489\n",
      "Epoch: 900: loss=0.161565 valid_loss=0.076038\n",
      "Epoch: 1000: loss=0.169741 valid_loss=0.076983\n",
      "Accuracy = 96.66666666666667%\n",
      "96.66666666666667%, 16/32 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100: loss=1.160752 valid_loss=1.105269\n",
      "Epoch: 200: loss=0.860493 valid_loss=0.814043\n",
      "Epoch: 300: loss=0.421102 valid_loss=0.399687\n",
      "Epoch: 400: loss=0.283077 valid_loss=0.260616\n",
      "Epoch: 500: loss=0.217074 valid_loss=0.149335\n",
      "Epoch: 600: loss=0.172893 valid_loss=0.181584\n",
      "Epoch: 700: loss=0.270273 valid_loss=0.090869\n",
      "Epoch: 800: loss=0.147676 valid_loss=0.092609\n",
      "Epoch: 900: loss=0.149039 valid_loss=0.066960\n",
      "Epoch: 1000: loss=0.130323 valid_loss=0.069842\n",
      "Accuracy = 100.0%\n",
      "100.0%, 17/34 features\n",
      "Epoch: 100: loss=1.175550 valid_loss=1.117970\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 200: loss=1.155314 valid_loss=1.104735\n",
      "Epoch: 300: loss=1.072287 valid_loss=1.015521\n",
      "Epoch: 400: loss=0.480327 valid_loss=0.485571\n",
      "Epoch: 500: loss=0.300800 valid_loss=0.338174\n",
      "Epoch: 600: loss=0.220393 valid_loss=0.099807\n",
      "Epoch: 700: loss=0.177598 valid_loss=0.088252\n",
      "Epoch: 800: loss=0.195708 valid_loss=0.049329\n",
      "Epoch: 900: loss=0.201171 valid_loss=0.038042\n",
      "Epoch: 1000: loss=0.146001 valid_loss=0.036081\n",
      "Accuracy = 100.0%\n",
      "100.0%, 18/36 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100: loss=1.165121 valid_loss=1.113514\n",
      "Epoch: 200: loss=0.920323 valid_loss=0.891940\n",
      "Epoch: 300: loss=0.396396 valid_loss=0.373128\n",
      "Epoch: 400: loss=0.262351 valid_loss=0.216505\n",
      "Epoch: 500: loss=0.209406 valid_loss=0.133859\n",
      "Epoch: 600: loss=0.176819 valid_loss=0.112439\n",
      "Epoch: 700: loss=0.203125 valid_loss=0.161847\n",
      "Epoch: 800: loss=0.144204 valid_loss=0.087677\n",
      "Epoch: 900: loss=0.156921 valid_loss=0.103018\n",
      "Epoch: 1000: loss=0.152441 valid_loss=0.083023\n",
      "Accuracy = 96.66666666666667%\n",
      "96.66666666666667%, 19/38 features\n",
      "Epoch: 100: loss=1.156621 valid_loss=1.110111\n",
      "Epoch: 200: loss=1.156990 valid_loss=1.047965\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 300: loss=0.612954 valid_loss=0.584937\n",
      "Epoch: 400: loss=0.345956 valid_loss=0.288539\n",
      "Epoch: 500: loss=0.394476 valid_loss=0.122354\n",
      "Epoch: 600: loss=0.206916 valid_loss=0.075766\n",
      "Epoch: 700: loss=0.180745 valid_loss=0.056899\n",
      "Epoch: 800: loss=0.177660 valid_loss=0.046465\n",
      "Epoch: 900: loss=0.176688 valid_loss=0.041624\n",
      "Epoch: 1000: loss=0.168873 valid_loss=0.044109\n",
      "Accuracy = 100.0%\n",
      "100.0%, 4/8 features\n",
      "Epoch: 100: loss=1.166508 valid_loss=1.104516\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 200: loss=0.880992 valid_loss=0.832874\n",
      "Epoch: 300: loss=0.541006 valid_loss=0.504700\n",
      "Epoch: 400: loss=0.409102 valid_loss=0.356311\n",
      "Epoch: 500: loss=0.335492 valid_loss=0.250646\n",
      "Epoch: 600: loss=0.256530 valid_loss=0.185309\n",
      "Epoch: 700: loss=0.203152 valid_loss=0.133590\n",
      "Epoch: 800: loss=0.185163 valid_loss=0.112594\n",
      "Epoch: 900: loss=0.166742 valid_loss=0.094970\n",
      "Epoch: 1000: loss=0.157109 valid_loss=0.084205\n",
      "Accuracy = 96.66666666666667%\n",
      "96.66666666666667%, 3/10 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100: loss=1.084042 valid_loss=1.042452\n",
      "Epoch: 200: loss=0.658897 valid_loss=0.682219\n",
      "Epoch: 300: loss=0.512774 valid_loss=0.378929\n",
      "Epoch: 400: loss=0.609876 valid_loss=0.447219\n",
      "Epoch: 500: loss=0.312805 valid_loss=0.243015\n",
      "Epoch: 600: loss=0.220591 valid_loss=0.153409\n",
      "Epoch: 700: loss=0.192556 valid_loss=0.127008\n",
      "Epoch: 800: loss=0.196046 valid_loss=0.111965\n",
      "Epoch: 900: loss=0.165597 valid_loss=0.100483\n",
      "Epoch: 1000: loss=0.156655 valid_loss=0.092699\n",
      "Accuracy = 96.66666666666667%\n",
      "96.66666666666667%, 3/12 features\n",
      "Epoch: 100: loss=1.131042 valid_loss=1.081502\n",
      "Epoch: 200: loss=0.754815 valid_loss=0.735526\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 300: loss=0.459170 valid_loss=0.556991\n",
      "Epoch: 400: loss=0.313423 valid_loss=0.289985\n",
      "Epoch: 500: loss=0.410908 valid_loss=0.179008\n",
      "Epoch: 600: loss=0.200009 valid_loss=0.147294\n",
      "Epoch: 700: loss=0.178321 valid_loss=0.122358\n",
      "Epoch: 800: loss=0.158159 valid_loss=0.110650\n",
      "Epoch: 900: loss=0.154191 valid_loss=0.098716\n",
      "Epoch: 1000: loss=0.146217 valid_loss=0.091989\n",
      "Accuracy = 96.66666666666667%\n",
      "96.66666666666667%, 3/14 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100: loss=1.161371 valid_loss=1.096380\n",
      "Epoch: 200: loss=0.858827 valid_loss=0.793218\n",
      "Epoch: 300: loss=0.454696 valid_loss=0.435538\n",
      "Epoch: 400: loss=0.302324 valid_loss=0.271674\n",
      "Epoch: 500: loss=0.301818 valid_loss=0.164784\n",
      "Epoch: 600: loss=0.184170 valid_loss=0.102395\n",
      "Epoch: 700: loss=0.196468 valid_loss=0.081169\n",
      "Epoch: 800: loss=0.156940 valid_loss=0.058998\n",
      "Epoch: 900: loss=0.151495 valid_loss=0.053571\n",
      "Epoch: 1000: loss=0.142948 valid_loss=0.046247\n",
      "Accuracy = 100.0%\n",
      "100.0%, 4/16 features\n",
      "Epoch: 100: loss=1.170071 valid_loss=1.112476\n",
      "Epoch: 200: loss=1.053904 valid_loss=1.031468\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 300: loss=0.680892 valid_loss=0.631708\n",
      "Epoch: 400: loss=0.482748 valid_loss=0.351033\n",
      "Epoch: 500: loss=0.237084 valid_loss=0.170252\n",
      "Epoch: 600: loss=0.195816 valid_loss=0.116780\n",
      "Epoch: 700: loss=0.174818 valid_loss=0.092234\n",
      "Epoch: 800: loss=0.168568 valid_loss=0.082770\n",
      "Epoch: 900: loss=0.143819 valid_loss=0.064440\n",
      "Epoch: 1000: loss=0.137518 valid_loss=0.055550\n",
      "Accuracy = 96.66666666666667%\n",
      "96.66666666666667%, 5/19 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100: loss=1.172168 valid_loss=1.119146\n",
      "Epoch: 200: loss=1.162958 valid_loss=1.112923\n",
      "Epoch: 300: loss=1.087280 valid_loss=1.058004\n",
      "Epoch: 400: loss=0.635421 valid_loss=0.589362\n",
      "Epoch: 500: loss=0.387857 valid_loss=0.315814\n",
      "Epoch: 600: loss=0.274802 valid_loss=0.262241\n",
      "Epoch: 700: loss=0.245322 valid_loss=0.173747\n",
      "Epoch: 800: loss=0.193206 valid_loss=0.119243\n",
      "Epoch: 900: loss=0.179707 valid_loss=0.144088\n",
      "Epoch: 1000: loss=0.159360 valid_loss=0.090310\n",
      "Accuracy = 100.0%\n",
      "100.0%, 7/21 features\n",
      "Epoch: 100: loss=1.158283 valid_loss=1.111199\n",
      "Epoch: 200: loss=0.990720 valid_loss=0.925669\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 300: loss=0.477689 valid_loss=0.412766\n",
      "Epoch: 400: loss=0.276416 valid_loss=0.242968\n",
      "Epoch: 500: loss=0.202987 valid_loss=0.155753\n",
      "Epoch: 600: loss=0.181456 valid_loss=0.113236\n",
      "Epoch: 700: loss=0.174053 valid_loss=0.103130\n",
      "Epoch: 800: loss=0.189553 valid_loss=0.091985\n",
      "Epoch: 900: loss=0.142118 valid_loss=0.074033\n",
      "Epoch: 1000: loss=0.147406 valid_loss=0.072288\n",
      "Accuracy = 96.66666666666667%\n",
      "96.66666666666667%, 7/23 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100: loss=1.174516 valid_loss=1.110288\n",
      "Epoch: 200: loss=1.092900 valid_loss=1.054386\n",
      "Epoch: 300: loss=0.616731 valid_loss=0.594036\n",
      "Epoch: 400: loss=0.339854 valid_loss=0.345839\n",
      "Epoch: 500: loss=0.275010 valid_loss=0.244887\n",
      "Epoch: 600: loss=0.215500 valid_loss=0.119929\n",
      "Epoch: 700: loss=0.220566 valid_loss=0.115705\n",
      "Epoch: 800: loss=0.166410 valid_loss=0.077215\n",
      "Epoch: 900: loss=0.198431 valid_loss=0.087581\n",
      "Epoch: 1000: loss=0.150888 valid_loss=0.061870\n",
      "Accuracy = 100.0%\n",
      "100.0%, 9/25 features\n",
      "Epoch: 100: loss=1.176830 valid_loss=1.112860\n",
      "Epoch: 200: loss=1.053576 valid_loss=1.015869\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 300: loss=0.487121 valid_loss=0.430614\n",
      "Epoch: 400: loss=0.284547 valid_loss=0.221019\n",
      "Epoch: 500: loss=0.251396 valid_loss=0.137481\n",
      "Epoch: 600: loss=0.197401 valid_loss=0.097140\n",
      "Epoch: 700: loss=0.180474 valid_loss=0.079953\n",
      "Epoch: 800: loss=0.226816 valid_loss=0.147189\n",
      "Epoch: 900: loss=0.182615 valid_loss=0.072567\n",
      "Epoch: 1000: loss=0.154039 valid_loss=0.057827\n",
      "Accuracy = 96.66666666666667%\n",
      "96.66666666666667%, 12/27 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100: loss=1.151672 valid_loss=1.111381\n",
      "Epoch: 200: loss=0.871860 valid_loss=0.862409\n",
      "Epoch: 300: loss=0.390814 valid_loss=0.410229\n",
      "Epoch: 400: loss=0.258873 valid_loss=0.290181\n",
      "Epoch: 500: loss=0.204807 valid_loss=0.113227\n",
      "Epoch: 600: loss=0.182628 valid_loss=0.109708\n",
      "Epoch: 700: loss=0.363195 valid_loss=0.092005\n",
      "Epoch: 800: loss=0.160007 valid_loss=0.074354\n",
      "Epoch: 900: loss=0.156142 valid_loss=0.064694\n",
      "Epoch: 1000: loss=0.152192 valid_loss=0.057645\n",
      "Accuracy = 100.0%\n",
      "100.0%, 15/30 features\n",
      "Epoch: 100: loss=1.168996 valid_loss=1.115004\n",
      "Epoch: 200: loss=1.032258 valid_loss=1.007952\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 300: loss=0.553983 valid_loss=0.474145\n",
      "Epoch: 400: loss=0.300826 valid_loss=0.262737\n",
      "Epoch: 500: loss=0.267743 valid_loss=0.153425\n",
      "Epoch: 600: loss=0.192980 valid_loss=0.113031\n",
      "Epoch: 700: loss=0.158093 valid_loss=0.144230\n",
      "Epoch: 800: loss=0.143049 valid_loss=0.101167\n",
      "Epoch: 900: loss=0.120573 valid_loss=0.099341\n",
      "Epoch: 1000: loss=0.129050 valid_loss=0.105293\n",
      "Accuracy = 96.66666666666667%\n",
      "96.66666666666667%, 16/32 features\n",
      "Epoch: 100: loss=1.166028 valid_loss=1.110480\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 200: loss=1.056945 valid_loss=1.008616\n",
      "Epoch: 300: loss=0.471697 valid_loss=0.406105\n",
      "Epoch: 400: loss=0.279642 valid_loss=0.177110\n",
      "Epoch: 500: loss=0.192183 valid_loss=0.122863\n",
      "Epoch: 600: loss=0.171529 valid_loss=0.085765\n",
      "Epoch: 700: loss=0.166682 valid_loss=0.076429\n",
      "Epoch: 800: loss=0.189701 valid_loss=0.069484\n",
      "Epoch: 900: loss=0.160085 valid_loss=0.068290\n",
      "Epoch: 1000: loss=0.147757 valid_loss=0.093086\n",
      "Accuracy = 96.66666666666667%\n",
      "96.66666666666667%, 17/34 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100: loss=1.159718 valid_loss=1.110284\n",
      "Epoch: 200: loss=0.846136 valid_loss=0.871636\n",
      "Epoch: 300: loss=0.481691 valid_loss=0.410432\n",
      "Epoch: 400: loss=0.311565 valid_loss=0.280483\n",
      "Epoch: 500: loss=0.227197 valid_loss=0.187196\n",
      "Epoch: 600: loss=0.247858 valid_loss=0.135614\n",
      "Epoch: 700: loss=0.173360 valid_loss=0.101052\n",
      "Epoch: 800: loss=0.170498 valid_loss=0.080559\n",
      "Epoch: 900: loss=0.147325 valid_loss=0.068700\n",
      "Epoch: 1000: loss=0.239658 valid_loss=0.097724\n",
      "Accuracy = 100.0%\n",
      "100.0%, 18/36 features\n",
      "Epoch: 100: loss=1.113377 valid_loss=1.091021\n",
      "Epoch: 200: loss=0.729792 valid_loss=0.679947\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 300: loss=0.406395 valid_loss=0.353395\n",
      "Epoch: 400: loss=0.286351 valid_loss=0.279409\n",
      "Epoch: 500: loss=0.226028 valid_loss=0.185915\n",
      "Epoch: 600: loss=0.199753 valid_loss=0.137269\n",
      "Epoch: 700: loss=0.169023 valid_loss=0.106204\n",
      "Epoch: 800: loss=0.170893 valid_loss=0.097495\n",
      "Epoch: 900: loss=0.181257 valid_loss=0.125111\n",
      "Epoch: 1000: loss=0.157185 valid_loss=0.077230\n",
      "Accuracy = 100.0%\n",
      "100.0%, 19/38 features\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from Gini import gini_filter\n",
    "two_step_results = {\n",
    "    '# features': [],\n",
    "    \"accuracy\": []\n",
    "}\n",
    "for t in range(10):\n",
    "    accs = []\n",
    "    for i in range(len(num_normal)):\n",
    "        X_train, X_test, Y_train, Y_test = insert_feature_noise(X, y, num_random_noise=num_normal[i],\n",
    "            num_overwhelemed=num_overwhelmed[i], num_shortcut=num_shortcut[i])\n",
    "        \n",
    "        X_train, X_test, Y_train, Y_test = gini_filter(torch.tensor(X_train.values), torch.tensor(X_test.values), \n",
    "            torch.tensor(Y_train, dtype=torch.int64), \n",
    "            torch.tensor(Y_test, dtype=torch.int64), \n",
    "            left=0.5)\n",
    "        X_train, X_test, Y_train, Y_test = X_train.numpy(), X_test.numpy(), Y_train.numpy(), Y_test.numpy()\n",
    "        clf = STG(task_type='classification',\n",
    "                input_dim=X_train.shape[1],\n",
    "                output_dim=3,\n",
    "                hidden_dims=(16, 16),\n",
    "                activation='ReLU', optimizer='SGD', learning_rate=0.1,\n",
    "                batch_size=X_train.shape[0], feature_selection=True, \n",
    "                sigma=0.5, lam=0.1, random_state=1, device=\"cpu\")\n",
    "        clf.fit(X_train, Y_train, nr_epochs=1000,\n",
    "            print_interval=100, \n",
    "            valid_X=X_test, \n",
    "            valid_y=Y_test)\n",
    "        \n",
    "        acc = print_accuracy(clf.predict, X_test, Y_test)\n",
    "        two_step_results[\"accuracy\"].append(acc)\n",
    "        total_feature = 2*num_normal[i] + num_overwhelmed[i] + num_shortcut[i]\n",
    "        two_step_results['# features'].append(total_feature)\n",
    "        num_gate = np.count_nonzero(clf.get_gates(mode='prob'))\n",
    "        print(f\"{acc}%, {num_gate}/{total_feature + 4} features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\r\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\r\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\r\n<svg height=\"262.19625pt\" version=\"1.1\" viewBox=\"0 0 390.255398 262.19625\" width=\"390.255398pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\r\n <metadata>\r\n  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\r\n   <cc:Work>\r\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\r\n    <dc:date>2022-03-29T18:46:05.691353</dc:date>\r\n    <dc:format>image/svg+xml</dc:format>\r\n    <dc:creator>\r\n     <cc:Agent>\r\n      <dc:title>Matplotlib v3.4.3, https://matplotlib.org/</dc:title>\r\n     </cc:Agent>\r\n    </dc:creator>\r\n   </cc:Work>\r\n  </rdf:RDF>\r\n </metadata>\r\n <defs>\r\n  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\r\n </defs>\r\n <g id=\"figure_1\">\r\n  <g id=\"patch_1\">\r\n   <path d=\"M 0 262.19625 \r\nL 390.255398 262.19625 \r\nL 390.255398 0 \r\nL 0 0 \r\nz\r\n\" style=\"fill:none;\"/>\r\n  </g>\r\n  <g id=\"axes_1\">\r\n   <g id=\"patch_2\">\r\n    <path d=\"M 46.965625 224.64 \r\nL 381.765625 224.64 \r\nL 381.765625 7.2 \r\nL 46.965625 7.2 \r\nz\r\n\" style=\"fill:#ffffff;\"/>\r\n   </g>\r\n   <g id=\"PolyCollection_1\">\r\n    <defs>\r\n     <path d=\"M 62.183807 -215.1622 \r\nL 62.183807 -215.1622 \r\nL 82.474716 -236.12749 \r\nL 102.765625 -200.186994 \r\nL 123.056534 -218.157242 \r\nL 143.347443 -215.1622 \r\nL 173.783807 -191.20187 \r\nL 194.074716 -176.226663 \r\nL 214.365625 -167.241539 \r\nL 234.656534 -149.271291 \r\nL 254.947443 -137.291126 \r\nL 285.383807 -98.355589 \r\nL 305.674716 -110.335754 \r\nL 325.965625 -92.365506 \r\nL 346.256534 -65.410134 \r\nL 366.547443 -47.439886 \r\nL 366.547443 -95.360548 \r\nL 366.547443 -95.360548 \r\nL 346.256534 -128.306002 \r\nL 325.965625 -170.236581 \r\nL 305.674716 -158.256415 \r\nL 285.383807 -167.241539 \r\nL 254.947443 -197.191952 \r\nL 234.656534 -194.196911 \r\nL 214.365625 -212.167159 \r\nL 194.074716 -230.137407 \r\nL 173.783807 -221.152283 \r\nL 143.347443 -236.12749 \r\nL 123.056534 -242.117572 \r\nL 102.765625 -230.137407 \r\nL 82.474716 -245.112614 \r\nL 62.183807 -215.1622 \r\nz\r\n\" id=\"mb3b3a12707\" style=\"stroke:#1f77b4;stroke-opacity:0.2;\"/>\r\n    </defs>\r\n    <g clip-path=\"url(#p01157220f7)\">\r\n     <use style=\"fill:#1f77b4;fill-opacity:0.2;stroke:#1f77b4;stroke-opacity:0.2;\" x=\"0\" xlink:href=\"#mb3b3a12707\" y=\"262.19625\"/>\r\n    </g>\r\n   </g>\r\n   <g id=\"PolyCollection_2\">\r\n    <defs>\r\n     <path d=\"M 62.183807 -245.112614 \r\nL 62.183807 -245.112614 \r\nL 82.474716 -245.112614 \r\nL 102.765625 -227.142366 \r\nL 123.056534 -230.137407 \r\nL 143.347443 -236.12749 \r\nL 173.783807 -200.186994 \r\nL 194.074716 -224.147324 \r\nL 214.365625 -218.157242 \r\nL 234.656534 -197.191952 \r\nL 254.947443 -197.191952 \r\nL 285.383807 -182.216746 \r\nL 305.674716 -194.196911 \r\nL 325.965625 -164.246498 \r\nL 346.256534 -140.286167 \r\nL 366.547443 -158.256415 \r\nL 366.547443 -212.242035 \r\nL 366.547443 -212.242035 \r\nL 346.256534 -197.191952 \r\nL 325.965625 -206.177076 \r\nL 305.674716 -221.152283 \r\nL 285.383807 -212.167159 \r\nL 254.947443 -224.147324 \r\nL 234.656534 -215.1622 \r\nL 214.365625 -242.117572 \r\nL 194.074716 -245.112614 \r\nL 173.783807 -230.137407 \r\nL 143.347443 -245.112614 \r\nL 123.056534 -245.112614 \r\nL 102.765625 -245.112614 \r\nL 82.474716 -245.112614 \r\nL 62.183807 -245.112614 \r\nz\r\n\" id=\"md88e426376\" style=\"stroke:#ff7f0e;stroke-opacity:0.2;\"/>\r\n    </defs>\r\n    <g clip-path=\"url(#p01157220f7)\">\r\n     <use style=\"fill:#ff7f0e;fill-opacity:0.2;stroke:#ff7f0e;stroke-opacity:0.2;\" x=\"0\" xlink:href=\"#md88e426376\" y=\"262.19625\"/>\r\n    </g>\r\n   </g>\r\n   <g id=\"PolyCollection_3\">\r\n    <defs>\r\n     <path d=\"M 62.183807 -245.112614 \r\nL 62.183807 -230.137407 \r\nL 82.474716 -119.320878 \r\nL 102.765625 -185.136911 \r\nL 123.056534 -224.147324 \r\nL 143.347443 -170.236581 \r\nL 173.783807 -203.182035 \r\nL 194.074716 -215.1622 \r\nL 214.365625 -200.186994 \r\nL 234.656534 -203.182035 \r\nL 254.947443 -203.182035 \r\nL 285.383807 -209.172118 \r\nL 305.674716 -212.167159 \r\nL 325.965625 -185.211787 \r\nL 346.256534 -212.167159 \r\nL 366.547443 -221.152283 \r\nL 366.547443 -239.122531 \r\nL 366.547443 -239.122531 \r\nL 346.256534 -239.122531 \r\nL 325.965625 -224.147324 \r\nL 305.674716 -236.12749 \r\nL 285.383807 -236.202366 \r\nL 254.947443 -233.132448 \r\nL 234.656534 -221.152283 \r\nL 214.365625 -230.137407 \r\nL 194.074716 -239.122531 \r\nL 173.783807 -221.152283 \r\nL 143.347443 -242.117572 \r\nL 123.056534 -242.117572 \r\nL 102.765625 -236.12749 \r\nL 82.474716 -230.137407 \r\nL 62.183807 -245.112614 \r\nz\r\n\" id=\"m37902323f9\" style=\"stroke:#2ca02c;stroke-opacity:0.2;\"/>\r\n    </defs>\r\n    <g clip-path=\"url(#p01157220f7)\">\r\n     <use style=\"fill:#2ca02c;fill-opacity:0.2;stroke:#2ca02c;stroke-opacity:0.2;\" x=\"0\" xlink:href=\"#m37902323f9\" y=\"262.19625\"/>\r\n    </g>\r\n   </g>\r\n   <g id=\"PolyCollection_4\">\r\n    <defs>\r\n     <path d=\"M 62.183807 -245.112614 \r\nL 62.183807 -230.137407 \r\nL 82.474716 -215.1622 \r\nL 102.765625 -215.1622 \r\nL 123.056534 -218.157242 \r\nL 143.347443 -227.142366 \r\nL 173.783807 -218.157242 \r\nL 194.074716 -227.142366 \r\nL 214.365625 -215.1622 \r\nL 234.656534 -212.167159 \r\nL 254.947443 -218.157242 \r\nL 285.383807 -227.142366 \r\nL 305.674716 -218.157242 \r\nL 325.965625 -215.1622 \r\nL 346.256534 -218.157242 \r\nL 366.547443 -212.167159 \r\nL 366.547443 -239.122531 \r\nL 366.547443 -239.122531 \r\nL 346.256534 -242.117572 \r\nL 325.965625 -239.122531 \r\nL 305.674716 -236.12749 \r\nL 285.383807 -242.117572 \r\nL 254.947443 -242.117572 \r\nL 234.656534 -236.12749 \r\nL 214.365625 -233.132448 \r\nL 194.074716 -245.112614 \r\nL 173.783807 -233.132448 \r\nL 143.347443 -245.112614 \r\nL 123.056534 -236.12749 \r\nL 102.765625 -239.122531 \r\nL 82.474716 -224.147324 \r\nL 62.183807 -245.112614 \r\nz\r\n\" id=\"mf4caa63714\" style=\"stroke:#d62728;stroke-opacity:0.2;\"/>\r\n    </defs>\r\n    <g clip-path=\"url(#p01157220f7)\">\r\n     <use style=\"fill:#d62728;fill-opacity:0.2;stroke:#d62728;stroke-opacity:0.2;\" x=\"0\" xlink:href=\"#mf4caa63714\" y=\"262.19625\"/>\r\n    </g>\r\n   </g>\r\n   <g id=\"matplotlib.axis_1\">\r\n    <g id=\"xtick_1\">\r\n     <g id=\"line2d_1\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL 0 3.5 \r\n\" id=\"m60c123f758\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"72.329261\" xlink:href=\"#m60c123f758\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_1\">\r\n      <!-- 5 -->\r\n      <g transform=\"translate(69.148011 239.238437)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 691 4666 \r\nL 3169 4666 \r\nL 3169 4134 \r\nL 1269 4134 \r\nL 1269 2991 \r\nQ 1406 3038 1543 3061 \r\nQ 1681 3084 1819 3084 \r\nQ 2600 3084 3056 2656 \r\nQ 3513 2228 3513 1497 \r\nQ 3513 744 3044 326 \r\nQ 2575 -91 1722 -91 \r\nQ 1428 -91 1123 -41 \r\nQ 819 9 494 109 \r\nL 494 744 \r\nQ 775 591 1075 516 \r\nQ 1375 441 1709 441 \r\nQ 2250 441 2565 725 \r\nQ 2881 1009 2881 1497 \r\nQ 2881 1984 2565 2268 \r\nQ 2250 2553 1709 2553 \r\nQ 1456 2553 1204 2497 \r\nQ 953 2441 691 2322 \r\nL 691 4666 \r\nz\r\n\" id=\"DejaVuSans-35\" transform=\"scale(0.015625)\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-35\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_2\">\r\n     <g id=\"line2d_2\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"123.056534\" xlink:href=\"#m60c123f758\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_2\">\r\n      <!-- 10 -->\r\n      <g transform=\"translate(116.694034 239.238437)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 794 531 \r\nL 1825 531 \r\nL 1825 4091 \r\nL 703 3866 \r\nL 703 4441 \r\nL 1819 4666 \r\nL 2450 4666 \r\nL 2450 531 \r\nL 3481 531 \r\nL 3481 0 \r\nL 794 0 \r\nL 794 531 \r\nz\r\n\" id=\"DejaVuSans-31\" transform=\"scale(0.015625)\"/>\r\n        <path d=\"M 2034 4250 \r\nQ 1547 4250 1301 3770 \r\nQ 1056 3291 1056 2328 \r\nQ 1056 1369 1301 889 \r\nQ 1547 409 2034 409 \r\nQ 2525 409 2770 889 \r\nQ 3016 1369 3016 2328 \r\nQ 3016 3291 2770 3770 \r\nQ 2525 4250 2034 4250 \r\nz\r\nM 2034 4750 \r\nQ 2819 4750 3233 4129 \r\nQ 3647 3509 3647 2328 \r\nQ 3647 1150 3233 529 \r\nQ 2819 -91 2034 -91 \r\nQ 1250 -91 836 529 \r\nQ 422 1150 422 2328 \r\nQ 422 3509 836 4129 \r\nQ 1250 4750 2034 4750 \r\nz\r\n\" id=\"DejaVuSans-30\" transform=\"scale(0.015625)\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-31\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_3\">\r\n     <g id=\"line2d_3\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"173.783807\" xlink:href=\"#m60c123f758\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_3\">\r\n      <!-- 15 -->\r\n      <g transform=\"translate(167.421307 239.238437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-31\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-35\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_4\">\r\n     <g id=\"line2d_4\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"224.51108\" xlink:href=\"#m60c123f758\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_4\">\r\n      <!-- 20 -->\r\n      <g transform=\"translate(218.14858 239.238437)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 1228 531 \r\nL 3431 531 \r\nL 3431 0 \r\nL 469 0 \r\nL 469 531 \r\nQ 828 903 1448 1529 \r\nQ 2069 2156 2228 2338 \r\nQ 2531 2678 2651 2914 \r\nQ 2772 3150 2772 3378 \r\nQ 2772 3750 2511 3984 \r\nQ 2250 4219 1831 4219 \r\nQ 1534 4219 1204 4116 \r\nQ 875 4013 500 3803 \r\nL 500 4441 \r\nQ 881 4594 1212 4672 \r\nQ 1544 4750 1819 4750 \r\nQ 2544 4750 2975 4387 \r\nQ 3406 4025 3406 3419 \r\nQ 3406 3131 3298 2873 \r\nQ 3191 2616 2906 2266 \r\nQ 2828 2175 2409 1742 \r\nQ 1991 1309 1228 531 \r\nz\r\n\" id=\"DejaVuSans-32\" transform=\"scale(0.015625)\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-32\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_5\">\r\n     <g id=\"line2d_5\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"275.238352\" xlink:href=\"#m60c123f758\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_5\">\r\n      <!-- 25 -->\r\n      <g transform=\"translate(268.875852 239.238437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-32\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-35\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_6\">\r\n     <g id=\"line2d_6\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"325.965625\" xlink:href=\"#m60c123f758\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_6\">\r\n      <!-- 30 -->\r\n      <g transform=\"translate(319.603125 239.238437)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 2597 2516 \r\nQ 3050 2419 3304 2112 \r\nQ 3559 1806 3559 1356 \r\nQ 3559 666 3084 287 \r\nQ 2609 -91 1734 -91 \r\nQ 1441 -91 1130 -33 \r\nQ 819 25 488 141 \r\nL 488 750 \r\nQ 750 597 1062 519 \r\nQ 1375 441 1716 441 \r\nQ 2309 441 2620 675 \r\nQ 2931 909 2931 1356 \r\nQ 2931 1769 2642 2001 \r\nQ 2353 2234 1838 2234 \r\nL 1294 2234 \r\nL 1294 2753 \r\nL 1863 2753 \r\nQ 2328 2753 2575 2939 \r\nQ 2822 3125 2822 3475 \r\nQ 2822 3834 2567 4026 \r\nQ 2313 4219 1838 4219 \r\nQ 1578 4219 1281 4162 \r\nQ 984 4106 628 3988 \r\nL 628 4550 \r\nQ 988 4650 1302 4700 \r\nQ 1616 4750 1894 4750 \r\nQ 2613 4750 3031 4423 \r\nQ 3450 4097 3450 3541 \r\nQ 3450 3153 3228 2886 \r\nQ 3006 2619 2597 2516 \r\nz\r\n\" id=\"DejaVuSans-33\" transform=\"scale(0.015625)\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-33\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_7\">\r\n     <g id=\"line2d_7\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"376.692898\" xlink:href=\"#m60c123f758\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_7\">\r\n      <!-- 35 -->\r\n      <g transform=\"translate(370.330398 239.238437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-33\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-35\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"text_8\">\r\n     <!-- Number of Noisy Features -->\r\n     <g transform=\"translate(149.445312 252.916562)scale(0.1 -0.1)\">\r\n      <defs>\r\n       <path d=\"M 628 4666 \r\nL 1478 4666 \r\nL 3547 763 \r\nL 3547 4666 \r\nL 4159 4666 \r\nL 4159 0 \r\nL 3309 0 \r\nL 1241 3903 \r\nL 1241 0 \r\nL 628 0 \r\nL 628 4666 \r\nz\r\n\" id=\"DejaVuSans-4e\" transform=\"scale(0.015625)\"/>\r\n       <path d=\"M 544 1381 \r\nL 544 3500 \r\nL 1119 3500 \r\nL 1119 1403 \r\nQ 1119 906 1312 657 \r\nQ 1506 409 1894 409 \r\nQ 2359 409 2629 706 \r\nQ 2900 1003 2900 1516 \r\nL 2900 3500 \r\nL 3475 3500 \r\nL 3475 0 \r\nL 2900 0 \r\nL 2900 538 \r\nQ 2691 219 2414 64 \r\nQ 2138 -91 1772 -91 \r\nQ 1169 -91 856 284 \r\nQ 544 659 544 1381 \r\nz\r\nM 1991 3584 \r\nL 1991 3584 \r\nz\r\n\" id=\"DejaVuSans-75\" transform=\"scale(0.015625)\"/>\r\n       <path d=\"M 3328 2828 \r\nQ 3544 3216 3844 3400 \r\nQ 4144 3584 4550 3584 \r\nQ 5097 3584 5394 3201 \r\nQ 5691 2819 5691 2113 \r\nL 5691 0 \r\nL 5113 0 \r\nL 5113 2094 \r\nQ 5113 2597 4934 2840 \r\nQ 4756 3084 4391 3084 \r\nQ 3944 3084 3684 2787 \r\nQ 3425 2491 3425 1978 \r\nL 3425 0 \r\nL 2847 0 \r\nL 2847 2094 \r\nQ 2847 2600 2669 2842 \r\nQ 2491 3084 2119 3084 \r\nQ 1678 3084 1418 2786 \r\nQ 1159 2488 1159 1978 \r\nL 1159 0 \r\nL 581 0 \r\nL 581 3500 \r\nL 1159 3500 \r\nL 1159 2956 \r\nQ 1356 3278 1631 3431 \r\nQ 1906 3584 2284 3584 \r\nQ 2666 3584 2933 3390 \r\nQ 3200 3197 3328 2828 \r\nz\r\n\" id=\"DejaVuSans-6d\" transform=\"scale(0.015625)\"/>\r\n       <path d=\"M 3116 1747 \r\nQ 3116 2381 2855 2742 \r\nQ 2594 3103 2138 3103 \r\nQ 1681 3103 1420 2742 \r\nQ 1159 2381 1159 1747 \r\nQ 1159 1113 1420 752 \r\nQ 1681 391 2138 391 \r\nQ 2594 391 2855 752 \r\nQ 3116 1113 3116 1747 \r\nz\r\nM 1159 2969 \r\nQ 1341 3281 1617 3432 \r\nQ 1894 3584 2278 3584 \r\nQ 2916 3584 3314 3078 \r\nQ 3713 2572 3713 1747 \r\nQ 3713 922 3314 415 \r\nQ 2916 -91 2278 -91 \r\nQ 1894 -91 1617 61 \r\nQ 1341 213 1159 525 \r\nL 1159 0 \r\nL 581 0 \r\nL 581 4863 \r\nL 1159 4863 \r\nL 1159 2969 \r\nz\r\n\" id=\"DejaVuSans-62\" transform=\"scale(0.015625)\"/>\r\n       <path d=\"M 3597 1894 \r\nL 3597 1613 \r\nL 953 1613 \r\nQ 991 1019 1311 708 \r\nQ 1631 397 2203 397 \r\nQ 2534 397 2845 478 \r\nQ 3156 559 3463 722 \r\nL 3463 178 \r\nQ 3153 47 2828 -22 \r\nQ 2503 -91 2169 -91 \r\nQ 1331 -91 842 396 \r\nQ 353 884 353 1716 \r\nQ 353 2575 817 3079 \r\nQ 1281 3584 2069 3584 \r\nQ 2775 3584 3186 3129 \r\nQ 3597 2675 3597 1894 \r\nz\r\nM 3022 2063 \r\nQ 3016 2534 2758 2815 \r\nQ 2500 3097 2075 3097 \r\nQ 1594 3097 1305 2825 \r\nQ 1016 2553 972 2059 \r\nL 3022 2063 \r\nz\r\n\" id=\"DejaVuSans-65\" transform=\"scale(0.015625)\"/>\r\n       <path d=\"M 2631 2963 \r\nQ 2534 3019 2420 3045 \r\nQ 2306 3072 2169 3072 \r\nQ 1681 3072 1420 2755 \r\nQ 1159 2438 1159 1844 \r\nL 1159 0 \r\nL 581 0 \r\nL 581 3500 \r\nL 1159 3500 \r\nL 1159 2956 \r\nQ 1341 3275 1631 3429 \r\nQ 1922 3584 2338 3584 \r\nQ 2397 3584 2469 3576 \r\nQ 2541 3569 2628 3553 \r\nL 2631 2963 \r\nz\r\n\" id=\"DejaVuSans-72\" transform=\"scale(0.015625)\"/>\r\n       <path id=\"DejaVuSans-20\" transform=\"scale(0.015625)\"/>\r\n       <path d=\"M 1959 3097 \r\nQ 1497 3097 1228 2736 \r\nQ 959 2375 959 1747 \r\nQ 959 1119 1226 758 \r\nQ 1494 397 1959 397 \r\nQ 2419 397 2687 759 \r\nQ 2956 1122 2956 1747 \r\nQ 2956 2369 2687 2733 \r\nQ 2419 3097 1959 3097 \r\nz\r\nM 1959 3584 \r\nQ 2709 3584 3137 3096 \r\nQ 3566 2609 3566 1747 \r\nQ 3566 888 3137 398 \r\nQ 2709 -91 1959 -91 \r\nQ 1206 -91 779 398 \r\nQ 353 888 353 1747 \r\nQ 353 2609 779 3096 \r\nQ 1206 3584 1959 3584 \r\nz\r\n\" id=\"DejaVuSans-6f\" transform=\"scale(0.015625)\"/>\r\n       <path d=\"M 2375 4863 \r\nL 2375 4384 \r\nL 1825 4384 \r\nQ 1516 4384 1395 4259 \r\nQ 1275 4134 1275 3809 \r\nL 1275 3500 \r\nL 2222 3500 \r\nL 2222 3053 \r\nL 1275 3053 \r\nL 1275 0 \r\nL 697 0 \r\nL 697 3053 \r\nL 147 3053 \r\nL 147 3500 \r\nL 697 3500 \r\nL 697 3744 \r\nQ 697 4328 969 4595 \r\nQ 1241 4863 1831 4863 \r\nL 2375 4863 \r\nz\r\n\" id=\"DejaVuSans-66\" transform=\"scale(0.015625)\"/>\r\n       <path d=\"M 603 3500 \r\nL 1178 3500 \r\nL 1178 0 \r\nL 603 0 \r\nL 603 3500 \r\nz\r\nM 603 4863 \r\nL 1178 4863 \r\nL 1178 4134 \r\nL 603 4134 \r\nL 603 4863 \r\nz\r\n\" id=\"DejaVuSans-69\" transform=\"scale(0.015625)\"/>\r\n       <path d=\"M 2834 3397 \r\nL 2834 2853 \r\nQ 2591 2978 2328 3040 \r\nQ 2066 3103 1784 3103 \r\nQ 1356 3103 1142 2972 \r\nQ 928 2841 928 2578 \r\nQ 928 2378 1081 2264 \r\nQ 1234 2150 1697 2047 \r\nL 1894 2003 \r\nQ 2506 1872 2764 1633 \r\nQ 3022 1394 3022 966 \r\nQ 3022 478 2636 193 \r\nQ 2250 -91 1575 -91 \r\nQ 1294 -91 989 -36 \r\nQ 684 19 347 128 \r\nL 347 722 \r\nQ 666 556 975 473 \r\nQ 1284 391 1588 391 \r\nQ 1994 391 2212 530 \r\nQ 2431 669 2431 922 \r\nQ 2431 1156 2273 1281 \r\nQ 2116 1406 1581 1522 \r\nL 1381 1569 \r\nQ 847 1681 609 1914 \r\nQ 372 2147 372 2553 \r\nQ 372 3047 722 3315 \r\nQ 1072 3584 1716 3584 \r\nQ 2034 3584 2315 3537 \r\nQ 2597 3491 2834 3397 \r\nz\r\n\" id=\"DejaVuSans-73\" transform=\"scale(0.015625)\"/>\r\n       <path d=\"M 2059 -325 \r\nQ 1816 -950 1584 -1140 \r\nQ 1353 -1331 966 -1331 \r\nL 506 -1331 \r\nL 506 -850 \r\nL 844 -850 \r\nQ 1081 -850 1212 -737 \r\nQ 1344 -625 1503 -206 \r\nL 1606 56 \r\nL 191 3500 \r\nL 800 3500 \r\nL 1894 763 \r\nL 2988 3500 \r\nL 3597 3500 \r\nL 2059 -325 \r\nz\r\n\" id=\"DejaVuSans-79\" transform=\"scale(0.015625)\"/>\r\n       <path d=\"M 628 4666 \r\nL 3309 4666 \r\nL 3309 4134 \r\nL 1259 4134 \r\nL 1259 2759 \r\nL 3109 2759 \r\nL 3109 2228 \r\nL 1259 2228 \r\nL 1259 0 \r\nL 628 0 \r\nL 628 4666 \r\nz\r\n\" id=\"DejaVuSans-46\" transform=\"scale(0.015625)\"/>\r\n       <path d=\"M 2194 1759 \r\nQ 1497 1759 1228 1600 \r\nQ 959 1441 959 1056 \r\nQ 959 750 1161 570 \r\nQ 1363 391 1709 391 \r\nQ 2188 391 2477 730 \r\nQ 2766 1069 2766 1631 \r\nL 2766 1759 \r\nL 2194 1759 \r\nz\r\nM 3341 1997 \r\nL 3341 0 \r\nL 2766 0 \r\nL 2766 531 \r\nQ 2569 213 2275 61 \r\nQ 1981 -91 1556 -91 \r\nQ 1019 -91 701 211 \r\nQ 384 513 384 1019 \r\nQ 384 1609 779 1909 \r\nQ 1175 2209 1959 2209 \r\nL 2766 2209 \r\nL 2766 2266 \r\nQ 2766 2663 2505 2880 \r\nQ 2244 3097 1772 3097 \r\nQ 1472 3097 1187 3025 \r\nQ 903 2953 641 2809 \r\nL 641 3341 \r\nQ 956 3463 1253 3523 \r\nQ 1550 3584 1831 3584 \r\nQ 2591 3584 2966 3190 \r\nQ 3341 2797 3341 1997 \r\nz\r\n\" id=\"DejaVuSans-61\" transform=\"scale(0.015625)\"/>\r\n       <path d=\"M 1172 4494 \r\nL 1172 3500 \r\nL 2356 3500 \r\nL 2356 3053 \r\nL 1172 3053 \r\nL 1172 1153 \r\nQ 1172 725 1289 603 \r\nQ 1406 481 1766 481 \r\nL 2356 481 \r\nL 2356 0 \r\nL 1766 0 \r\nQ 1100 0 847 248 \r\nQ 594 497 594 1153 \r\nL 594 3053 \r\nL 172 3053 \r\nL 172 3500 \r\nL 594 3500 \r\nL 594 4494 \r\nL 1172 4494 \r\nz\r\n\" id=\"DejaVuSans-74\" transform=\"scale(0.015625)\"/>\r\n      </defs>\r\n      <use xlink:href=\"#DejaVuSans-4e\"/>\r\n      <use x=\"74.804688\" xlink:href=\"#DejaVuSans-75\"/>\r\n      <use x=\"138.183594\" xlink:href=\"#DejaVuSans-6d\"/>\r\n      <use x=\"235.595703\" xlink:href=\"#DejaVuSans-62\"/>\r\n      <use x=\"299.072266\" xlink:href=\"#DejaVuSans-65\"/>\r\n      <use x=\"360.595703\" xlink:href=\"#DejaVuSans-72\"/>\r\n      <use x=\"401.708984\" xlink:href=\"#DejaVuSans-20\"/>\r\n      <use x=\"433.496094\" xlink:href=\"#DejaVuSans-6f\"/>\r\n      <use x=\"494.677734\" xlink:href=\"#DejaVuSans-66\"/>\r\n      <use x=\"529.882812\" xlink:href=\"#DejaVuSans-20\"/>\r\n      <use x=\"561.669922\" xlink:href=\"#DejaVuSans-4e\"/>\r\n      <use x=\"636.474609\" xlink:href=\"#DejaVuSans-6f\"/>\r\n      <use x=\"697.65625\" xlink:href=\"#DejaVuSans-69\"/>\r\n      <use x=\"725.439453\" xlink:href=\"#DejaVuSans-73\"/>\r\n      <use x=\"777.539062\" xlink:href=\"#DejaVuSans-79\"/>\r\n      <use x=\"836.71875\" xlink:href=\"#DejaVuSans-20\"/>\r\n      <use x=\"868.505859\" xlink:href=\"#DejaVuSans-46\"/>\r\n      <use x=\"920.525391\" xlink:href=\"#DejaVuSans-65\"/>\r\n      <use x=\"982.048828\" xlink:href=\"#DejaVuSans-61\"/>\r\n      <use x=\"1043.328125\" xlink:href=\"#DejaVuSans-74\"/>\r\n      <use x=\"1082.537109\" xlink:href=\"#DejaVuSans-75\"/>\r\n      <use x=\"1145.916016\" xlink:href=\"#DejaVuSans-72\"/>\r\n      <use x=\"1184.779297\" xlink:href=\"#DejaVuSans-65\"/>\r\n      <use x=\"1246.302734\" xlink:href=\"#DejaVuSans-73\"/>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"matplotlib.axis_2\">\r\n    <g id=\"ytick_1\">\r\n     <g id=\"line2d_8\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL -3.5 0 \r\n\" id=\"m367f1e86a6\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"46.965625\" xlink:href=\"#m367f1e86a6\" y=\"196.786116\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_9\">\r\n      <!-- 80 -->\r\n      <g transform=\"translate(27.240625 200.585334)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 2034 2216 \r\nQ 1584 2216 1326 1975 \r\nQ 1069 1734 1069 1313 \r\nQ 1069 891 1326 650 \r\nQ 1584 409 2034 409 \r\nQ 2484 409 2743 651 \r\nQ 3003 894 3003 1313 \r\nQ 3003 1734 2745 1975 \r\nQ 2488 2216 2034 2216 \r\nz\r\nM 1403 2484 \r\nQ 997 2584 770 2862 \r\nQ 544 3141 544 3541 \r\nQ 544 4100 942 4425 \r\nQ 1341 4750 2034 4750 \r\nQ 2731 4750 3128 4425 \r\nQ 3525 4100 3525 3541 \r\nQ 3525 3141 3298 2862 \r\nQ 3072 2584 2669 2484 \r\nQ 3125 2378 3379 2068 \r\nQ 3634 1759 3634 1313 \r\nQ 3634 634 3220 271 \r\nQ 2806 -91 2034 -91 \r\nQ 1263 -91 848 271 \r\nQ 434 634 434 1313 \r\nQ 434 1759 690 2068 \r\nQ 947 2378 1403 2484 \r\nz\r\nM 1172 3481 \r\nQ 1172 3119 1398 2916 \r\nQ 1625 2713 2034 2713 \r\nQ 2441 2713 2670 2916 \r\nQ 2900 3119 2900 3481 \r\nQ 2900 3844 2670 4047 \r\nQ 2441 4250 2034 4250 \r\nQ 1625 4250 1398 4047 \r\nQ 1172 3844 1172 3481 \r\nz\r\n\" id=\"DejaVuSans-38\" transform=\"scale(0.015625)\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-38\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_2\">\r\n     <g id=\"line2d_9\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"46.965625\" xlink:href=\"#m367f1e86a6\" y=\"151.860496\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_10\">\r\n      <!-- 85 -->\r\n      <g transform=\"translate(27.240625 155.659715)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-38\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-35\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_3\">\r\n     <g id=\"line2d_10\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"46.965625\" xlink:href=\"#m367f1e86a6\" y=\"106.934876\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_11\">\r\n      <!-- 90 -->\r\n      <g transform=\"translate(27.240625 110.734095)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 703 97 \r\nL 703 672 \r\nQ 941 559 1184 500 \r\nQ 1428 441 1663 441 \r\nQ 2288 441 2617 861 \r\nQ 2947 1281 2994 2138 \r\nQ 2813 1869 2534 1725 \r\nQ 2256 1581 1919 1581 \r\nQ 1219 1581 811 2004 \r\nQ 403 2428 403 3163 \r\nQ 403 3881 828 4315 \r\nQ 1253 4750 1959 4750 \r\nQ 2769 4750 3195 4129 \r\nQ 3622 3509 3622 2328 \r\nQ 3622 1225 3098 567 \r\nQ 2575 -91 1691 -91 \r\nQ 1453 -91 1209 -44 \r\nQ 966 3 703 97 \r\nz\r\nM 1959 2075 \r\nQ 2384 2075 2632 2365 \r\nQ 2881 2656 2881 3163 \r\nQ 2881 3666 2632 3958 \r\nQ 2384 4250 1959 4250 \r\nQ 1534 4250 1286 3958 \r\nQ 1038 3666 1038 3163 \r\nQ 1038 2656 1286 2365 \r\nQ 1534 2075 1959 2075 \r\nz\r\n\" id=\"DejaVuSans-39\" transform=\"scale(0.015625)\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-39\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_4\">\r\n     <g id=\"line2d_11\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"46.965625\" xlink:href=\"#m367f1e86a6\" y=\"62.009256\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_12\">\r\n      <!-- 95 -->\r\n      <g transform=\"translate(27.240625 65.808475)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-39\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-35\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_5\">\r\n     <g id=\"line2d_12\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"46.965625\" xlink:href=\"#m367f1e86a6\" y=\"17.083636\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_13\">\r\n      <!-- 100 -->\r\n      <g transform=\"translate(20.878125 20.882855)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-31\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"text_14\">\r\n     <!-- Test Accuracy -->\r\n     <g transform=\"translate(14.798438 150.1825)rotate(-90)scale(0.1 -0.1)\">\r\n      <defs>\r\n       <path d=\"M -19 4666 \r\nL 3928 4666 \r\nL 3928 4134 \r\nL 2272 4134 \r\nL 2272 0 \r\nL 1638 0 \r\nL 1638 4134 \r\nL -19 4134 \r\nL -19 4666 \r\nz\r\n\" id=\"DejaVuSans-54\" transform=\"scale(0.015625)\"/>\r\n       <path d=\"M 2188 4044 \r\nL 1331 1722 \r\nL 3047 1722 \r\nL 2188 4044 \r\nz\r\nM 1831 4666 \r\nL 2547 4666 \r\nL 4325 0 \r\nL 3669 0 \r\nL 3244 1197 \r\nL 1141 1197 \r\nL 716 0 \r\nL 50 0 \r\nL 1831 4666 \r\nz\r\n\" id=\"DejaVuSans-41\" transform=\"scale(0.015625)\"/>\r\n       <path d=\"M 3122 3366 \r\nL 3122 2828 \r\nQ 2878 2963 2633 3030 \r\nQ 2388 3097 2138 3097 \r\nQ 1578 3097 1268 2742 \r\nQ 959 2388 959 1747 \r\nQ 959 1106 1268 751 \r\nQ 1578 397 2138 397 \r\nQ 2388 397 2633 464 \r\nQ 2878 531 3122 666 \r\nL 3122 134 \r\nQ 2881 22 2623 -34 \r\nQ 2366 -91 2075 -91 \r\nQ 1284 -91 818 406 \r\nQ 353 903 353 1747 \r\nQ 353 2603 823 3093 \r\nQ 1294 3584 2113 3584 \r\nQ 2378 3584 2631 3529 \r\nQ 2884 3475 3122 3366 \r\nz\r\n\" id=\"DejaVuSans-63\" transform=\"scale(0.015625)\"/>\r\n      </defs>\r\n      <use xlink:href=\"#DejaVuSans-54\"/>\r\n      <use x=\"44.083984\" xlink:href=\"#DejaVuSans-65\"/>\r\n      <use x=\"105.607422\" xlink:href=\"#DejaVuSans-73\"/>\r\n      <use x=\"157.707031\" xlink:href=\"#DejaVuSans-74\"/>\r\n      <use x=\"196.916016\" xlink:href=\"#DejaVuSans-20\"/>\r\n      <use x=\"228.703125\" xlink:href=\"#DejaVuSans-41\"/>\r\n      <use x=\"295.361328\" xlink:href=\"#DejaVuSans-63\"/>\r\n      <use x=\"350.341797\" xlink:href=\"#DejaVuSans-63\"/>\r\n      <use x=\"405.322266\" xlink:href=\"#DejaVuSans-75\"/>\r\n      <use x=\"468.701172\" xlink:href=\"#DejaVuSans-72\"/>\r\n      <use x=\"509.814453\" xlink:href=\"#DejaVuSans-61\"/>\r\n      <use x=\"571.09375\" xlink:href=\"#DejaVuSans-63\"/>\r\n      <use x=\"626.074219\" xlink:href=\"#DejaVuSans-79\"/>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"line2d_13\">\r\n    <path clip-path=\"url(#p01157220f7)\" d=\"M 62.183807 47.03405 \r\nL 82.474716 20.078678 \r\nL 102.765625 47.03405 \r\nL 123.056534 32.058843 \r\nL 143.347443 35.053884 \r\nL 173.783807 56.019174 \r\nL 194.074716 59.014215 \r\nL 214.365625 73.989421 \r\nL 234.656534 88.964628 \r\nL 254.947443 94.954711 \r\nL 285.383807 127.900165 \r\nL 305.674716 127.900165 \r\nL 325.965625 130.895207 \r\nL 346.256534 169.830744 \r\nL 366.547443 190.796033 \r\n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-opacity:0.3;stroke-width:1.5;\"/>\r\n    <defs>\r\n     <path d=\"M 0 3 \r\nC 0.795609 3 1.55874 2.683901 2.12132 2.12132 \r\nC 2.683901 1.55874 3 0.795609 3 0 \r\nC 3 -0.795609 2.683901 -1.55874 2.12132 -2.12132 \r\nC 1.55874 -2.683901 0.795609 -3 0 -3 \r\nC -0.795609 -3 -1.55874 -2.683901 -2.12132 -2.12132 \r\nC -2.683901 -1.55874 -3 -0.795609 -3 0 \r\nC -3 0.795609 -2.683901 1.55874 -2.12132 2.12132 \r\nC -1.55874 2.683901 -0.795609 3 0 3 \r\nz\r\n\" id=\"mdad5604e33\" style=\"stroke:#ffffff;stroke-opacity:0.3;stroke-width:0.75;\"/>\r\n    </defs>\r\n    <g clip-path=\"url(#p01157220f7)\">\r\n     <use style=\"fill:#1f77b4;fill-opacity:0.3;stroke:#ffffff;stroke-opacity:0.3;stroke-width:0.75;\" x=\"62.183807\" xlink:href=\"#mdad5604e33\" y=\"47.03405\"/>\r\n     <use style=\"fill:#1f77b4;fill-opacity:0.3;stroke:#ffffff;stroke-opacity:0.3;stroke-width:0.75;\" x=\"82.474716\" xlink:href=\"#mdad5604e33\" y=\"20.078678\"/>\r\n     <use style=\"fill:#1f77b4;fill-opacity:0.3;stroke:#ffffff;stroke-opacity:0.3;stroke-width:0.75;\" x=\"102.765625\" xlink:href=\"#mdad5604e33\" y=\"47.03405\"/>\r\n     <use style=\"fill:#1f77b4;fill-opacity:0.3;stroke:#ffffff;stroke-opacity:0.3;stroke-width:0.75;\" x=\"123.056534\" xlink:href=\"#mdad5604e33\" y=\"32.058843\"/>\r\n     <use style=\"fill:#1f77b4;fill-opacity:0.3;stroke:#ffffff;stroke-opacity:0.3;stroke-width:0.75;\" x=\"143.347443\" xlink:href=\"#mdad5604e33\" y=\"35.053884\"/>\r\n     <use style=\"fill:#1f77b4;fill-opacity:0.3;stroke:#ffffff;stroke-opacity:0.3;stroke-width:0.75;\" x=\"173.783807\" xlink:href=\"#mdad5604e33\" y=\"56.019174\"/>\r\n     <use style=\"fill:#1f77b4;fill-opacity:0.3;stroke:#ffffff;stroke-opacity:0.3;stroke-width:0.75;\" x=\"194.074716\" xlink:href=\"#mdad5604e33\" y=\"59.014215\"/>\r\n     <use style=\"fill:#1f77b4;fill-opacity:0.3;stroke:#ffffff;stroke-opacity:0.3;stroke-width:0.75;\" x=\"214.365625\" xlink:href=\"#mdad5604e33\" y=\"73.989421\"/>\r\n     <use style=\"fill:#1f77b4;fill-opacity:0.3;stroke:#ffffff;stroke-opacity:0.3;stroke-width:0.75;\" x=\"234.656534\" xlink:href=\"#mdad5604e33\" y=\"88.964628\"/>\r\n     <use style=\"fill:#1f77b4;fill-opacity:0.3;stroke:#ffffff;stroke-opacity:0.3;stroke-width:0.75;\" x=\"254.947443\" xlink:href=\"#mdad5604e33\" y=\"94.954711\"/>\r\n     <use style=\"fill:#1f77b4;fill-opacity:0.3;stroke:#ffffff;stroke-opacity:0.3;stroke-width:0.75;\" x=\"285.383807\" xlink:href=\"#mdad5604e33\" y=\"127.900165\"/>\r\n     <use style=\"fill:#1f77b4;fill-opacity:0.3;stroke:#ffffff;stroke-opacity:0.3;stroke-width:0.75;\" x=\"305.674716\" xlink:href=\"#mdad5604e33\" y=\"127.900165\"/>\r\n     <use style=\"fill:#1f77b4;fill-opacity:0.3;stroke:#ffffff;stroke-opacity:0.3;stroke-width:0.75;\" x=\"325.965625\" xlink:href=\"#mdad5604e33\" y=\"130.895207\"/>\r\n     <use style=\"fill:#1f77b4;fill-opacity:0.3;stroke:#ffffff;stroke-opacity:0.3;stroke-width:0.75;\" x=\"346.256534\" xlink:href=\"#mdad5604e33\" y=\"169.830744\"/>\r\n     <use style=\"fill:#1f77b4;fill-opacity:0.3;stroke:#ffffff;stroke-opacity:0.3;stroke-width:0.75;\" x=\"366.547443\" xlink:href=\"#mdad5604e33\" y=\"190.796033\"/>\r\n    </g>\r\n   </g>\r\n   <g id=\"line2d_14\">\r\n    <path clip-path=\"url(#p01157220f7)\" d=\"M 62.183807 17.083636 \r\nL 82.474716 17.083636 \r\nL 102.765625 26.06876 \r\nL 123.056534 23.073719 \r\nL 143.347443 20.078678 \r\nL 173.783807 47.03405 \r\nL 194.074716 26.06876 \r\nL 214.365625 32.058843 \r\nL 234.656534 56.019174 \r\nL 254.947443 50.029091 \r\nL 285.383807 65.004298 \r\nL 305.674716 53.024132 \r\nL 325.965625 76.984463 \r\nL 346.256534 91.959669 \r\nL 366.547443 76.984463 \r\n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-opacity:0.3;stroke-width:1.5;\"/>\r\n    <defs>\r\n     <path d=\"M 0 3 \r\nC 0.795609 3 1.55874 2.683901 2.12132 2.12132 \r\nC 2.683901 1.55874 3 0.795609 3 0 \r\nC 3 -0.795609 2.683901 -1.55874 2.12132 -2.12132 \r\nC 1.55874 -2.683901 0.795609 -3 0 -3 \r\nC -0.795609 -3 -1.55874 -2.683901 -2.12132 -2.12132 \r\nC -2.683901 -1.55874 -3 -0.795609 -3 0 \r\nC -3 0.795609 -2.683901 1.55874 -2.12132 2.12132 \r\nC -1.55874 2.683901 -0.795609 3 0 3 \r\nz\r\n\" id=\"m566df594df\" style=\"stroke:#ffffff;stroke-opacity:0.3;stroke-width:0.75;\"/>\r\n    </defs>\r\n    <g clip-path=\"url(#p01157220f7)\">\r\n     <use style=\"fill:#ff7f0e;fill-opacity:0.3;stroke:#ffffff;stroke-opacity:0.3;stroke-width:0.75;\" x=\"62.183807\" xlink:href=\"#m566df594df\" y=\"17.083636\"/>\r\n     <use style=\"fill:#ff7f0e;fill-opacity:0.3;stroke:#ffffff;stroke-opacity:0.3;stroke-width:0.75;\" x=\"82.474716\" xlink:href=\"#m566df594df\" y=\"17.083636\"/>\r\n     <use style=\"fill:#ff7f0e;fill-opacity:0.3;stroke:#ffffff;stroke-opacity:0.3;stroke-width:0.75;\" x=\"102.765625\" xlink:href=\"#m566df594df\" y=\"26.06876\"/>\r\n     <use style=\"fill:#ff7f0e;fill-opacity:0.3;stroke:#ffffff;stroke-opacity:0.3;stroke-width:0.75;\" x=\"123.056534\" xlink:href=\"#m566df594df\" y=\"23.073719\"/>\r\n     <use style=\"fill:#ff7f0e;fill-opacity:0.3;stroke:#ffffff;stroke-opacity:0.3;stroke-width:0.75;\" x=\"143.347443\" xlink:href=\"#m566df594df\" y=\"20.078678\"/>\r\n     <use style=\"fill:#ff7f0e;fill-opacity:0.3;stroke:#ffffff;stroke-opacity:0.3;stroke-width:0.75;\" x=\"173.783807\" xlink:href=\"#m566df594df\" y=\"47.03405\"/>\r\n     <use style=\"fill:#ff7f0e;fill-opacity:0.3;stroke:#ffffff;stroke-opacity:0.3;stroke-width:0.75;\" x=\"194.074716\" xlink:href=\"#m566df594df\" y=\"26.06876\"/>\r\n     <use style=\"fill:#ff7f0e;fill-opacity:0.3;stroke:#ffffff;stroke-opacity:0.3;stroke-width:0.75;\" x=\"214.365625\" xlink:href=\"#m566df594df\" y=\"32.058843\"/>\r\n     <use style=\"fill:#ff7f0e;fill-opacity:0.3;stroke:#ffffff;stroke-opacity:0.3;stroke-width:0.75;\" x=\"234.656534\" xlink:href=\"#m566df594df\" y=\"56.019174\"/>\r\n     <use style=\"fill:#ff7f0e;fill-opacity:0.3;stroke:#ffffff;stroke-opacity:0.3;stroke-width:0.75;\" x=\"254.947443\" xlink:href=\"#m566df594df\" y=\"50.029091\"/>\r\n     <use style=\"fill:#ff7f0e;fill-opacity:0.3;stroke:#ffffff;stroke-opacity:0.3;stroke-width:0.75;\" x=\"285.383807\" xlink:href=\"#m566df594df\" y=\"65.004298\"/>\r\n     <use style=\"fill:#ff7f0e;fill-opacity:0.3;stroke:#ffffff;stroke-opacity:0.3;stroke-width:0.75;\" x=\"305.674716\" xlink:href=\"#m566df594df\" y=\"53.024132\"/>\r\n     <use style=\"fill:#ff7f0e;fill-opacity:0.3;stroke:#ffffff;stroke-opacity:0.3;stroke-width:0.75;\" x=\"325.965625\" xlink:href=\"#m566df594df\" y=\"76.984463\"/>\r\n     <use style=\"fill:#ff7f0e;fill-opacity:0.3;stroke:#ffffff;stroke-opacity:0.3;stroke-width:0.75;\" x=\"346.256534\" xlink:href=\"#m566df594df\" y=\"91.959669\"/>\r\n     <use style=\"fill:#ff7f0e;fill-opacity:0.3;stroke:#ffffff;stroke-opacity:0.3;stroke-width:0.75;\" x=\"366.547443\" xlink:href=\"#m566df594df\" y=\"76.984463\"/>\r\n    </g>\r\n   </g>\r\n   <g id=\"line2d_15\">\r\n    <path clip-path=\"url(#p01157220f7)\" d=\"M 62.183807 23.073719 \r\nL 82.474716 70.99438 \r\nL 102.765625 47.03405 \r\nL 123.056534 29.063802 \r\nL 143.347443 47.03405 \r\nL 173.783807 50.029091 \r\nL 194.074716 35.053884 \r\nL 214.365625 47.03405 \r\nL 234.656534 50.029091 \r\nL 254.947443 44.039008 \r\nL 285.383807 38.048926 \r\nL 305.674716 38.048926 \r\nL 325.965625 56.019174 \r\nL 346.256534 35.053884 \r\nL 366.547443 32.058843 \r\n\" style=\"fill:none;stroke:#2ca02c;stroke-linecap:square;stroke-width:1.5;\"/>\r\n    <defs>\r\n     <path d=\"M -3 3 \r\nL 3 3 \r\nL 3 -3 \r\nL -3 -3 \r\nz\r\n\" id=\"m7e8e9db2c7\" style=\"stroke:#ffffff;stroke-linejoin:miter;stroke-width:0.75;\"/>\r\n    </defs>\r\n    <g clip-path=\"url(#p01157220f7)\">\r\n     <use style=\"fill:#2ca02c;stroke:#ffffff;stroke-linejoin:miter;stroke-width:0.75;\" x=\"62.183807\" xlink:href=\"#m7e8e9db2c7\" y=\"23.073719\"/>\r\n     <use style=\"fill:#2ca02c;stroke:#ffffff;stroke-linejoin:miter;stroke-width:0.75;\" x=\"82.474716\" xlink:href=\"#m7e8e9db2c7\" y=\"70.99438\"/>\r\n     <use style=\"fill:#2ca02c;stroke:#ffffff;stroke-linejoin:miter;stroke-width:0.75;\" x=\"102.765625\" xlink:href=\"#m7e8e9db2c7\" y=\"47.03405\"/>\r\n     <use style=\"fill:#2ca02c;stroke:#ffffff;stroke-linejoin:miter;stroke-width:0.75;\" x=\"123.056534\" xlink:href=\"#m7e8e9db2c7\" y=\"29.063802\"/>\r\n     <use style=\"fill:#2ca02c;stroke:#ffffff;stroke-linejoin:miter;stroke-width:0.75;\" x=\"143.347443\" xlink:href=\"#m7e8e9db2c7\" y=\"47.03405\"/>\r\n     <use style=\"fill:#2ca02c;stroke:#ffffff;stroke-linejoin:miter;stroke-width:0.75;\" x=\"173.783807\" xlink:href=\"#m7e8e9db2c7\" y=\"50.029091\"/>\r\n     <use style=\"fill:#2ca02c;stroke:#ffffff;stroke-linejoin:miter;stroke-width:0.75;\" x=\"194.074716\" xlink:href=\"#m7e8e9db2c7\" y=\"35.053884\"/>\r\n     <use style=\"fill:#2ca02c;stroke:#ffffff;stroke-linejoin:miter;stroke-width:0.75;\" x=\"214.365625\" xlink:href=\"#m7e8e9db2c7\" y=\"47.03405\"/>\r\n     <use style=\"fill:#2ca02c;stroke:#ffffff;stroke-linejoin:miter;stroke-width:0.75;\" x=\"234.656534\" xlink:href=\"#m7e8e9db2c7\" y=\"50.029091\"/>\r\n     <use style=\"fill:#2ca02c;stroke:#ffffff;stroke-linejoin:miter;stroke-width:0.75;\" x=\"254.947443\" xlink:href=\"#m7e8e9db2c7\" y=\"44.039008\"/>\r\n     <use style=\"fill:#2ca02c;stroke:#ffffff;stroke-linejoin:miter;stroke-width:0.75;\" x=\"285.383807\" xlink:href=\"#m7e8e9db2c7\" y=\"38.048926\"/>\r\n     <use style=\"fill:#2ca02c;stroke:#ffffff;stroke-linejoin:miter;stroke-width:0.75;\" x=\"305.674716\" xlink:href=\"#m7e8e9db2c7\" y=\"38.048926\"/>\r\n     <use style=\"fill:#2ca02c;stroke:#ffffff;stroke-linejoin:miter;stroke-width:0.75;\" x=\"325.965625\" xlink:href=\"#m7e8e9db2c7\" y=\"56.019174\"/>\r\n     <use style=\"fill:#2ca02c;stroke:#ffffff;stroke-linejoin:miter;stroke-width:0.75;\" x=\"346.256534\" xlink:href=\"#m7e8e9db2c7\" y=\"35.053884\"/>\r\n     <use style=\"fill:#2ca02c;stroke:#ffffff;stroke-linejoin:miter;stroke-width:0.75;\" x=\"366.547443\" xlink:href=\"#m7e8e9db2c7\" y=\"32.058843\"/>\r\n    </g>\r\n   </g>\r\n   <g id=\"line2d_16\">\r\n    <path clip-path=\"url(#p01157220f7)\" d=\"M 62.183807 23.073719 \r\nL 82.474716 44.039008 \r\nL 102.765625 35.053884 \r\nL 123.056534 35.053884 \r\nL 143.347443 26.06876 \r\nL 173.783807 38.048926 \r\nL 194.074716 26.06876 \r\nL 214.365625 38.048926 \r\nL 234.656534 38.048926 \r\nL 254.947443 32.058843 \r\nL 285.383807 26.06876 \r\nL 305.674716 35.053884 \r\nL 325.965625 35.053884 \r\nL 346.256534 32.058843 \r\nL 366.547443 35.053884 \r\n\" style=\"fill:none;stroke:#d62728;stroke-linecap:square;stroke-width:1.5;\"/>\r\n    <defs>\r\n     <path d=\"M 0 3 \r\nC 0.795609 3 1.55874 2.683901 2.12132 2.12132 \r\nC 2.683901 1.55874 3 0.795609 3 0 \r\nC 3 -0.795609 2.683901 -1.55874 2.12132 -2.12132 \r\nC 1.55874 -2.683901 0.795609 -3 0 -3 \r\nC -0.795609 -3 -1.55874 -2.683901 -2.12132 -2.12132 \r\nC -2.683901 -1.55874 -3 -0.795609 -3 0 \r\nC -3 0.795609 -2.683901 1.55874 -2.12132 2.12132 \r\nC -1.55874 2.683901 -0.795609 3 0 3 \r\nz\r\n\" id=\"m40d21c9d03\" style=\"stroke:#ffffff;stroke-width:0.75;\"/>\r\n    </defs>\r\n    <g clip-path=\"url(#p01157220f7)\">\r\n     <use style=\"fill:#d62728;stroke:#ffffff;stroke-width:0.75;\" x=\"62.183807\" xlink:href=\"#m40d21c9d03\" y=\"23.073719\"/>\r\n     <use style=\"fill:#d62728;stroke:#ffffff;stroke-width:0.75;\" x=\"82.474716\" xlink:href=\"#m40d21c9d03\" y=\"44.039008\"/>\r\n     <use style=\"fill:#d62728;stroke:#ffffff;stroke-width:0.75;\" x=\"102.765625\" xlink:href=\"#m40d21c9d03\" y=\"35.053884\"/>\r\n     <use style=\"fill:#d62728;stroke:#ffffff;stroke-width:0.75;\" x=\"123.056534\" xlink:href=\"#m40d21c9d03\" y=\"35.053884\"/>\r\n     <use style=\"fill:#d62728;stroke:#ffffff;stroke-width:0.75;\" x=\"143.347443\" xlink:href=\"#m40d21c9d03\" y=\"26.06876\"/>\r\n     <use style=\"fill:#d62728;stroke:#ffffff;stroke-width:0.75;\" x=\"173.783807\" xlink:href=\"#m40d21c9d03\" y=\"38.048926\"/>\r\n     <use style=\"fill:#d62728;stroke:#ffffff;stroke-width:0.75;\" x=\"194.074716\" xlink:href=\"#m40d21c9d03\" y=\"26.06876\"/>\r\n     <use style=\"fill:#d62728;stroke:#ffffff;stroke-width:0.75;\" x=\"214.365625\" xlink:href=\"#m40d21c9d03\" y=\"38.048926\"/>\r\n     <use style=\"fill:#d62728;stroke:#ffffff;stroke-width:0.75;\" x=\"234.656534\" xlink:href=\"#m40d21c9d03\" y=\"38.048926\"/>\r\n     <use style=\"fill:#d62728;stroke:#ffffff;stroke-width:0.75;\" x=\"254.947443\" xlink:href=\"#m40d21c9d03\" y=\"32.058843\"/>\r\n     <use style=\"fill:#d62728;stroke:#ffffff;stroke-width:0.75;\" x=\"285.383807\" xlink:href=\"#m40d21c9d03\" y=\"26.06876\"/>\r\n     <use style=\"fill:#d62728;stroke:#ffffff;stroke-width:0.75;\" x=\"305.674716\" xlink:href=\"#m40d21c9d03\" y=\"35.053884\"/>\r\n     <use style=\"fill:#d62728;stroke:#ffffff;stroke-width:0.75;\" x=\"325.965625\" xlink:href=\"#m40d21c9d03\" y=\"35.053884\"/>\r\n     <use style=\"fill:#d62728;stroke:#ffffff;stroke-width:0.75;\" x=\"346.256534\" xlink:href=\"#m40d21c9d03\" y=\"32.058843\"/>\r\n     <use style=\"fill:#d62728;stroke:#ffffff;stroke-width:0.75;\" x=\"366.547443\" xlink:href=\"#m40d21c9d03\" y=\"35.053884\"/>\r\n    </g>\r\n   </g>\r\n   <g id=\"patch_3\">\r\n    <path d=\"M 46.965625 224.64 \r\nL 46.965625 7.2 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_4\">\r\n    <path d=\"M 381.765625 224.64 \r\nL 381.765625 7.2 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_5\">\r\n    <path d=\"M 46.965625 224.64 \r\nL 381.765625 224.64 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_6\">\r\n    <path d=\"M 46.965625 7.2 \r\nL 381.765625 7.2 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"legend_1\">\r\n    <g id=\"patch_7\">\r\n     <path d=\"M 53.965625 219.64 \r\nL 189.7875 219.64 \r\nQ 191.7875 219.64 191.7875 217.64 \r\nL 191.7875 159.9275 \r\nQ 191.7875 157.9275 189.7875 157.9275 \r\nL 53.965625 157.9275 \r\nQ 51.965625 157.9275 51.965625 159.9275 \r\nL 51.965625 217.64 \r\nQ 51.965625 219.64 53.965625 219.64 \r\nz\r\n\" style=\"fill:#ffffff;opacity:0.8;stroke:#cccccc;stroke-linejoin:miter;\"/>\r\n    </g>\r\n    <g id=\"line2d_17\">\r\n     <path d=\"M 55.965625 166.025937 \r\nL 75.965625 166.025937 \r\n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-opacity:0.3;stroke-width:1.5;\"/>\r\n    </g>\r\n    <g id=\"line2d_18\">\r\n     <g>\r\n      <use style=\"fill:#1f77b4;fill-opacity:0.3;stroke:#ffffff;stroke-opacity:0.3;stroke-width:0.75;\" x=\"65.965625\" xlink:href=\"#mdad5604e33\" y=\"166.025937\"/>\r\n     </g>\r\n    </g>\r\n    <g id=\"text_15\">\r\n     <!-- No Feature Selection -->\r\n     <g transform=\"translate(83.965625 169.525937)scale(0.1 -0.1)\">\r\n      <defs>\r\n       <path d=\"M 3425 4513 \r\nL 3425 3897 \r\nQ 3066 4069 2747 4153 \r\nQ 2428 4238 2131 4238 \r\nQ 1616 4238 1336 4038 \r\nQ 1056 3838 1056 3469 \r\nQ 1056 3159 1242 3001 \r\nQ 1428 2844 1947 2747 \r\nL 2328 2669 \r\nQ 3034 2534 3370 2195 \r\nQ 3706 1856 3706 1288 \r\nQ 3706 609 3251 259 \r\nQ 2797 -91 1919 -91 \r\nQ 1588 -91 1214 -16 \r\nQ 841 59 441 206 \r\nL 441 856 \r\nQ 825 641 1194 531 \r\nQ 1563 422 1919 422 \r\nQ 2459 422 2753 634 \r\nQ 3047 847 3047 1241 \r\nQ 3047 1584 2836 1778 \r\nQ 2625 1972 2144 2069 \r\nL 1759 2144 \r\nQ 1053 2284 737 2584 \r\nQ 422 2884 422 3419 \r\nQ 422 4038 858 4394 \r\nQ 1294 4750 2059 4750 \r\nQ 2388 4750 2728 4690 \r\nQ 3069 4631 3425 4513 \r\nz\r\n\" id=\"DejaVuSans-53\" transform=\"scale(0.015625)\"/>\r\n       <path d=\"M 603 4863 \r\nL 1178 4863 \r\nL 1178 0 \r\nL 603 0 \r\nL 603 4863 \r\nz\r\n\" id=\"DejaVuSans-6c\" transform=\"scale(0.015625)\"/>\r\n       <path d=\"M 3513 2113 \r\nL 3513 0 \r\nL 2938 0 \r\nL 2938 2094 \r\nQ 2938 2591 2744 2837 \r\nQ 2550 3084 2163 3084 \r\nQ 1697 3084 1428 2787 \r\nQ 1159 2491 1159 1978 \r\nL 1159 0 \r\nL 581 0 \r\nL 581 3500 \r\nL 1159 3500 \r\nL 1159 2956 \r\nQ 1366 3272 1645 3428 \r\nQ 1925 3584 2291 3584 \r\nQ 2894 3584 3203 3211 \r\nQ 3513 2838 3513 2113 \r\nz\r\n\" id=\"DejaVuSans-6e\" transform=\"scale(0.015625)\"/>\r\n      </defs>\r\n      <use xlink:href=\"#DejaVuSans-4e\"/>\r\n      <use x=\"74.804688\" xlink:href=\"#DejaVuSans-6f\"/>\r\n      <use x=\"135.986328\" xlink:href=\"#DejaVuSans-20\"/>\r\n      <use x=\"167.773438\" xlink:href=\"#DejaVuSans-46\"/>\r\n      <use x=\"219.792969\" xlink:href=\"#DejaVuSans-65\"/>\r\n      <use x=\"281.316406\" xlink:href=\"#DejaVuSans-61\"/>\r\n      <use x=\"342.595703\" xlink:href=\"#DejaVuSans-74\"/>\r\n      <use x=\"381.804688\" xlink:href=\"#DejaVuSans-75\"/>\r\n      <use x=\"445.183594\" xlink:href=\"#DejaVuSans-72\"/>\r\n      <use x=\"484.046875\" xlink:href=\"#DejaVuSans-65\"/>\r\n      <use x=\"545.570312\" xlink:href=\"#DejaVuSans-20\"/>\r\n      <use x=\"577.357422\" xlink:href=\"#DejaVuSans-53\"/>\r\n      <use x=\"640.833984\" xlink:href=\"#DejaVuSans-65\"/>\r\n      <use x=\"702.357422\" xlink:href=\"#DejaVuSans-6c\"/>\r\n      <use x=\"730.140625\" xlink:href=\"#DejaVuSans-65\"/>\r\n      <use x=\"791.664062\" xlink:href=\"#DejaVuSans-63\"/>\r\n      <use x=\"846.644531\" xlink:href=\"#DejaVuSans-74\"/>\r\n      <use x=\"885.853516\" xlink:href=\"#DejaVuSans-69\"/>\r\n      <use x=\"913.636719\" xlink:href=\"#DejaVuSans-6f\"/>\r\n      <use x=\"974.818359\" xlink:href=\"#DejaVuSans-6e\"/>\r\n     </g>\r\n    </g>\r\n    <g id=\"line2d_19\">\r\n     <path d=\"M 55.965625 180.704062 \r\nL 75.965625 180.704062 \r\n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-opacity:0.3;stroke-width:1.5;\"/>\r\n    </g>\r\n    <g id=\"line2d_20\">\r\n     <g>\r\n      <use style=\"fill:#ff7f0e;fill-opacity:0.3;stroke:#ffffff;stroke-opacity:0.3;stroke-width:0.75;\" x=\"65.965625\" xlink:href=\"#m566df594df\" y=\"180.704062\"/>\r\n     </g>\r\n    </g>\r\n    <g id=\"text_16\">\r\n     <!-- Gini-Filter Half 0.5 -->\r\n     <g transform=\"translate(83.965625 184.204062)scale(0.1 -0.1)\">\r\n      <defs>\r\n       <path d=\"M 3809 666 \r\nL 3809 1919 \r\nL 2778 1919 \r\nL 2778 2438 \r\nL 4434 2438 \r\nL 4434 434 \r\nQ 4069 175 3628 42 \r\nQ 3188 -91 2688 -91 \r\nQ 1594 -91 976 548 \r\nQ 359 1188 359 2328 \r\nQ 359 3472 976 4111 \r\nQ 1594 4750 2688 4750 \r\nQ 3144 4750 3555 4637 \r\nQ 3966 4525 4313 4306 \r\nL 4313 3634 \r\nQ 3963 3931 3569 4081 \r\nQ 3175 4231 2741 4231 \r\nQ 1884 4231 1454 3753 \r\nQ 1025 3275 1025 2328 \r\nQ 1025 1384 1454 906 \r\nQ 1884 428 2741 428 \r\nQ 3075 428 3337 486 \r\nQ 3600 544 3809 666 \r\nz\r\n\" id=\"DejaVuSans-47\" transform=\"scale(0.015625)\"/>\r\n       <path d=\"M 313 2009 \r\nL 1997 2009 \r\nL 1997 1497 \r\nL 313 1497 \r\nL 313 2009 \r\nz\r\n\" id=\"DejaVuSans-2d\" transform=\"scale(0.015625)\"/>\r\n       <path d=\"M 628 4666 \r\nL 1259 4666 \r\nL 1259 2753 \r\nL 3553 2753 \r\nL 3553 4666 \r\nL 4184 4666 \r\nL 4184 0 \r\nL 3553 0 \r\nL 3553 2222 \r\nL 1259 2222 \r\nL 1259 0 \r\nL 628 0 \r\nL 628 4666 \r\nz\r\n\" id=\"DejaVuSans-48\" transform=\"scale(0.015625)\"/>\r\n       <path d=\"M 684 794 \r\nL 1344 794 \r\nL 1344 0 \r\nL 684 0 \r\nL 684 794 \r\nz\r\n\" id=\"DejaVuSans-2e\" transform=\"scale(0.015625)\"/>\r\n      </defs>\r\n      <use xlink:href=\"#DejaVuSans-47\"/>\r\n      <use x=\"77.490234\" xlink:href=\"#DejaVuSans-69\"/>\r\n      <use x=\"105.273438\" xlink:href=\"#DejaVuSans-6e\"/>\r\n      <use x=\"168.652344\" xlink:href=\"#DejaVuSans-69\"/>\r\n      <use x=\"196.435547\" xlink:href=\"#DejaVuSans-2d\"/>\r\n      <use x=\"232.519531\" xlink:href=\"#DejaVuSans-46\"/>\r\n      <use x=\"282.789062\" xlink:href=\"#DejaVuSans-69\"/>\r\n      <use x=\"310.572266\" xlink:href=\"#DejaVuSans-6c\"/>\r\n      <use x=\"338.355469\" xlink:href=\"#DejaVuSans-74\"/>\r\n      <use x=\"377.564453\" xlink:href=\"#DejaVuSans-65\"/>\r\n      <use x=\"439.087891\" xlink:href=\"#DejaVuSans-72\"/>\r\n      <use x=\"480.201172\" xlink:href=\"#DejaVuSans-20\"/>\r\n      <use x=\"511.988281\" xlink:href=\"#DejaVuSans-48\"/>\r\n      <use x=\"587.183594\" xlink:href=\"#DejaVuSans-61\"/>\r\n      <use x=\"648.462891\" xlink:href=\"#DejaVuSans-6c\"/>\r\n      <use x=\"676.246094\" xlink:href=\"#DejaVuSans-66\"/>\r\n      <use x=\"711.451172\" xlink:href=\"#DejaVuSans-20\"/>\r\n      <use x=\"743.238281\" xlink:href=\"#DejaVuSans-30\"/>\r\n      <use x=\"806.861328\" xlink:href=\"#DejaVuSans-2e\"/>\r\n      <use x=\"838.648438\" xlink:href=\"#DejaVuSans-35\"/>\r\n     </g>\r\n    </g>\r\n    <g id=\"line2d_21\">\r\n     <path d=\"M 55.965625 195.382187 \r\nL 75.965625 195.382187 \r\n\" style=\"fill:none;stroke:#2ca02c;stroke-linecap:square;stroke-width:1.5;\"/>\r\n    </g>\r\n    <g id=\"line2d_22\">\r\n     <g>\r\n      <use style=\"fill:#2ca02c;stroke:#ffffff;stroke-linejoin:miter;stroke-width:0.75;\" x=\"65.965625\" xlink:href=\"#m7e8e9db2c7\" y=\"195.382187\"/>\r\n     </g>\r\n    </g>\r\n    <g id=\"text_17\">\r\n     <!-- STG -->\r\n     <g transform=\"translate(83.965625 198.882187)scale(0.1 -0.1)\">\r\n      <use xlink:href=\"#DejaVuSans-53\"/>\r\n      <use x=\"63.476562\" xlink:href=\"#DejaVuSans-54\"/>\r\n      <use x=\"124.560547\" xlink:href=\"#DejaVuSans-47\"/>\r\n     </g>\r\n    </g>\r\n    <g id=\"line2d_23\">\r\n     <path d=\"M 55.965625 210.060312 \r\nL 75.965625 210.060312 \r\n\" style=\"fill:none;stroke:#d62728;stroke-linecap:square;stroke-width:1.5;\"/>\r\n    </g>\r\n    <g id=\"line2d_24\">\r\n     <g>\r\n      <use style=\"fill:#d62728;stroke:#ffffff;stroke-width:0.75;\" x=\"65.965625\" xlink:href=\"#m40d21c9d03\" y=\"210.060312\"/>\r\n     </g>\r\n    </g>\r\n    <g id=\"text_18\">\r\n     <!-- GINI + STG -->\r\n     <g transform=\"translate(83.965625 213.560312)scale(0.1 -0.1)\">\r\n      <defs>\r\n       <path d=\"M 628 4666 \r\nL 1259 4666 \r\nL 1259 0 \r\nL 628 0 \r\nL 628 4666 \r\nz\r\n\" id=\"DejaVuSans-49\" transform=\"scale(0.015625)\"/>\r\n       <path d=\"M 2944 4013 \r\nL 2944 2272 \r\nL 4684 2272 \r\nL 4684 1741 \r\nL 2944 1741 \r\nL 2944 0 \r\nL 2419 0 \r\nL 2419 1741 \r\nL 678 1741 \r\nL 678 2272 \r\nL 2419 2272 \r\nL 2419 4013 \r\nL 2944 4013 \r\nz\r\n\" id=\"DejaVuSans-2b\" transform=\"scale(0.015625)\"/>\r\n      </defs>\r\n      <use xlink:href=\"#DejaVuSans-47\"/>\r\n      <use x=\"77.490234\" xlink:href=\"#DejaVuSans-49\"/>\r\n      <use x=\"106.982422\" xlink:href=\"#DejaVuSans-4e\"/>\r\n      <use x=\"181.787109\" xlink:href=\"#DejaVuSans-49\"/>\r\n      <use x=\"211.279297\" xlink:href=\"#DejaVuSans-20\"/>\r\n      <use x=\"243.066406\" xlink:href=\"#DejaVuSans-2b\"/>\r\n      <use x=\"326.855469\" xlink:href=\"#DejaVuSans-20\"/>\r\n      <use x=\"358.642578\" xlink:href=\"#DejaVuSans-53\"/>\r\n      <use x=\"422.119141\" xlink:href=\"#DejaVuSans-54\"/>\r\n      <use x=\"483.203125\" xlink:href=\"#DejaVuSans-47\"/>\r\n     </g>\r\n    </g>\r\n   </g>\r\n  </g>\r\n </g>\r\n <defs>\r\n  <clipPath id=\"p01157220f7\">\r\n   <rect height=\"217.44\" width=\"334.8\" x=\"46.965625\" y=\"7.2\"/>\r\n  </clipPath>\r\n </defs>\r\n</svg>\r\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.lineplot(data=nofs_results, x=\"# features\", y=\"accuracy\", \n",
    "label='No Feature Selection', marker='o', alpha=0.3)\n",
    "sns.lineplot(data=gini_half_results, x=\"# features\", y=\"accuracy\",\n",
    "label='Gini-Filter Half 0.5', marker='o', alpha=0.3)\n",
    "sns.lineplot(data=stg_results, x=\"# features\", y=\"accuracy\", \n",
    "label='STG', marker='s')\n",
    "sns.lineplot(data=two_step_results, x=\"# features\", y=\"accuracy\",\n",
    "label='GINI + STG', marker='o')\n",
    "plt.xlabel('Number of Noisy Features')\n",
    "plt.ylabel('Test Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame(nofs_results).to_csv('IrisMotivation/no_fs_results.csv')\n",
    "pd.DataFrame(gini_half_results).to_csv('IrisMotivation/gini_half_results.csv')\n",
    "pd.DataFrame(stg_results).to_csv('IrisMotivation/stg_results.csv')\n",
    "pd.DataFrame(two_step_results).to_csv('IrisMotivation/two_step_results.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convergence Speed Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000 Loss: 1.088822841644287 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 33.333333333333336\n",
      "Epoch 2/1000 Loss: 1.0880446434020996 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 33.333333333333336\n",
      "Epoch 3/1000 Loss: 1.0865976810455322 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 33.333333333333336\n",
      "Epoch 4/1000 Loss: 1.0845359563827515 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 33.333333333333336\n",
      "Epoch 5/1000 Loss: 1.0818235874176025 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 33.333333333333336\n",
      "Epoch 6/1000 Loss: 1.078792691230774 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 33.333333333333336\n",
      "Epoch 7/1000 Loss: 1.0754834413528442 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 33.333333333333336\n",
      "Epoch 8/1000 Loss: 1.0718413591384888 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 33.333333333333336\n",
      "Epoch 9/1000 Loss: 1.0676801204681396 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 36.111111111111114\n",
      "Epoch 10/1000 Loss: 1.0628113746643066 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 38.888888888888886\n",
      "Epoch 11/1000 Loss: 1.0568482875823975 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 41.666666666666664\n",
      "Epoch 12/1000 Loss: 1.0497359037399292 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 44.44444444444444\n",
      "Epoch 13/1000 Loss: 1.0410627126693726 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 44.44444444444444\n",
      "Epoch 14/1000 Loss: 1.0301319360733032 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 47.22222222222222\n",
      "Epoch 15/1000 Loss: 1.0162620544433594 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 52.77777777777778\n",
      "Epoch 16/1000 Loss: 0.9987338781356812 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 52.77777777777778\n",
      "Epoch 17/1000 Loss: 0.9767977595329285 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 58.333333333333336\n",
      "Epoch 18/1000 Loss: 0.949190616607666 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 61.111111111111114\n",
      "Epoch 19/1000 Loss: 0.9152268171310425 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 61.111111111111114\n",
      "Epoch 20/1000 Loss: 0.8736124634742737 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 61.111111111111114\n",
      "Epoch 21/1000 Loss: 0.8237116932868958 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 61.111111111111114\n",
      "Epoch 22/1000 Loss: 0.7665058374404907 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 61.111111111111114\n",
      "Epoch 23/1000 Loss: 0.7036651372909546 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 61.111111111111114\n",
      "Epoch 24/1000 Loss: 0.6395127177238464 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 61.111111111111114\n",
      "Epoch 25/1000 Loss: 0.5784406661987305 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 61.111111111111114\n",
      "Epoch 26/1000 Loss: 0.5232645869255066 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 61.111111111111114\n",
      "Epoch 27/1000 Loss: 0.4751880168914795 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 61.111111111111114\n",
      "Epoch 28/1000 Loss: 0.43208077549934387 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 66.66666666666667\n",
      "Epoch 29/1000 Loss: 0.3925797939300537 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 83.33333333333333\n",
      "Epoch 30/1000 Loss: 0.35732370615005493 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 88.88888888888889\n",
      "Epoch 31/1000 Loss: 0.32792454957962036 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 32/1000 Loss: 0.30218449234962463 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 33/1000 Loss: 0.2755862772464752 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 34/1000 Loss: 0.24906453490257263 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 35/1000 Loss: 0.22297577559947968 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 36/1000 Loss: 0.199813574552536 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 37/1000 Loss: 0.17830979824066162 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 38/1000 Loss: 0.1591658890247345 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 39/1000 Loss: 0.14197364449501038 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 40/1000 Loss: 0.12747548520565033 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 41/1000 Loss: 0.11389323323965073 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 42/1000 Loss: 0.10257640480995178 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 43/1000 Loss: 0.09218540787696838 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 44/1000 Loss: 0.08337825536727905 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 45/1000 Loss: 0.07518893480300903 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 46/1000 Loss: 0.06794402748346329 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 47/1000 Loss: 0.06136009097099304 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 48/1000 Loss: 0.055226318538188934 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 49/1000 Loss: 0.0496772937476635 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 50/1000 Loss: 0.044371359050273895 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 51/1000 Loss: 0.03958650305867195 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 52/1000 Loss: 0.034946173429489136 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 53/1000 Loss: 0.0308612622320652 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 54/1000 Loss: 0.027056250721216202 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 55/1000 Loss: 0.023792240768671036 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 56/1000 Loss: 0.020753102377057076 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 57/1000 Loss: 0.018194522708654404 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 58/1000 Loss: 0.01590569317340851 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 59/1000 Loss: 0.014061212539672852 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 60/1000 Loss: 0.012500944547355175 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 61/1000 Loss: 0.011180400848388672 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 62/1000 Loss: 0.010131639428436756 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 63/1000 Loss: 0.009229320101439953 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 64/1000 Loss: 0.008482652716338634 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 65/1000 Loss: 0.007827721536159515 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 66/1000 Loss: 0.007240959443151951 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 67/1000 Loss: 0.006734746042639017 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 68/1000 Loss: 0.00627365056425333 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 69/1000 Loss: 0.005850622896105051 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 70/1000 Loss: 0.005474289879202843 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 71/1000 Loss: 0.005127814132720232 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 72/1000 Loss: 0.004807115066796541 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 73/1000 Loss: 0.004519605077803135 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 74/1000 Loss: 0.004259422887116671 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 75/1000 Loss: 0.004019715823233128 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 76/1000 Loss: 0.003803097177296877 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 77/1000 Loss: 0.0036102819722145796 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 78/1000 Loss: 0.003435896011069417 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 79/1000 Loss: 0.0032768207602202892 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 80/1000 Loss: 0.00313380709849298 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 81/1000 Loss: 0.0030060093849897385 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 82/1000 Loss: 0.0028900904580950737 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 83/1000 Loss: 0.0027837674133479595 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 84/1000 Loss: 0.002686871914193034 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 85/1000 Loss: 0.0025990100111812353 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 86/1000 Loss: 0.002518471796065569 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 87/1000 Loss: 0.002443593228235841 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 88/1000 Loss: 0.0023739198222756386 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 89/1000 Loss: 0.0023093849886208773 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 90/1000 Loss: 0.0022495384328067303 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 91/1000 Loss: 0.00219345442019403 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 92/1000 Loss: 0.0021405313163995743 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 93/1000 Loss: 0.002090662019327283 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 94/1000 Loss: 0.0020438663195818663 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 95/1000 Loss: 0.0019998853094875813 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 96/1000 Loss: 0.0019583022221922874 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 97/1000 Loss: 0.0019188260193914175 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 98/1000 Loss: 0.0018813873175531626 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 99/1000 Loss: 0.0018459876300767064 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 100/1000 Loss: 0.0018124806229025126 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 101/1000 Loss: 0.0017806575633585453 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 102/1000 Loss: 0.0017502992413938046 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 103/1000 Loss: 0.0017213200917467475 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 104/1000 Loss: 0.0016937066102400422 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 105/1000 Loss: 0.0016673762584105134 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 106/1000 Loss: 0.0016421936452388763 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 107/1000 Loss: 0.0016180266393348575 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 108/1000 Loss: 0.0015947965439409018 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 109/1000 Loss: 0.001572454464621842 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 110/1000 Loss: 0.0015509562799707055 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 111/1000 Loss: 0.0015302611282095313 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 112/1000 Loss: 0.0015102577162906528 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 113/1000 Loss: 0.0014908952871337533 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 114/1000 Loss: 0.0014721292536705732 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 115/1000 Loss: 0.001453953213058412 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 116/1000 Loss: 0.0014363236259669065 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 117/1000 Loss: 0.0014192117378115654 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 118/1000 Loss: 0.0014025761047378182 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 119/1000 Loss: 0.0013863699277862906 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 120/1000 Loss: 0.0013705836609005928 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 121/1000 Loss: 0.0013552202144637704 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 122/1000 Loss: 0.0013402553740888834 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 123/1000 Loss: 0.0013256559614092112 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 124/1000 Loss: 0.0013113964814692736 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 125/1000 Loss: 0.0012974912533536553 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 126/1000 Loss: 0.0012838991824537516 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 127/1000 Loss: 0.0012706310953944921 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 128/1000 Loss: 0.0012577580055221915 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 129/1000 Loss: 0.001245165360160172 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 130/1000 Loss: 0.0012328497832641006 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 131/1000 Loss: 0.001220806734636426 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 132/1000 Loss: 0.0012090225936844945 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 133/1000 Loss: 0.001197463134303689 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 134/1000 Loss: 0.001186150242574513 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 135/1000 Loss: 0.0011750530684366822 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 136/1000 Loss: 0.0011641921009868383 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 137/1000 Loss: 0.0011535381199792027 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 138/1000 Loss: 0.001143079251050949 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 139/1000 Loss: 0.0011328259715810418 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 140/1000 Loss: 0.0011227543000131845 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 141/1000 Loss: 0.0011128614423796535 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 142/1000 Loss: 0.0011031634639948606 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 143/1000 Loss: 0.001093616709113121 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 144/1000 Loss: 0.001084249233826995 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 145/1000 Loss: 0.0010750314686447382 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 146/1000 Loss: 0.0010659719118848443 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 147/1000 Loss: 0.0010570744052529335 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 148/1000 Loss: 0.0010483079822733998 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 149/1000 Loss: 0.0010396962752565742 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 150/1000 Loss: 0.0010312270605936646 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 151/1000 Loss: 0.0010228774044662714 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 152/1000 Loss: 0.0010146753629669547 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 153/1000 Loss: 0.0010066033573821187 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 154/1000 Loss: 0.0009986720979213715 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 155/1000 Loss: 0.0009909032378345728 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 156/1000 Loss: 0.0009832530049607158 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 157/1000 Loss: 0.0009757253574207425 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 158/1000 Loss: 0.0009683065582066774 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 159/1000 Loss: 0.0009609992848709226 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 160/1000 Loss: 0.0009538095910102129 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 161/1000 Loss: 0.0009467215859331191 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 162/1000 Loss: 0.0009397309040650725 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 163/1000 Loss: 0.00093284179456532 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 164/1000 Loss: 0.0009260556544177234 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 165/1000 Loss: 0.0009193694568239152 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 166/1000 Loss: 0.0009127720259130001 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 167/1000 Loss: 0.0009062676108442247 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 168/1000 Loss: 0.0008998618577606976 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 169/1000 Loss: 0.0008935323567129672 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 170/1000 Loss: 0.0008872916805557907 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 171/1000 Loss: 0.0008811400039121509 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 172/1000 Loss: 0.0008750644628889859 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 173/1000 Loss: 0.000869069539476186 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 174/1000 Loss: 0.0008631480159237981 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 175/1000 Loss: 0.0008573111845180392 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 176/1000 Loss: 0.0008515522349625826 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 177/1000 Loss: 0.0008458649390377104 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 178/1000 Loss: 0.0008402526145800948 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 179/1000 Loss: 0.0008347093826159835 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 180/1000 Loss: 0.0008292323327623308 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 181/1000 Loss: 0.0008238315349444747 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 182/1000 Loss: 0.00081849709386006 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 183/1000 Loss: 0.0008132219081744552 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 184/1000 Loss: 0.0008080158149823546 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 185/1000 Loss: 0.0008028775919228792 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 186/1000 Loss: 0.000797795772086829 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 187/1000 Loss: 0.0007927732658572495 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 188/1000 Loss: 0.0007878200267441571 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 189/1000 Loss: 0.000782920396886766 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 190/1000 Loss: 0.0007780787418596447 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 191/1000 Loss: 0.0007732879603281617 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 192/1000 Loss: 0.0007685577147640288 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 193/1000 Loss: 0.0007638825918547809 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 194/1000 Loss: 0.0007592696929350495 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 195/1000 Loss: 0.0007546933484263718 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 196/1000 Loss: 0.0007501707295887172 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 197/1000 Loss: 0.0007457102765329182 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 198/1000 Loss: 0.0007412905688397586 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 199/1000 Loss: 0.0007369232480414212 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 200/1000 Loss: 0.0007326152990572155 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 201/1000 Loss: 0.0007283369195647538 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 202/1000 Loss: 0.0007241151179187 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 203/1000 Loss: 0.0007199498359113932 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 204/1000 Loss: 0.0007158127264119685 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 205/1000 Loss: 0.000711727945599705 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 206/1000 Loss: 0.0007076783804222941 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 207/1000 Loss: 0.0007036896422505379 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 208/1000 Loss: 0.0006997376913204789 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 209/1000 Loss: 0.0006958180456422269 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 210/1000 Loss: 0.0006919436855241656 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 211/1000 Loss: 0.0006881101871840656 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 212/1000 Loss: 0.0006843291921541095 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 213/1000 Loss: 0.000680569268297404 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 214/1000 Loss: 0.0006768687162548304 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 215/1000 Loss: 0.0006731835892423987 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 216/1000 Loss: 0.0006695479969494045 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 217/1000 Loss: 0.0006659377831965685 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 218/1000 Loss: 0.0006623729132115841 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 219/1000 Loss: 0.0006588488467969 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 220/1000 Loss: 0.0006553545827046037 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 221/1000 Loss: 0.0006518984446302056 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 222/1000 Loss: 0.0006484747864305973 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 223/1000 Loss: 0.0006450822111219168 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 224/1000 Loss: 0.0006417250260710716 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 225/1000 Loss: 0.0006383974105119705 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 226/1000 Loss: 0.0006351108313538134 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 227/1000 Loss: 0.0006318468367680907 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 228/1000 Loss: 0.0006286153802648187 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 229/1000 Loss: 0.0006254178588278592 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 230/1000 Loss: 0.0006222499068826437 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 231/1000 Loss: 0.000619110360275954 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 232/1000 Loss: 0.0006159975309856236 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 233/1000 Loss: 0.0006129174144007266 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 234/1000 Loss: 0.0006098654121160507 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 235/1000 Loss: 0.0006068501970730722 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 236/1000 Loss: 0.0006038618157617748 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 237/1000 Loss: 0.0006008946802467108 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 238/1000 Loss: 0.0005979501293040812 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 239/1000 Loss: 0.0005950395134277642 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 240/1000 Loss: 0.0005921543925069273 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 241/1000 Loss: 0.0005893003544770181 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 242/1000 Loss: 0.0005864574923180044 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 243/1000 Loss: 0.0005836571799591184 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 244/1000 Loss: 0.0005808795103803277 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 245/1000 Loss: 0.0005781187210232019 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 246/1000 Loss: 0.0005753933801315725 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 247/1000 Loss: 0.0005726792733184993 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 248/1000 Loss: 0.0005699991015717387 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 249/1000 Loss: 0.0005673401174135506 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 250/1000 Loss: 0.0005647010402753949 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 251/1000 Loss: 0.0005620901356451213 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 252/1000 Loss: 0.0005594862159341574 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 253/1000 Loss: 0.0005569163477048278 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 254/1000 Loss: 0.0005543748266063631 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 255/1000 Loss: 0.0005518487887457013 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 256/1000 Loss: 0.0005493396893143654 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 257/1000 Loss: 0.0005468631279654801 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 258/1000 Loss: 0.000544396520126611 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 259/1000 Loss: 0.000541956746019423 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 260/1000 Loss: 0.0005395367043092847 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 261/1000 Loss: 0.0005371266161091626 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 262/1000 Loss: 0.000534750462975353 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 263/1000 Loss: 0.0005323956138454378 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 264/1000 Loss: 0.0005300434422679245 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 265/1000 Loss: 0.0005277267773635685 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 266/1000 Loss: 0.0005254256539046764 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 267/1000 Loss: 0.0005231342511251569 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 268/1000 Loss: 0.0005208698567003012 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 269/1000 Loss: 0.0005186238558962941 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 270/1000 Loss: 0.000516393396537751 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 271/1000 Loss: 0.0005141827277839184 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 272/1000 Loss: 0.0005119919078424573 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 273/1000 Loss: 0.0005098109249956906 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 274/1000 Loss: 0.0005076483357697725 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 275/1000 Loss: 0.0005055069923400879 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 276/1000 Loss: 0.0005033897468820214 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 277/1000 Loss: 0.0005012823385186493 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 278/1000 Loss: 0.0004991848836652935 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 279/1000 Loss: 0.0004971086164005101 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 280/1000 Loss: 0.0004950421280227602 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 281/1000 Loss: 0.0004929983988404274 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 282/1000 Loss: 0.0004909730632789433 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 283/1000 Loss: 0.0004889647243544459 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 284/1000 Loss: 0.0004869676777161658 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 285/1000 Loss: 0.00048498480464331806 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 286/1000 Loss: 0.0004830187826883048 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 287/1000 Loss: 0.00048105992027558386 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 288/1000 Loss: 0.0004791251558344811 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 289/1000 Loss: 0.00047721012379042804 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 290/1000 Loss: 0.0004752994573209435 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 291/1000 Loss: 0.00047340276069007814 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 292/1000 Loss: 0.00047153030754998326 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 293/1000 Loss: 0.0004696604737546295 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 294/1000 Loss: 0.0004678120603784919 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 295/1000 Loss: 0.0004659776750486344 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 296/1000 Loss: 0.00046415883116424084 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 297/1000 Loss: 0.0004623429267667234 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 298/1000 Loss: 0.0004605510621331632 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 299/1000 Loss: 0.00045876906369812787 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 300/1000 Loss: 0.00045699550537392497 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 301/1000 Loss: 0.00045524188317358494 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 302/1000 Loss: 0.00045349381980486214 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 303/1000 Loss: 0.0004517656343523413 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 304/1000 Loss: 0.0004500430077314377 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 305/1000 Loss: 0.0004483402881305665 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 306/1000 Loss: 0.00044664606684818864 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 307/1000 Loss: 0.000444963137852028 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 308/1000 Loss: 0.0004432915011420846 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 309/1000 Loss: 0.00044162978883832693 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 310/1000 Loss: 0.0004399864992592484 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 311/1000 Loss: 0.0004383531049825251 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 312/1000 Loss: 0.0004367267247289419 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 313/1000 Loss: 0.00043511323747225106 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 314/1000 Loss: 0.0004335080739110708 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 315/1000 Loss: 0.00043191428994759917 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 316/1000 Loss: 0.0004303389578126371 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 317/1000 Loss: 0.0004287692136131227 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 318/1000 Loss: 0.0004272079677321017 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 319/1000 Loss: 0.00042566083720885217 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 320/1000 Loss: 0.0004241280257701874 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 321/1000 Loss: 0.0004225921875331551 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 322/1000 Loss: 0.00042107346234843135 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 323/1000 Loss: 0.0004195702786091715 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 324/1000 Loss: 0.0004180770483799279 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 325/1000 Loss: 0.0004165865248069167 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 326/1000 Loss: 0.0004151116299908608 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 327/1000 Loss: 0.0004136409843340516 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 328/1000 Loss: 0.0004121859383303672 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 329/1000 Loss: 0.00041074358159676194 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 330/1000 Loss: 0.0004093027382623404 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 331/1000 Loss: 0.0004078744968865067 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 332/1000 Loss: 0.00040645344415679574 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 333/1000 Loss: 0.0004050421994179487 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 334/1000 Loss: 0.0004036423924844712 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 335/1000 Loss: 0.0004022453213110566 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 336/1000 Loss: 0.00040086242370307446 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 337/1000 Loss: 0.00039948380435816944 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 338/1000 Loss: 0.0003981292829848826 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 339/1000 Loss: 0.0003967689990531653 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 340/1000 Loss: 0.00039542862214148045 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 341/1000 Loss: 0.00039408536395058036 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 342/1000 Loss: 0.0003927577054128051 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 343/1000 Loss: 0.00039142719469964504 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 344/1000 Loss: 0.0003901193558704108 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 345/1000 Loss: 0.000388818618375808 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 346/1000 Loss: 0.00038752207183279097 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 347/1000 Loss: 0.00038623405271209776 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 348/1000 Loss: 0.00038494463660754263 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 349/1000 Loss: 0.0003836792311631143 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 350/1000 Loss: 0.0003824109735433012 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 351/1000 Loss: 0.00038115415372885764 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 352/1000 Loss: 0.00037990714190527797 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 353/1000 Loss: 0.0003786601882893592 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 354/1000 Loss: 0.00037742307176813483 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 355/1000 Loss: 0.0003761973639484495 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 356/1000 Loss: 0.000374974450096488 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 357/1000 Loss: 0.00037376716500148177 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 358/1000 Loss: 0.00037256264477036893 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 359/1000 Loss: 0.00037136944592930377 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 360/1000 Loss: 0.0003701664099935442 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 361/1000 Loss: 0.00036898881080560386 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 362/1000 Loss: 0.0003678168577607721 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 363/1000 Loss: 0.0003666435368359089 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 364/1000 Loss: 0.0003654843312688172 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 365/1000 Loss: 0.00036433080094866455 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 366/1000 Loss: 0.00036317729973234236 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 367/1000 Loss: 0.0003620393399614841 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 368/1000 Loss: 0.00036089992499910295 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 369/1000 Loss: 0.0003597747127059847 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 370/1000 Loss: 0.0003586566308513284 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 371/1000 Loss: 0.00035753281554207206 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 372/1000 Loss: 0.00035643024602904916 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 373/1000 Loss: 0.0003553234273567796 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 374/1000 Loss: 0.00035422941436991096 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 375/1000 Loss: 0.0003531424736138433 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 376/1000 Loss: 0.0003520540485624224 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 377/1000 Loss: 0.0003509783709887415 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 378/1000 Loss: 0.0003499070298857987 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 379/1000 Loss: 0.00034884977503679693 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 380/1000 Loss: 0.0003477882710285485 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 381/1000 Loss: 0.00034672676702030003 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 382/1000 Loss: 0.00034568089176900685 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 383/1000 Loss: 0.00034463920746929944 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 384/1000 Loss: 0.00034360322752036154 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 385/1000 Loss: 0.0003425714385230094 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 386/1000 Loss: 0.0003415439568925649 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 387/1000 Loss: 0.00034052919363602996 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 388/1000 Loss: 0.0003395215608179569 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 389/1000 Loss: 0.0003385110176168382 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 390/1000 Loss: 0.00033750192960724235 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 391/1000 Loss: 0.00033651693956926465 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 392/1000 Loss: 0.0003355319204274565 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 393/1000 Loss: 0.00033455254742875695 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 394/1000 Loss: 0.00033357186475768685 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 395/1000 Loss: 0.0003326066071167588 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 396/1000 Loss: 0.00033163008629344404 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 397/1000 Loss: 0.00033067341428250074 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 398/1000 Loss: 0.00032970678876154125 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 399/1000 Loss: 0.00032875718898139894 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 400/1000 Loss: 0.00032781463232822716 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 401/1000 Loss: 0.00032687210477888584 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 402/1000 Loss: 0.00032593950163573027 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 403/1000 Loss: 0.0003250125446356833 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 404/1000 Loss: 0.00032408139668405056 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 405/1000 Loss: 0.00032316154101863503 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 406/1000 Loss: 0.0003222416853532195 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 407/1000 Loss: 0.00032133315107785165 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 408/1000 Loss: 0.0003204203676432371 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 409/1000 Loss: 0.0003195274621248245 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 410/1000 Loss: 0.0003186259709764272 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 411/1000 Loss: 0.0003177344333380461 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 412/1000 Loss: 0.00031685279100202024 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 413/1000 Loss: 0.00031596978078596294 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 414/1000 Loss: 0.0003150895645376295 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 415/1000 Loss: 0.00031421499443240464 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 416/1000 Loss: 0.00031333763035945594 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 417/1000 Loss: 0.0003124758368358016 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 418/1000 Loss: 0.00031161829247139394 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 419/1000 Loss: 0.0003107635711785406 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 420/1000 Loss: 0.00030990602681413293 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 421/1000 Loss: 0.0003090612299274653 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 422/1000 Loss: 0.00030822213739156723 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 423/1000 Loss: 0.0003073844127357006 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 424/1000 Loss: 0.0003065424389205873 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 425/1000 Loss: 0.0003057203139178455 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 426/1000 Loss: 0.0003048911166843027 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 427/1000 Loss: 0.0003040746378246695 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 428/1000 Loss: 0.00030325251282192767 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 429/1000 Loss: 0.0003024417092092335 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 430/1000 Loss: 0.000301629479508847 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 431/1000 Loss: 0.00030081867589615285 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 432/1000 Loss: 0.0003000220749527216 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 433/1000 Loss: 0.0002992254158016294 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 434/1000 Loss: 0.00029842875665053725 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 435/1000 Loss: 0.0002976491523440927 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 436/1000 Loss: 0.00029685249319300056 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 437/1000 Loss: 0.00029607288888655603 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 438/1000 Loss: 0.0002952974755316973 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 439/1000 Loss: 0.0002945192391052842 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 440/1000 Loss: 0.0002937524113804102 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 441/1000 Loss: 0.00029299542075023055 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 442/1000 Loss: 0.000292224227450788 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 443/1000 Loss: 0.0002914686920121312 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 444/1000 Loss: 0.00029071312746964395 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 445/1000 Loss: 0.00028996466426178813 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 446/1000 Loss: 0.00028921477496623993 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 447/1000 Loss: 0.00028847623616456985 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 448/1000 Loss: 0.00028774194652214646 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 449/1000 Loss: 0.0002870034077204764 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 450/1000 Loss: 0.0002862619876395911 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 451/1000 Loss: 0.00028553191805258393 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 452/1000 Loss: 0.0002848118601832539 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 453/1000 Loss: 0.0002840902889147401 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 454/1000 Loss: 0.00028336592367850244 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 455/1000 Loss: 0.0002826471463777125 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 456/1000 Loss: 0.00028193980688229203 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 457/1000 Loss: 0.0002812267339322716 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 458/1000 Loss: 0.0002805278927553445 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 459/1000 Loss: 0.0002798247442115098 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 460/1000 Loss: 0.0002791272709146142 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 461/1000 Loss: 0.0002784269454423338 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 462/1000 Loss: 0.0002777294721454382 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 463/1000 Loss: 0.00027704049716703594 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 464/1000 Loss: 0.00027635859441943467 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 465/1000 Loss: 0.00027567255892790854 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 466/1000 Loss: 0.00027499484713189304 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 467/1000 Loss: 0.0002743214718066156 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 468/1000 Loss: 0.0002736495225690305 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 469/1000 Loss: 0.0002729761181399226 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 470/1000 Loss: 0.00027231124113313854 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 471/1000 Loss: 0.00027164636412635446 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 472/1000 Loss: 0.0002709871914703399 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 473/1000 Loss: 0.00027032510843127966 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 474/1000 Loss: 0.0002696616284083575 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 475/1000 Loss: 0.00026901945238932967 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 476/1000 Loss: 0.0002683644706849009 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 477/1000 Loss: 0.00026772223645821214 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 478/1000 Loss: 0.00026708428049460053 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 479/1000 Loss: 0.00026643642922863364 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 480/1000 Loss: 0.00026579847326502204 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 481/1000 Loss: 0.0002651647664606571 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 482/1000 Loss: 0.0002645310596562922 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 483/1000 Loss: 0.0002639058220665902 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 484/1000 Loss: 0.000263282039668411 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 485/1000 Loss: 0.0002626525820232928 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 486/1000 Loss: 0.00026203447487205267 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 487/1000 Loss: 0.00026141206035390496 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 488/1000 Loss: 0.00026079252711497247 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 489/1000 Loss: 0.00026018288917839527 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 490/1000 Loss: 0.00025957467732951045 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 491/1000 Loss: 0.0002589664945844561 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 492/1000 Loss: 0.000258361134910956 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 493/1000 Loss: 0.00025775007088668644 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 494/1000 Loss: 0.00025715745869092643 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 495/1000 Loss: 0.0002565548929851502 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 496/1000 Loss: 0.00025596795603632927 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 497/1000 Loss: 0.000255378196015954 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 498/1000 Loss: 0.0002547897747717798 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 499/1000 Loss: 0.00025419576559215784 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 500/1000 Loss: 0.0002536130486987531 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 501/1000 Loss: 0.00025303461006842554 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 502/1000 Loss: 0.00025245617143809795 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 503/1000 Loss: 0.00025187060236930847 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 504/1000 Loss: 0.0002513006911613047 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 505/1000 Loss: 0.00025073072174564004 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 506/1000 Loss: 0.00025015792925842106 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 507/1000 Loss: 0.0002495936641935259 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 508/1000 Loss: 0.0002490251208655536 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 509/1000 Loss: 0.00024845654843375087 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 510/1000 Loss: 0.0002479050599504262 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 511/1000 Loss: 0.00024734786711633205 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 512/1000 Loss: 0.0002467934973537922 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 513/1000 Loss: 0.00024623770150355995 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 514/1000 Loss: 0.0002456876100040972 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 515/1000 Loss: 0.00024513466632924974 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 516/1000 Loss: 0.00024458736879751086 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 517/1000 Loss: 0.00024404434952884912 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 518/1000 Loss: 0.00024350134481210262 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 519/1000 Loss: 0.00024296682386193424 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 520/1000 Loss: 0.00024242233484983444 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 521/1000 Loss: 0.0002418934745946899 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 522/1000 Loss: 0.00024136182037182152 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 523/1000 Loss: 0.00024082728486973792 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 524/1000 Loss: 0.00024029985070228577 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 525/1000 Loss: 0.00023977385717444122 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 526/1000 Loss: 0.00023925062851049006 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 527/1000 Loss: 0.00023872747260611504 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 528/1000 Loss: 0.0002382071252213791 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 529/1000 Loss: 0.00023769524705130607 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 530/1000 Loss: 0.00023717207659501582 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 531/1000 Loss: 0.00023666024208068848 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 532/1000 Loss: 0.00023615124518983066 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 533/1000 Loss: 0.0002356393524678424 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 534/1000 Loss: 0.00023513741325587034 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 535/1000 Loss: 0.00023463550314772874 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 536/1000 Loss: 0.00023413359303958714 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 537/1000 Loss: 0.00023361317289527506 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 538/1000 Loss: 0.00023312689154408872 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 539/1000 Loss: 0.00023262918693944812 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 540/1000 Loss: 0.0002321385982213542 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 541/1000 Loss: 0.00023163524747360498 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 542/1000 Loss: 0.0002311488933628425 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 543/1000 Loss: 0.0002306597598362714 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 544/1000 Loss: 0.00023017053899820894 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 545/1000 Loss: 0.00022968988923821598 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 546/1000 Loss: 0.00022919784532859921 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 547/1000 Loss: 0.0002287200331920758 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 548/1000 Loss: 0.00022823933977633715 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 549/1000 Loss: 0.00022776720288675278 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 550/1000 Loss: 0.00022728652402292937 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 551/1000 Loss: 0.00022681434347759932 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 552/1000 Loss: 0.00022634220658801496 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 553/1000 Loss: 0.00022586717386730015 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 554/1000 Loss: 0.00022540208010468632 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 555/1000 Loss: 0.00022493841242976487 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 556/1000 Loss: 0.00022446626098826528 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 557/1000 Loss: 0.00022400829766411334 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 558/1000 Loss: 0.00022354318934958428 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 559/1000 Loss: 0.00022307952167466283 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 560/1000 Loss: 0.0002226257638540119 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 561/1000 Loss: 0.00022216352226678282 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 562/1000 Loss: 0.00022171686578076333 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 563/1000 Loss: 0.00022126312251202762 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 564/1000 Loss: 0.0002208065561717376 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 565/1000 Loss: 0.00022035989968571812 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 566/1000 Loss: 0.00021990046661812812 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 567/1000 Loss: 0.00021945663320366293 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 568/1000 Loss: 0.00021902417938690633 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 569/1000 Loss: 0.00021857887622900307 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 570/1000 Loss: 0.00021813508647028357 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 571/1000 Loss: 0.00021769407612737268 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 572/1000 Loss: 0.00021726160775870085 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 573/1000 Loss: 0.0002168276987504214 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 574/1000 Loss: 0.0002163952449336648 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 575/1000 Loss: 0.00021594999998342246 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 576/1000 Loss: 0.00021551325335167348 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 577/1000 Loss: 0.00021509068028535694 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 578/1000 Loss: 0.00021466244652401656 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 579/1000 Loss: 0.00021423988800961524 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 580/1000 Loss: 0.00021380312682595104 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 581/1000 Loss: 0.0002133834350388497 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 582/1000 Loss: 0.00021296650811564177 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 583/1000 Loss: 0.00021254393504932523 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 584/1000 Loss: 0.00021212564024608582 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 585/1000 Loss: 0.00021170875697862357 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 586/1000 Loss: 0.00021129327069502324 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 587/1000 Loss: 0.00021088063658680767 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 588/1000 Loss: 0.00021046657639089972 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 589/1000 Loss: 0.00021005675080232322 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 590/1000 Loss: 0.00020963698625564575 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 591/1000 Loss: 0.00020923714328091592 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 592/1000 Loss: 0.00020883016986772418 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 593/1000 Loss: 0.00020842600497417152 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 594/1000 Loss: 0.00020801617938559502 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 595/1000 Loss: 0.0002076219825539738 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 596/1000 Loss: 0.00020721643522847444 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 597/1000 Loss: 0.00020682082686107606 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 598/1000 Loss: 0.0002064223517663777 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 599/1000 Loss: 0.00020602245058398694 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 600/1000 Loss: 0.0002056296361843124 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 601/1000 Loss: 0.00020523399871308357 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 602/1000 Loss: 0.00020484262495301664 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 603/1000 Loss: 0.00020444414985831827 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 604/1000 Loss: 0.00020406128896865994 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 605/1000 Loss: 0.00020366988610476255 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 606/1000 Loss: 0.00020328417303971946 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 607/1000 Loss: 0.00020289844542276114 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 608/1000 Loss: 0.0002025155845331028 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 609/1000 Loss: 0.00020213272364344448 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 610/1000 Loss: 0.00020174842211417854 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 611/1000 Loss: 0.00020136695820838213 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 612/1000 Loss: 0.00020099968241993338 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 613/1000 Loss: 0.0002006167924264446 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 614/1000 Loss: 0.0002002395922318101 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 615/1000 Loss: 0.0001998652151087299 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 616/1000 Loss: 0.00019949226407334208 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 617/1000 Loss: 0.00019912075367756188 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 618/1000 Loss: 0.00019874778809025884 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 619/1000 Loss: 0.0001983776455745101 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 620/1000 Loss: 0.00019801461894530803 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 621/1000 Loss: 0.00019764450553338975 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 622/1000 Loss: 0.00019727296603377908 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 623/1000 Loss: 0.0001969099248526618 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 624/1000 Loss: 0.00019654830975923687 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 625/1000 Loss: 0.00019618526857811958 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 626/1000 Loss: 0.00019582652021199465 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 627/1000 Loss: 0.00019547052215784788 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 628/1000 Loss: 0.00019510751008056104 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 629/1000 Loss: 0.0001947501441463828 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 630/1000 Loss: 0.0001944013056345284 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 631/1000 Loss: 0.0001940411311807111 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 632/1000 Loss: 0.00019368944049347192 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 633/1000 Loss: 0.00019334196986164898 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 634/1000 Loss: 0.00019298460392747074 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 635/1000 Loss: 0.00019263573631178588 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 636/1000 Loss: 0.000192289735423401 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 637/1000 Loss: 0.00019193236948922276 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 638/1000 Loss: 0.00019159626390319318 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 639/1000 Loss: 0.00019124454411212355 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 640/1000 Loss: 0.00019090987916570157 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 641/1000 Loss: 0.0001905624521896243 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 642/1000 Loss: 0.00019022492051590234 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 643/1000 Loss: 0.00018988030205946416 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 644/1000 Loss: 0.00018954281404148787 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 645/1000 Loss: 0.00018920669390354306 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 646/1000 Loss: 0.0001888635306386277 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 647/1000 Loss: 0.0001885344972833991 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 648/1000 Loss: 0.0001881898642750457 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 649/1000 Loss: 0.00018785805150400847 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 650/1000 Loss: 0.0001875304733403027 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 651/1000 Loss: 0.00018719719082582742 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 652/1000 Loss: 0.00018686818657442927 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 653/1000 Loss: 0.00018652925791684538 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 654/1000 Loss: 0.0001862087519839406 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 655/1000 Loss: 0.00018589108367450535 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 656/1000 Loss: 0.00018555924179963768 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 657/1000 Loss: 0.0001852330460678786 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 658/1000 Loss: 0.00018489977810531855 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 659/1000 Loss: 0.00018457927217241377 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 660/1000 Loss: 0.00018426727910991758 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 661/1000 Loss: 0.0001839411270339042 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 662/1000 Loss: 0.00018362057744525373 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 663/1000 Loss: 0.00018330008606426418 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 664/1000 Loss: 0.00018298241775482893 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 665/1000 Loss: 0.00018266474944539368 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 666/1000 Loss: 0.00018234990420751274 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 667/1000 Loss: 0.00018203647050540894 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 668/1000 Loss: 0.00018172734417021275 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 669/1000 Loss: 0.00018141108739655465 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 670/1000 Loss: 0.00018110474047716707 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 671/1000 Loss: 0.0001807913213269785 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 672/1000 Loss: 0.00018048216588795185 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 673/1000 Loss: 0.0001801730104489252 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 674/1000 Loss: 0.00017985956219490618 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 675/1000 Loss: 0.00017955891962628812 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 676/1000 Loss: 0.00017925399879459292 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 677/1000 Loss: 0.00017894909251481295 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 678/1000 Loss: 0.00017864418623503298 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 679/1000 Loss: 0.00017833359015639871 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 680/1000 Loss: 0.00017803578521125019 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 681/1000 Loss: 0.000177730864379555 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 682/1000 Loss: 0.00017743586795404553 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 683/1000 Loss: 0.0001771323586581275 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 684/1000 Loss: 0.0001768359652487561 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 685/1000 Loss: 0.00017654240946285427 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 686/1000 Loss: 0.00017624882457312196 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 687/1000 Loss: 0.00017595384269952774 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 688/1000 Loss: 0.0001756588462740183 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 689/1000 Loss: 0.000175358189153485 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 690/1000 Loss: 0.00017507313168607652 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 691/1000 Loss: 0.00017477813526056707 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 692/1000 Loss: 0.00017449450388085097 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 693/1000 Loss: 0.00017419664072804153 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 694/1000 Loss: 0.00017390874563716352 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 695/1000 Loss: 0.00017361943901050836 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 696/1000 Loss: 0.00017333011783193797 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 697/1000 Loss: 0.00017304647190030664 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 698/1000 Loss: 0.0001727642520563677 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 699/1000 Loss: 0.00017247494542971253 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 700/1000 Loss: 0.00017219127039425075 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 701/1000 Loss: 0.00017190906510222703 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 702/1000 Loss: 0.00017162681615445763 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 703/1000 Loss: 0.0001713474339339882 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 704/1000 Loss: 0.0001710666110739112 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 705/1000 Loss: 0.00017079005192499608 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 706/1000 Loss: 0.00017051209579221904 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 707/1000 Loss: 0.00017023266991600394 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 708/1000 Loss: 0.00016995187615975738 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 709/1000 Loss: 0.00016968382988125086 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 710/1000 Loss: 0.0001694086822681129 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 711/1000 Loss: 0.00016912927094381303 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 712/1000 Loss: 0.00016885556397028267 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 713/1000 Loss: 0.000168586106155999 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 714/1000 Loss: 0.00016832372057251632 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 715/1000 Loss: 0.00016803581092972308 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 716/1000 Loss: 0.00016777627752162516 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 717/1000 Loss: 0.00016750252689234912 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 718/1000 Loss: 0.00016724441957194358 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 719/1000 Loss: 0.00016696644888725132 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 720/1000 Loss: 0.0001667012256802991 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 721/1000 Loss: 0.0001664402661845088 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 722/1000 Loss: 0.00016616795619484037 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 723/1000 Loss: 0.00016590274753980339 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 724/1000 Loss: 0.00016564034740440547 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 725/1000 Loss: 0.00016537656483706087 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 726/1000 Loss: 0.00016511560534127057 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 727/1000 Loss: 0.00016484611842315644 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 728/1000 Loss: 0.00016459792095702142 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 729/1000 Loss: 0.00016432275879196823 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 730/1000 Loss: 0.00016407883958891034 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 731/1000 Loss: 0.00016381076420657337 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 732/1000 Loss: 0.00016355549450963736 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 733/1000 Loss: 0.0001633030187804252 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 734/1000 Loss: 0.00016304489690810442 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 735/1000 Loss: 0.00016278109978884459 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 736/1000 Loss: 0.00016253715148195624 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 737/1000 Loss: 0.00016228329332079738 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 738/1000 Loss: 0.00016202514234464616 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 739/1000 Loss: 0.00016177553334273398 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 740/1000 Loss: 0.0001615287474123761 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 741/1000 Loss: 0.0001612677879165858 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 742/1000 Loss: 0.00016101814981084317 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 743/1000 Loss: 0.0001607728045200929 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 744/1000 Loss: 0.00016052459250204265 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 745/1000 Loss: 0.00016026929370127618 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 746/1000 Loss: 0.00016003528435248882 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 747/1000 Loss: 0.00015977572184056044 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 748/1000 Loss: 0.00015953462570905685 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 749/1000 Loss: 0.0001592807238921523 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 750/1000 Loss: 0.00015904389147181064 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 751/1000 Loss: 0.00015880278078839183 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 752/1000 Loss: 0.0001585616555530578 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 753/1000 Loss: 0.00015831770724616945 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 754/1000 Loss: 0.00015807517047505826 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 755/1000 Loss: 0.00015783548587933183 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 756/1000 Loss: 0.00015759152302052826 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 757/1000 Loss: 0.0001573560875840485 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 758/1000 Loss: 0.00015711638843640685 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 759/1000 Loss: 0.00015687810082454234 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 760/1000 Loss: 0.00015664125385228544 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 761/1000 Loss: 0.00015640721539966762 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 762/1000 Loss: 0.00015615759184584022 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 763/1000 Loss: 0.0001559306838316843 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 764/1000 Loss: 0.00015569098468404263 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 765/1000 Loss: 0.00015545978385489434 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 766/1000 Loss: 0.0001552229223307222 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 767/1000 Loss: 0.000154990324517712 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 768/1000 Loss: 0.00015476338739972562 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 769/1000 Loss: 0.0001545293489471078 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 770/1000 Loss: 0.0001543038379168138 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 771/1000 Loss: 0.00015406272723339498 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 772/1000 Loss: 0.00015384287689812481 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 773/1000 Loss: 0.00015360744146164507 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 774/1000 Loss: 0.00015338475350290537 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 775/1000 Loss: 0.00015315071505028754 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 776/1000 Loss: 0.00015292095486074686 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 777/1000 Loss: 0.00015269969298969954 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 778/1000 Loss: 0.00015247418195940554 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 779/1000 Loss: 0.0001522443926660344 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 780/1000 Loss: 0.00015202740905806422 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 781/1000 Loss: 0.0001517961936770007 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 782/1000 Loss: 0.00015157635789364576 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 783/1000 Loss: 0.00015134515706449747 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 784/1000 Loss: 0.00015112390974536538 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 785/1000 Loss: 0.00015090263332240283 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 786/1000 Loss: 0.00015068420907482505 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 787/1000 Loss: 0.00015046009502839297 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 788/1000 Loss: 0.0001502416853327304 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 789/1000 Loss: 0.00015002752479631454 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 790/1000 Loss: 0.000149806248373352 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 791/1000 Loss: 0.00014958639803808182 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 792/1000 Loss: 0.00014936372463125736 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 793/1000 Loss: 0.00014915663632564247 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 794/1000 Loss: 0.0001489410497015342 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 795/1000 Loss: 0.00014871552411932498 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 796/1000 Loss: 0.00014850420120637864 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 797/1000 Loss: 0.0001482900115661323 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 798/1000 Loss: 0.00014807442494202405 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 799/1000 Loss: 0.0001478616613894701 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 800/1000 Loss: 0.000147647486301139 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 801/1000 Loss: 0.00014743614883627743 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 802/1000 Loss: 0.0001472233998356387 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 803/1000 Loss: 0.0001470163115300238 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 804/1000 Loss: 0.00014680640015285462 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 805/1000 Loss: 0.00014659504813607782 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 806/1000 Loss: 0.0001463893859181553 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 807/1000 Loss: 0.00014616955013480037 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 808/1000 Loss: 0.00014596953405998647 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 809/1000 Loss: 0.00014575962268281728 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 810/1000 Loss: 0.00014555394591297954 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 811/1000 Loss: 0.00014534687215927988 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 812/1000 Loss: 0.00014514119538944215 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 813/1000 Loss: 0.00014493837079498917 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 814/1000 Loss: 0.00014473128248937428 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 815/1000 Loss: 0.00014452279719989747 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 816/1000 Loss: 0.00014431712043005973 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 817/1000 Loss: 0.00014411003212444484 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 818/1000 Loss: 0.00014391855802387 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 819/1000 Loss: 0.00014371288125403225 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 820/1000 Loss: 0.00014350579294841737 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 821/1000 Loss: 0.00014330721751321107 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 822/1000 Loss: 0.00014311008271761239 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 823/1000 Loss: 0.00014290583203546703 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 824/1000 Loss: 0.00014271007967181504 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 825/1000 Loss: 0.00014250582898966968 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 826/1000 Loss: 0.00014230584201868623 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 827/1000 Loss: 0.00014210726658347994 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 828/1000 Loss: 0.00014191435184329748 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 829/1000 Loss: 0.00014172287774272263 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 830/1000 Loss: 0.0001415172009728849 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 831/1000 Loss: 0.0001413243153365329 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 832/1000 Loss: 0.00014112003555055708 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 833/1000 Loss: 0.0001409299875376746 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 834/1000 Loss: 0.00014072998601477593 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 835/1000 Loss: 0.00014053707127459347 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 836/1000 Loss: 0.0001403498463332653 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 837/1000 Loss: 0.00014014841872267425 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 838/1000 Loss: 0.0001399597676936537 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 839/1000 Loss: 0.00013977396884001791 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 840/1000 Loss: 0.00013957962801214308 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 841/1000 Loss: 0.00013938390475232154 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 842/1000 Loss: 0.00013920375204179436 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 843/1000 Loss: 0.00013900517660658807 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 844/1000 Loss: 0.0001388179516652599 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 845/1000 Loss: 0.00013863074127584696 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 846/1000 Loss: 0.0001384406496072188 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 847/1000 Loss: 0.00013825057249050587 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 848/1000 Loss: 0.00013806761126033962 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 849/1000 Loss: 0.00013787327043246478 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 850/1000 Loss: 0.0001376902946503833 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 851/1000 Loss: 0.00013751017104368657 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 852/1000 Loss: 0.00013732009392697364 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 853/1000 Loss: 0.00013713569205719978 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 854/1000 Loss: 0.00013694845256395638 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 855/1000 Loss: 0.00013675980153493583 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 856/1000 Loss: 0.00013658251555170864 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 857/1000 Loss: 0.00013639384997077286 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 858/1000 Loss: 0.000136209448100999 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 859/1000 Loss: 0.0001360278984066099 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 860/1000 Loss: 0.00013584208500105888 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 861/1000 Loss: 0.00013565625704359263 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 862/1000 Loss: 0.0001354789565084502 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 863/1000 Loss: 0.00013530308206100017 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 864/1000 Loss: 0.0001351186801912263 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 865/1000 Loss: 0.0001349356898572296 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 866/1000 Loss: 0.0001347612269455567 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 867/1000 Loss: 0.00013458248577080667 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 868/1000 Loss: 0.00013439952454064041 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 869/1000 Loss: 0.0001342250470770523 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 870/1000 Loss: 0.0001340434973826632 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 871/1000 Loss: 0.00013386191858444363 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 872/1000 Loss: 0.00013369170483201742 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 873/1000 Loss: 0.00013351581583265215 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 874/1000 Loss: 0.00013333852984942496 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 875/1000 Loss: 0.00013316405238583684 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 876/1000 Loss: 0.00013298957492224872 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 877/1000 Loss: 0.00013280802522785962 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 878/1000 Loss: 0.00013263353321235627 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 879/1000 Loss: 0.00013245623267721385 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 880/1000 Loss: 0.00013229026808403432 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 881/1000 Loss: 0.00013211581972427666 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 882/1000 Loss: 0.0001319512666668743 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 883/1000 Loss: 0.00013177112850826234 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 884/1000 Loss: 0.00013160088565200567 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 885/1000 Loss: 0.0001314236142206937 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 886/1000 Loss: 0.00013125763507559896 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 887/1000 Loss: 0.00013108458369970322 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 888/1000 Loss: 0.00013091294385958463 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 889/1000 Loss: 0.00013074840535409749 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 890/1000 Loss: 0.00013057960313744843 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 891/1000 Loss: 0.00013040797784924507 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 892/1000 Loss: 0.00013024201325606555 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 893/1000 Loss: 0.00013007746019866318 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 894/1000 Loss: 0.00012990583491045982 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 895/1000 Loss: 0.0001297313574468717 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 896/1000 Loss: 0.00012957390572410077 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 897/1000 Loss: 0.00012940794113092124 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 898/1000 Loss: 0.0001292391389142722 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 899/1000 Loss: 0.00012907035124953836 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 900/1000 Loss: 0.00012891007645521313 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 901/1000 Loss: 0.0001287483610212803 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 902/1000 Loss: 0.0001285894977627322 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 903/1000 Loss: 0.00012841785792261362 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 904/1000 Loss: 0.00012825473095290363 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 905/1000 Loss: 0.00012809019244741648 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 906/1000 Loss: 0.00012792990310117602 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 907/1000 Loss: 0.00012776108633261174 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 908/1000 Loss: 0.00012760648678522557 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 909/1000 Loss: 0.00012744050764013082 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 910/1000 Loss: 0.00012728307046927512 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 911/1000 Loss: 0.0001271199289476499 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 912/1000 Loss: 0.0001269568019779399 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 913/1000 Loss: 0.00012679368956014514 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 914/1000 Loss: 0.00012663340021390468 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 915/1000 Loss: 0.0001264745369553566 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 916/1000 Loss: 0.00012631707068067044 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 917/1000 Loss: 0.00012615819287020713 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 918/1000 Loss: 0.00012600926856976002 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 919/1000 Loss: 0.00012584187788888812 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 920/1000 Loss: 0.00012568016245495528 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 921/1000 Loss: 0.00012553122360259295 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 922/1000 Loss: 0.00012537378643173724 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 923/1000 Loss: 0.00012521205644588917 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 924/1000 Loss: 0.00012506029452197254 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 925/1000 Loss: 0.00012490282824728638 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 926/1000 Loss: 0.00012474964023567736 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 927/1000 Loss: 0.00012459360004868358 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 928/1000 Loss: 0.0001244375598616898 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 929/1000 Loss: 0.00012429288472048938 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 930/1000 Loss: 0.00012412408250384033 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 931/1000 Loss: 0.00012397512909956276 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 932/1000 Loss: 0.00012381908891256899 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 933/1000 Loss: 0.00012367016461212188 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 934/1000 Loss: 0.0001235141244251281 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 935/1000 Loss: 0.0001233637594850734 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 936/1000 Loss: 0.00012321198300924152 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 937/1000 Loss: 0.00012306019198149443 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 938/1000 Loss: 0.00012291266466490924 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 939/1000 Loss: 0.0001227651519002393 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 940/1000 Loss: 0.0001226105378009379 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 941/1000 Loss: 0.0001224544976139441 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 942/1000 Loss: 0.00012230697029735893 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 943/1000 Loss: 0.0001221608545165509 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 944/1000 Loss: 0.00012200765922898427 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 945/1000 Loss: 0.00012186438834760338 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 946/1000 Loss: 0.0001217126045958139 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 947/1000 Loss: 0.00012156223965575919 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 948/1000 Loss: 0.00012141472689108923 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 949/1000 Loss: 0.00012126719229854643 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 950/1000 Loss: 0.00012112108379369602 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 951/1000 Loss: 0.00012097213766537607 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 952/1000 Loss: 0.00012082461034879088 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 953/1000 Loss: 0.00012067707575624809 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 954/1000 Loss: 0.00012052814417984337 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 955/1000 Loss: 0.0001203877036459744 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 956/1000 Loss: 0.00012024017632938921 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 957/1000 Loss: 0.00012009266356471926 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 958/1000 Loss: 0.00011995363456662744 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 959/1000 Loss: 0.00011981035640928894 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 960/1000 Loss: 0.00011966284364461899 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 961/1000 Loss: 0.00011951814667554572 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 962/1000 Loss: 0.00011937487579416484 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 963/1000 Loss: 0.00011922876728931442 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 964/1000 Loss: 0.00011908974556718022 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 965/1000 Loss: 0.00011894362978637218 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 966/1000 Loss: 0.00011880462989211082 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 967/1000 Loss: 0.00011866986460518092 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 968/1000 Loss: 0.00011852517491206527 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 969/1000 Loss: 0.00011838188947876915 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 970/1000 Loss: 0.00011824287503259256 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 971/1000 Loss: 0.00011810386786237359 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 972/1000 Loss: 0.00011796060425695032 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 973/1000 Loss: 0.00011782299407059327 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 974/1000 Loss: 0.00011768964759539813 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 975/1000 Loss: 0.00011754637671401724 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 976/1000 Loss: 0.00011740879563149065 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 977/1000 Loss: 0.0001172584161395207 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 978/1000 Loss: 0.00011712507694028318 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 979/1000 Loss: 0.0001169931492768228 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 980/1000 Loss: 0.00011685697973007336 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 981/1000 Loss: 0.0001167165391962044 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 982/1000 Loss: 0.00011658461153274402 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 983/1000 Loss: 0.00011644134065136313 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 984/1000 Loss: 0.00011630657536443323 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 985/1000 Loss: 0.00011616755364229903 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 986/1000 Loss: 0.00011603136954363436 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 987/1000 Loss: 0.00011589661880861968 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 988/1000 Loss: 0.00011576184624573216 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 989/1000 Loss: 0.00011563417501747608 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 990/1000 Loss: 0.00011549517512321472 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 991/1000 Loss: 0.00011535755766090006 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 992/1000 Loss: 0.00011522705608513206 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 993/1000 Loss: 0.000115090879262425 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 994/1000 Loss: 0.00011495469516376033 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 995/1000 Loss: 0.00011481567344162613 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 996/1000 Loss: 0.00011469227320048958 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 997/1000 Loss: 0.00011455892672529444 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 998/1000 Loss: 0.00011442413961049169 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 999/1000 Loss: 0.00011428938887547702 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Epoch 1000/1000 Loss: 0.00011416456254664809 num_feat: 38/38 Reg Loss: 0.0 Val Accuracy: 94.44444444444444\n",
      "Accuracy = 93.33333333333333%\n",
      "93.33333333333333%, 34 features\n"
     ]
    }
   ],
   "source": [
    "import ScikitWrapper as skw\n",
    "import warnings\n",
    "import importlib\n",
    "importlib.reload(skw)\n",
    "X, y = shap.datasets.iris()\n",
    "accs = []\n",
    "total_feature = []\n",
    "warnings.simplefilter(action='ignore')\n",
    "X_train, X_test, Y_train, Y_test = insert_feature_noise(X, y, \n",
    "            num_random_noise=num_normal[-1],\n",
    "            num_overwhelemed=num_overwhelmed[-1], \n",
    "            num_shortcut=num_shortcut[-1])\n",
    "X_train, y_train = X_train.to_numpy().astype(np.float32), Y_train.astype(np.int64)\n",
    "X_test, y_test = X_test.to_numpy().astype(np.float32), Y_test.astype(np.int64)\n",
    "clf = skw.ScikitClfWrapper(\n",
    "        input_dim=X_train.shape[1],\n",
    "        number_of_classes=3,\n",
    "        hidden_dims=(16, 16), lam=0, epochs=1000, sigma=0.5,\n",
    "        freeze_till=1000, lr=0.1, verbose=True,\n",
    "        device='cpu')\n",
    "clf.fit(X_train, Y_train)\n",
    "acc = print_accuracy(clf.predict, X_test, Y_test)\n",
    "total_feature = 2*num_normal[i] + num_overwhelmed[i] + num_shortcut[i]\n",
    "print(f\"{acc}%, {total_feature} features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epoch</th>\n",
       "      <th>valid_acc</th>\n",
       "      <th># feature</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>33.333333</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>33.333333</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>33.333333</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>33.333333</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>33.333333</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   epoch  valid_acc  # feature\n",
       "0      0  33.333333         38\n",
       "1      1  33.333333         38\n",
       "2      2  33.333333         38\n",
       "3      3  33.333333         38\n",
       "4      4  33.333333         38"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history = clf.get_history()\n",
    "history.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x22b948f9370>]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\r\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\r\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\r\n<svg height=\"248.518125pt\" version=\"1.1\" viewBox=\"0 0 372.103125 248.518125\" width=\"372.103125pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\r\n <metadata>\r\n  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\r\n   <cc:Work>\r\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\r\n    <dc:date>2022-03-29T21:21:48.593652</dc:date>\r\n    <dc:format>image/svg+xml</dc:format>\r\n    <dc:creator>\r\n     <cc:Agent>\r\n      <dc:title>Matplotlib v3.4.3, https://matplotlib.org/</dc:title>\r\n     </cc:Agent>\r\n    </dc:creator>\r\n   </cc:Work>\r\n  </rdf:RDF>\r\n </metadata>\r\n <defs>\r\n  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\r\n </defs>\r\n <g id=\"figure_1\">\r\n  <g id=\"patch_1\">\r\n   <path d=\"M 0 248.518125 \r\nL 372.103125 248.518125 \r\nL 372.103125 0 \r\nL 0 0 \r\nz\r\n\" style=\"fill:none;\"/>\r\n  </g>\r\n  <g id=\"axes_1\">\r\n   <g id=\"patch_2\">\r\n    <path d=\"M 30.103125 224.64 \r\nL 364.903125 224.64 \r\nL 364.903125 7.2 \r\nL 30.103125 7.2 \r\nz\r\n\" style=\"fill:#ffffff;\"/>\r\n   </g>\r\n   <g id=\"matplotlib.axis_1\">\r\n    <g id=\"xtick_1\">\r\n     <g id=\"line2d_1\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL 0 3.5 \r\n\" id=\"mf682466e89\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"45.321307\" xlink:href=\"#mf682466e89\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_1\">\r\n      <!-- 0 -->\r\n      <g transform=\"translate(42.140057 239.238437)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 2034 4250 \r\nQ 1547 4250 1301 3770 \r\nQ 1056 3291 1056 2328 \r\nQ 1056 1369 1301 889 \r\nQ 1547 409 2034 409 \r\nQ 2525 409 2770 889 \r\nQ 3016 1369 3016 2328 \r\nQ 3016 3291 2770 3770 \r\nQ 2525 4250 2034 4250 \r\nz\r\nM 2034 4750 \r\nQ 2819 4750 3233 4129 \r\nQ 3647 3509 3647 2328 \r\nQ 3647 1150 3233 529 \r\nQ 2819 -91 2034 -91 \r\nQ 1250 -91 836 529 \r\nQ 422 1150 422 2328 \r\nQ 422 3509 836 4129 \r\nQ 1250 4750 2034 4750 \r\nz\r\n\" id=\"DejaVuSans-30\" transform=\"scale(0.015625)\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_2\">\r\n     <g id=\"line2d_2\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"106.254968\" xlink:href=\"#mf682466e89\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_2\">\r\n      <!-- 200 -->\r\n      <g transform=\"translate(96.711218 239.238437)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 1228 531 \r\nL 3431 531 \r\nL 3431 0 \r\nL 469 0 \r\nL 469 531 \r\nQ 828 903 1448 1529 \r\nQ 2069 2156 2228 2338 \r\nQ 2531 2678 2651 2914 \r\nQ 2772 3150 2772 3378 \r\nQ 2772 3750 2511 3984 \r\nQ 2250 4219 1831 4219 \r\nQ 1534 4219 1204 4116 \r\nQ 875 4013 500 3803 \r\nL 500 4441 \r\nQ 881 4594 1212 4672 \r\nQ 1544 4750 1819 4750 \r\nQ 2544 4750 2975 4387 \r\nQ 3406 4025 3406 3419 \r\nQ 3406 3131 3298 2873 \r\nQ 3191 2616 2906 2266 \r\nQ 2828 2175 2409 1742 \r\nQ 1991 1309 1228 531 \r\nz\r\n\" id=\"DejaVuSans-32\" transform=\"scale(0.015625)\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-32\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_3\">\r\n     <g id=\"line2d_3\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"167.188629\" xlink:href=\"#mf682466e89\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_3\">\r\n      <!-- 400 -->\r\n      <g transform=\"translate(157.644879 239.238437)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 2419 4116 \r\nL 825 1625 \r\nL 2419 1625 \r\nL 2419 4116 \r\nz\r\nM 2253 4666 \r\nL 3047 4666 \r\nL 3047 1625 \r\nL 3713 1625 \r\nL 3713 1100 \r\nL 3047 1100 \r\nL 3047 0 \r\nL 2419 0 \r\nL 2419 1100 \r\nL 313 1100 \r\nL 313 1709 \r\nL 2253 4666 \r\nz\r\n\" id=\"DejaVuSans-34\" transform=\"scale(0.015625)\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-34\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_4\">\r\n     <g id=\"line2d_4\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"228.12229\" xlink:href=\"#mf682466e89\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_4\">\r\n      <!-- 600 -->\r\n      <g transform=\"translate(218.57854 239.238437)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 2113 2584 \r\nQ 1688 2584 1439 2293 \r\nQ 1191 2003 1191 1497 \r\nQ 1191 994 1439 701 \r\nQ 1688 409 2113 409 \r\nQ 2538 409 2786 701 \r\nQ 3034 994 3034 1497 \r\nQ 3034 2003 2786 2293 \r\nQ 2538 2584 2113 2584 \r\nz\r\nM 3366 4563 \r\nL 3366 3988 \r\nQ 3128 4100 2886 4159 \r\nQ 2644 4219 2406 4219 \r\nQ 1781 4219 1451 3797 \r\nQ 1122 3375 1075 2522 \r\nQ 1259 2794 1537 2939 \r\nQ 1816 3084 2150 3084 \r\nQ 2853 3084 3261 2657 \r\nQ 3669 2231 3669 1497 \r\nQ 3669 778 3244 343 \r\nQ 2819 -91 2113 -91 \r\nQ 1303 -91 875 529 \r\nQ 447 1150 447 2328 \r\nQ 447 3434 972 4092 \r\nQ 1497 4750 2381 4750 \r\nQ 2619 4750 2861 4703 \r\nQ 3103 4656 3366 4563 \r\nz\r\n\" id=\"DejaVuSans-36\" transform=\"scale(0.015625)\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-36\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_5\">\r\n     <g id=\"line2d_5\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"289.055951\" xlink:href=\"#mf682466e89\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_5\">\r\n      <!-- 800 -->\r\n      <g transform=\"translate(279.512201 239.238437)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 2034 2216 \r\nQ 1584 2216 1326 1975 \r\nQ 1069 1734 1069 1313 \r\nQ 1069 891 1326 650 \r\nQ 1584 409 2034 409 \r\nQ 2484 409 2743 651 \r\nQ 3003 894 3003 1313 \r\nQ 3003 1734 2745 1975 \r\nQ 2488 2216 2034 2216 \r\nz\r\nM 1403 2484 \r\nQ 997 2584 770 2862 \r\nQ 544 3141 544 3541 \r\nQ 544 4100 942 4425 \r\nQ 1341 4750 2034 4750 \r\nQ 2731 4750 3128 4425 \r\nQ 3525 4100 3525 3541 \r\nQ 3525 3141 3298 2862 \r\nQ 3072 2584 2669 2484 \r\nQ 3125 2378 3379 2068 \r\nQ 3634 1759 3634 1313 \r\nQ 3634 634 3220 271 \r\nQ 2806 -91 2034 -91 \r\nQ 1263 -91 848 271 \r\nQ 434 634 434 1313 \r\nQ 434 1759 690 2068 \r\nQ 947 2378 1403 2484 \r\nz\r\nM 1172 3481 \r\nQ 1172 3119 1398 2916 \r\nQ 1625 2713 2034 2713 \r\nQ 2441 2713 2670 2916 \r\nQ 2900 3119 2900 3481 \r\nQ 2900 3844 2670 4047 \r\nQ 2441 4250 2034 4250 \r\nQ 1625 4250 1398 4047 \r\nQ 1172 3844 1172 3481 \r\nz\r\n\" id=\"DejaVuSans-38\" transform=\"scale(0.015625)\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-38\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_6\">\r\n     <g id=\"line2d_6\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"349.989611\" xlink:href=\"#mf682466e89\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_6\">\r\n      <!-- 1000 -->\r\n      <g transform=\"translate(337.264611 239.238437)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 794 531 \r\nL 1825 531 \r\nL 1825 4091 \r\nL 703 3866 \r\nL 703 4441 \r\nL 1819 4666 \r\nL 2450 4666 \r\nL 2450 531 \r\nL 3481 531 \r\nL 3481 0 \r\nL 794 0 \r\nL 794 531 \r\nz\r\n\" id=\"DejaVuSans-31\" transform=\"scale(0.015625)\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-31\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\r\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"matplotlib.axis_2\">\r\n    <g id=\"ytick_1\">\r\n     <g id=\"line2d_7\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL -3.5 0 \r\n\" id=\"mad4bd3902f\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#mad4bd3902f\" y=\"224.244655\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_7\">\r\n      <!-- 0.8 -->\r\n      <g transform=\"translate(7.2 228.043873)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 684 794 \r\nL 1344 794 \r\nL 1344 0 \r\nL 684 0 \r\nL 684 794 \r\nz\r\n\" id=\"DejaVuSans-2e\" transform=\"scale(0.015625)\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-30\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-38\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_2\">\r\n     <g id=\"line2d_8\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#mad4bd3902f\" y=\"199.661355\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_8\">\r\n      <!-- 1.0 -->\r\n      <g transform=\"translate(7.2 203.460574)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-31\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_3\">\r\n     <g id=\"line2d_9\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#mad4bd3902f\" y=\"175.078056\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_9\">\r\n      <!-- 1.2 -->\r\n      <g transform=\"translate(7.2 178.877275)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-31\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-32\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_4\">\r\n     <g id=\"line2d_10\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#mad4bd3902f\" y=\"150.494757\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_10\">\r\n      <!-- 1.4 -->\r\n      <g transform=\"translate(7.2 154.293976)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-31\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-34\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_5\">\r\n     <g id=\"line2d_11\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#mad4bd3902f\" y=\"125.911458\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_11\">\r\n      <!-- 1.6 -->\r\n      <g transform=\"translate(7.2 129.710677)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-31\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-36\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_6\">\r\n     <g id=\"line2d_12\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#mad4bd3902f\" y=\"101.328159\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_12\">\r\n      <!-- 1.8 -->\r\n      <g transform=\"translate(7.2 105.127377)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-31\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-38\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_7\">\r\n     <g id=\"line2d_13\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#mad4bd3902f\" y=\"76.74486\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_13\">\r\n      <!-- 2.0 -->\r\n      <g transform=\"translate(7.2 80.544078)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-32\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_8\">\r\n     <g id=\"line2d_14\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#mad4bd3902f\" y=\"52.16156\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_14\">\r\n      <!-- 2.2 -->\r\n      <g transform=\"translate(7.2 55.960779)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-32\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-32\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_9\">\r\n     <g id=\"line2d_15\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#mad4bd3902f\" y=\"27.578261\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_15\">\r\n      <!-- 2.4 -->\r\n      <g transform=\"translate(7.2 31.37748)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-32\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-34\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"line2d_16\">\r\n    <path clip-path=\"url(#p45000a141d)\" d=\"M 45.321307 214.756364 \r\nL 47.453985 214.756364 \r\nL 48.672658 178.815868 \r\nL 48.977326 178.815868 \r\nL 49.281995 169.830744 \r\nL 49.586663 151.860496 \r\nL 49.891331 151.860496 \r\nL 50.196 133.890248 \r\nL 50.500668 124.905124 \r\nL 53.242683 124.905124 \r\nL 53.547351 106.934876 \r\nL 53.852019 53.024132 \r\nL 54.461356 17.083636 \r\nL 349.684943 17.083636 \r\nL 349.684943 17.083636 \r\n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\r\n   </g>\r\n   <g id=\"patch_3\">\r\n    <path d=\"M 30.103125 224.64 \r\nL 30.103125 7.2 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_4\">\r\n    <path d=\"M 364.903125 224.64 \r\nL 364.903125 7.2 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_5\">\r\n    <path d=\"M 30.103125 224.64 \r\nL 364.903125 224.64 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_6\">\r\n    <path d=\"M 30.103125 7.2 \r\nL 364.903125 7.2 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n  </g>\r\n </g>\r\n <defs>\r\n  <clipPath id=\"p45000a141d\">\r\n   <rect height=\"217.44\" width=\"334.8\" x=\"30.103125\" y=\"7.2\"/>\r\n  </clipPath>\r\n </defs>\r\n</svg>\r\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# plt.plot(history['epoch'], history['valid_acc'], label=\"validation accuracy\")\n",
    "plt.plot(history['epoch'], history['valid_acc']/history['# feature'], \n",
    "    label=\"validation accuracy/# feature\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000 Loss: 1.0930275917053223 num_feat: 38/38 Reg Loss: 0.08411549031734467 Val Accuracy: 27.77777777777778\n",
      "Epoch 2/1000 Loss: 1.0937050580978394 num_feat: 38/38 Reg Loss: 0.0841103345155716 Val Accuracy: 27.77777777777778\n",
      "Epoch 3/1000 Loss: 1.0919694900512695 num_feat: 38/38 Reg Loss: 0.08409985154867172 Val Accuracy: 27.77777777777778\n",
      "Epoch 4/1000 Loss: 1.0868761539459229 num_feat: 38/38 Reg Loss: 0.0840834379196167 Val Accuracy: 27.77777777777778\n",
      "Epoch 5/1000 Loss: 1.0882593393325806 num_feat: 38/38 Reg Loss: 0.08406230807304382 Val Accuracy: 27.77777777777778\n",
      "Epoch 6/1000 Loss: 1.0843054056167603 num_feat: 38/38 Reg Loss: 0.08403841406106949 Val Accuracy: 27.77777777777778\n",
      "Epoch 7/1000 Loss: 1.0849230289459229 num_feat: 38/38 Reg Loss: 0.08401001244783401 Val Accuracy: 27.77777777777778\n",
      "Epoch 8/1000 Loss: 1.0837808847427368 num_feat: 38/38 Reg Loss: 0.08397892862558365 Val Accuracy: 27.77777777777778\n",
      "Epoch 9/1000 Loss: 1.0865085124969482 num_feat: 38/38 Reg Loss: 0.08394590765237808 Val Accuracy: 27.77777777777778\n",
      "Epoch 10/1000 Loss: 1.085160732269287 num_feat: 38/38 Reg Loss: 0.08391012251377106 Val Accuracy: 27.77777777777778\n",
      "Epoch 11/1000 Loss: 1.0840837955474854 num_feat: 38/38 Reg Loss: 0.08387290686368942 Val Accuracy: 27.77777777777778\n",
      "Epoch 12/1000 Loss: 1.0835298299789429 num_feat: 38/38 Reg Loss: 0.08383365720510483 Val Accuracy: 27.77777777777778\n",
      "Epoch 13/1000 Loss: 1.0843819379806519 num_feat: 38/38 Reg Loss: 0.08379346132278442 Val Accuracy: 27.77777777777778\n",
      "Epoch 14/1000 Loss: 1.0807294845581055 num_feat: 38/38 Reg Loss: 0.08375240117311478 Val Accuracy: 27.77777777777778\n",
      "Epoch 15/1000 Loss: 1.0844767093658447 num_feat: 38/38 Reg Loss: 0.08371058851480484 Val Accuracy: 27.77777777777778\n",
      "Epoch 16/1000 Loss: 1.0787148475646973 num_feat: 38/38 Reg Loss: 0.08366789668798447 Val Accuracy: 27.77777777777778\n",
      "Epoch 17/1000 Loss: 1.0799643993377686 num_feat: 38/38 Reg Loss: 0.08362520486116409 Val Accuracy: 27.77777777777778\n",
      "Epoch 18/1000 Loss: 1.074407935142517 num_feat: 38/38 Reg Loss: 0.08358065783977509 Val Accuracy: 27.77777777777778\n",
      "Epoch 19/1000 Loss: 1.0660146474838257 num_feat: 38/38 Reg Loss: 0.08353553712368011 Val Accuracy: 27.77777777777778\n",
      "Epoch 20/1000 Loss: 1.0745470523834229 num_feat: 38/38 Reg Loss: 0.08348949253559113 Val Accuracy: 27.77777777777778\n",
      "Epoch 21/1000 Loss: 1.0618807077407837 num_feat: 38/38 Reg Loss: 0.08344428986310959 Val Accuracy: 27.77777777777778\n",
      "Epoch 22/1000 Loss: 1.0599558353424072 num_feat: 38/38 Reg Loss: 0.08339731395244598 Val Accuracy: 27.77777777777778\n",
      "Epoch 23/1000 Loss: 1.0713979005813599 num_feat: 38/38 Reg Loss: 0.08335196226835251 Val Accuracy: 27.77777777777778\n",
      "Epoch 24/1000 Loss: 1.0548616647720337 num_feat: 38/38 Reg Loss: 0.08330943435430527 Val Accuracy: 27.77777777777778\n",
      "Epoch 25/1000 Loss: 1.048576831817627 num_feat: 38/38 Reg Loss: 0.08326642960309982 Val Accuracy: 27.77777777777778\n",
      "Epoch 26/1000 Loss: 1.0611768960952759 num_feat: 38/38 Reg Loss: 0.08322441577911377 Val Accuracy: 27.77777777777778\n",
      "Epoch 27/1000 Loss: 1.057159185409546 num_feat: 38/38 Reg Loss: 0.0831839069724083 Val Accuracy: 36.111111111111114\n",
      "Epoch 28/1000 Loss: 1.0517799854278564 num_feat: 38/38 Reg Loss: 0.0831483006477356 Val Accuracy: 41.666666666666664\n",
      "Epoch 29/1000 Loss: 1.043142318725586 num_feat: 38/38 Reg Loss: 0.08312081545591354 Val Accuracy: 30.555555555555557\n",
      "Epoch 30/1000 Loss: 1.0382767915725708 num_feat: 38/38 Reg Loss: 0.08309841901063919 Val Accuracy: 27.77777777777778\n",
      "Epoch 31/1000 Loss: 1.0124986171722412 num_feat: 38/38 Reg Loss: 0.08307819813489914 Val Accuracy: 63.888888888888886\n",
      "Epoch 32/1000 Loss: 0.972388505935669 num_feat: 38/38 Reg Loss: 0.08305876702070236 Val Accuracy: 27.77777777777778\n",
      "Epoch 33/1000 Loss: 0.982880175113678 num_feat: 38/38 Reg Loss: 0.08304516971111298 Val Accuracy: 50.0\n",
      "Epoch 34/1000 Loss: 0.9558691382408142 num_feat: 38/38 Reg Loss: 0.08304200321435928 Val Accuracy: 33.333333333333336\n",
      "Epoch 35/1000 Loss: 0.944311261177063 num_feat: 38/38 Reg Loss: 0.08303753286600113 Val Accuracy: 66.66666666666667\n",
      "Epoch 36/1000 Loss: 0.9758796691894531 num_feat: 38/38 Reg Loss: 0.08303719758987427 Val Accuracy: 50.0\n",
      "Epoch 37/1000 Loss: 0.8850730061531067 num_feat: 38/38 Reg Loss: 0.08305749297142029 Val Accuracy: 69.44444444444444\n",
      "Epoch 38/1000 Loss: 0.8676720857620239 num_feat: 38/38 Reg Loss: 0.08307486027479172 Val Accuracy: 69.44444444444444\n",
      "Epoch 39/1000 Loss: 0.8597931861877441 num_feat: 38/38 Reg Loss: 0.08309290558099747 Val Accuracy: 69.44444444444444\n",
      "Epoch 40/1000 Loss: 0.7197033762931824 num_feat: 38/38 Reg Loss: 0.08315227180719376 Val Accuracy: 69.44444444444444\n",
      "Epoch 41/1000 Loss: 0.7206624150276184 num_feat: 38/38 Reg Loss: 0.08323221653699875 Val Accuracy: 69.44444444444444\n",
      "Epoch 42/1000 Loss: 0.6883147358894348 num_feat: 38/38 Reg Loss: 0.08332003653049469 Val Accuracy: 75.0\n",
      "Epoch 43/1000 Loss: 0.6448855400085449 num_feat: 38/38 Reg Loss: 0.08340101689100266 Val Accuracy: 69.44444444444444\n",
      "Epoch 44/1000 Loss: 0.5321459174156189 num_feat: 38/38 Reg Loss: 0.08344414085149765 Val Accuracy: 47.22222222222222\n",
      "Epoch 45/1000 Loss: 0.4908261001110077 num_feat: 38/38 Reg Loss: 0.0834856927394867 Val Accuracy: 80.55555555555556\n",
      "Epoch 46/1000 Loss: 0.9734109044075012 num_feat: 38/38 Reg Loss: 0.08352888375520706 Val Accuracy: 86.11111111111111\n",
      "Epoch 47/1000 Loss: 0.4493166208267212 num_feat: 38/38 Reg Loss: 0.08356382697820663 Val Accuracy: 83.33333333333333\n",
      "Epoch 48/1000 Loss: 0.5466042757034302 num_feat: 38/38 Reg Loss: 0.08355638384819031 Val Accuracy: 80.55555555555556\n",
      "Epoch 49/1000 Loss: 0.6973758339881897 num_feat: 38/38 Reg Loss: 0.08350683748722076 Val Accuracy: 69.44444444444444\n",
      "Epoch 50/1000 Loss: 0.527058482170105 num_feat: 38/38 Reg Loss: 0.0835004672408104 Val Accuracy: 86.11111111111111\n",
      "Epoch 51/1000 Loss: 0.5028263330459595 num_feat: 38/38 Reg Loss: 0.0834890753030777 Val Accuracy: 80.55555555555556\n",
      "Epoch 52/1000 Loss: 0.3795355558395386 num_feat: 38/38 Reg Loss: 0.08346693217754364 Val Accuracy: 77.77777777777777\n",
      "Epoch 53/1000 Loss: 0.4996108114719391 num_feat: 38/38 Reg Loss: 0.08343491703271866 Val Accuracy: 63.888888888888886\n",
      "Epoch 54/1000 Loss: 0.3466704189777374 num_feat: 38/38 Reg Loss: 0.08342092484235764 Val Accuracy: 66.66666666666667\n",
      "Epoch 55/1000 Loss: 0.3594231903553009 num_feat: 38/38 Reg Loss: 0.08340023458003998 Val Accuracy: 72.22222222222223\n",
      "Epoch 56/1000 Loss: 0.3516305387020111 num_feat: 38/38 Reg Loss: 0.08339416980743408 Val Accuracy: 80.55555555555556\n",
      "Epoch 57/1000 Loss: 0.29705244302749634 num_feat: 38/38 Reg Loss: 0.08337364345788956 Val Accuracy: 80.55555555555556\n",
      "Epoch 58/1000 Loss: 0.30008283257484436 num_feat: 38/38 Reg Loss: 0.0833364874124527 Val Accuracy: 75.0\n",
      "Epoch 59/1000 Loss: 0.2413180023431778 num_feat: 38/38 Reg Loss: 0.08331853896379471 Val Accuracy: 77.77777777777777\n",
      "Epoch 60/1000 Loss: 0.33545002341270447 num_feat: 38/38 Reg Loss: 0.08329445123672485 Val Accuracy: 83.33333333333333\n",
      "Epoch 61/1000 Loss: 0.23199249804019928 num_feat: 38/38 Reg Loss: 0.08320309221744537 Val Accuracy: 75.0\n",
      "Epoch 62/1000 Loss: 0.8217610716819763 num_feat: 38/38 Reg Loss: 0.08310599625110626 Val Accuracy: 77.77777777777777\n",
      "Epoch 63/1000 Loss: 0.23572374880313873 num_feat: 38/38 Reg Loss: 0.08302415162324905 Val Accuracy: 77.77777777777777\n",
      "Epoch 64/1000 Loss: 0.710616409778595 num_feat: 38/38 Reg Loss: 0.08298621326684952 Val Accuracy: 88.88888888888889\n",
      "Epoch 65/1000 Loss: 0.28034624457359314 num_feat: 38/38 Reg Loss: 0.08282922953367233 Val Accuracy: 77.77777777777777\n",
      "Epoch 66/1000 Loss: 0.20169374346733093 num_feat: 38/38 Reg Loss: 0.08266150951385498 Val Accuracy: 75.0\n",
      "Epoch 67/1000 Loss: 0.672235369682312 num_feat: 38/38 Reg Loss: 0.08253724873065948 Val Accuracy: 88.88888888888889\n",
      "Epoch 68/1000 Loss: 0.17887069284915924 num_feat: 38/38 Reg Loss: 0.08246167749166489 Val Accuracy: 91.66666666666667\n",
      "Epoch 69/1000 Loss: 0.19791090488433838 num_feat: 38/38 Reg Loss: 0.08238444477319717 Val Accuracy: 86.11111111111111\n",
      "Epoch 70/1000 Loss: 0.18733733892440796 num_feat: 38/38 Reg Loss: 0.0822981670498848 Val Accuracy: 86.11111111111111\n",
      "Epoch 71/1000 Loss: 0.48329758644104004 num_feat: 38/38 Reg Loss: 0.08220833539962769 Val Accuracy: 75.0\n",
      "Epoch 72/1000 Loss: 0.22843538224697113 num_feat: 38/38 Reg Loss: 0.08216869086027145 Val Accuracy: 63.888888888888886\n",
      "Epoch 73/1000 Loss: 0.6235960721969604 num_feat: 38/38 Reg Loss: 0.08212456852197647 Val Accuracy: 91.66666666666667\n",
      "Epoch 74/1000 Loss: 0.5118208527565002 num_feat: 38/38 Reg Loss: 0.08192407339811325 Val Accuracy: 80.55555555555556\n",
      "Epoch 75/1000 Loss: 0.2069871574640274 num_feat: 38/38 Reg Loss: 0.08157852292060852 Val Accuracy: 77.77777777777777\n",
      "Epoch 76/1000 Loss: 0.8894091248512268 num_feat: 38/38 Reg Loss: 0.08130183815956116 Val Accuracy: 83.33333333333333\n",
      "Epoch 77/1000 Loss: 0.2568630278110504 num_feat: 38/38 Reg Loss: 0.08121182769536972 Val Accuracy: 77.77777777777777\n",
      "Epoch 78/1000 Loss: 0.34681203961372375 num_feat: 38/38 Reg Loss: 0.08108773082494736 Val Accuracy: 77.77777777777777\n",
      "Epoch 79/1000 Loss: 0.2851134240627289 num_feat: 38/38 Reg Loss: 0.08100671321153641 Val Accuracy: 77.77777777777777\n",
      "Epoch 80/1000 Loss: 0.4813617467880249 num_feat: 38/38 Reg Loss: 0.08095743507146835 Val Accuracy: 80.55555555555556\n",
      "Epoch 81/1000 Loss: 0.7096997499465942 num_feat: 38/38 Reg Loss: 0.08060402423143387 Val Accuracy: 86.11111111111111\n",
      "Epoch 82/1000 Loss: 0.3016592860221863 num_feat: 37/38 Reg Loss: 0.07996003329753876 Val Accuracy: 75.0\n",
      "Epoch 83/1000 Loss: 0.30811434984207153 num_feat: 37/38 Reg Loss: 0.07942789793014526 Val Accuracy: 86.11111111111111\n",
      "Epoch 84/1000 Loss: 0.4095182418823242 num_feat: 37/38 Reg Loss: 0.07898422330617905 Val Accuracy: 55.55555555555556\n",
      "Epoch 85/1000 Loss: 0.4428292214870453 num_feat: 37/38 Reg Loss: 0.07860592007637024 Val Accuracy: 88.88888888888889\n",
      "Epoch 86/1000 Loss: 0.4015953540802002 num_feat: 37/38 Reg Loss: 0.07832387834787369 Val Accuracy: 66.66666666666667\n",
      "Epoch 87/1000 Loss: 0.49809500575065613 num_feat: 37/38 Reg Loss: 0.07802756875753403 Val Accuracy: 88.88888888888889\n",
      "Epoch 88/1000 Loss: 0.4476962387561798 num_feat: 37/38 Reg Loss: 0.0776301771402359 Val Accuracy: 80.55555555555556\n",
      "Epoch 89/1000 Loss: 0.1973806619644165 num_feat: 37/38 Reg Loss: 0.07719149440526962 Val Accuracy: 80.55555555555556\n",
      "Epoch 90/1000 Loss: 0.2672894597053528 num_feat: 36/38 Reg Loss: 0.07680845260620117 Val Accuracy: 75.0\n",
      "Epoch 91/1000 Loss: 0.1890306919813156 num_feat: 36/38 Reg Loss: 0.07652536779642105 Val Accuracy: 77.77777777777777\n",
      "Epoch 92/1000 Loss: 0.5658517479896545 num_feat: 36/38 Reg Loss: 0.07634355872869492 Val Accuracy: 86.11111111111111\n",
      "Epoch 93/1000 Loss: 0.2537163197994232 num_feat: 34/38 Reg Loss: 0.07611946761608124 Val Accuracy: 80.55555555555556\n",
      "Epoch 94/1000 Loss: 0.8946638703346252 num_feat: 35/38 Reg Loss: 0.07597381621599197 Val Accuracy: 86.11111111111111\n",
      "Epoch 95/1000 Loss: 0.17068400979042053 num_feat: 35/38 Reg Loss: 0.07543759793043137 Val Accuracy: 83.33333333333333\n",
      "Epoch 96/1000 Loss: 0.3409287631511688 num_feat: 34/38 Reg Loss: 0.0750027447938919 Val Accuracy: 86.11111111111111\n",
      "Epoch 97/1000 Loss: 0.3234240412712097 num_feat: 34/38 Reg Loss: 0.07467054575681686 Val Accuracy: 94.44444444444444\n",
      "Epoch 98/1000 Loss: 0.2772631049156189 num_feat: 34/38 Reg Loss: 0.07436560094356537 Val Accuracy: 88.88888888888889\n",
      "Epoch 99/1000 Loss: 0.3283030092716217 num_feat: 34/38 Reg Loss: 0.07404597103595734 Val Accuracy: 86.11111111111111\n",
      "Epoch 100/1000 Loss: 0.47920703887939453 num_feat: 34/38 Reg Loss: 0.07373785972595215 Val Accuracy: 91.66666666666667\n",
      "Epoch 101/1000 Loss: 0.1858447641134262 num_feat: 33/38 Reg Loss: 0.07336810976266861 Val Accuracy: 94.44444444444444\n",
      "Epoch 102/1000 Loss: 0.34804651141166687 num_feat: 33/38 Reg Loss: 0.07305939495563507 Val Accuracy: 91.66666666666667\n",
      "Epoch 103/1000 Loss: 0.135958731174469 num_feat: 33/38 Reg Loss: 0.07279032468795776 Val Accuracy: 83.33333333333333\n",
      "Epoch 104/1000 Loss: 0.15154694020748138 num_feat: 32/38 Reg Loss: 0.07252413779497147 Val Accuracy: 97.22222222222223\n",
      "Epoch 105/1000 Loss: 0.14470024406909943 num_feat: 32/38 Reg Loss: 0.07230792939662933 Val Accuracy: 97.22222222222223\n",
      "Epoch 106/1000 Loss: 0.16433338820934296 num_feat: 32/38 Reg Loss: 0.07205716520547867 Val Accuracy: 88.88888888888889\n",
      "Epoch 107/1000 Loss: 0.22279348969459534 num_feat: 32/38 Reg Loss: 0.07175739854574203 Val Accuracy: 91.66666666666667\n",
      "Epoch 108/1000 Loss: 0.13703250885009766 num_feat: 32/38 Reg Loss: 0.0715257078409195 Val Accuracy: 86.11111111111111\n",
      "Epoch 109/1000 Loss: 0.2393658459186554 num_feat: 32/38 Reg Loss: 0.07124368846416473 Val Accuracy: 97.22222222222223\n",
      "Epoch 110/1000 Loss: 0.3643670976161957 num_feat: 32/38 Reg Loss: 0.07100991159677505 Val Accuracy: 75.0\n",
      "Epoch 111/1000 Loss: 0.586821973323822 num_feat: 32/38 Reg Loss: 0.07077445834875107 Val Accuracy: 77.77777777777777\n",
      "Epoch 112/1000 Loss: 0.43913373351097107 num_feat: 31/38 Reg Loss: 0.07042106986045837 Val Accuracy: 94.44444444444444\n",
      "Epoch 113/1000 Loss: 0.09338120371103287 num_feat: 31/38 Reg Loss: 0.06975095719099045 Val Accuracy: 77.77777777777777\n",
      "Epoch 114/1000 Loss: 0.6979736685752869 num_feat: 30/38 Reg Loss: 0.06911365687847137 Val Accuracy: 88.88888888888889\n",
      "Epoch 115/1000 Loss: 0.8449937701225281 num_feat: 28/38 Reg Loss: 0.06858475506305695 Val Accuracy: 83.33333333333333\n",
      "Epoch 116/1000 Loss: 0.6990434527397156 num_feat: 28/38 Reg Loss: 0.06816845387220383 Val Accuracy: 58.333333333333336\n",
      "Epoch 117/1000 Loss: 2.1666486263275146 num_feat: 27/38 Reg Loss: 0.06784885376691818 Val Accuracy: 61.111111111111114\n",
      "Epoch 118/1000 Loss: 0.8306143879890442 num_feat: 27/38 Reg Loss: 0.06730207055807114 Val Accuracy: 72.22222222222223\n",
      "Epoch 119/1000 Loss: 0.3342114984989166 num_feat: 28/38 Reg Loss: 0.06653256714344025 Val Accuracy: 94.44444444444444\n",
      "Epoch 120/1000 Loss: 0.22518855333328247 num_feat: 28/38 Reg Loss: 0.06562363356351852 Val Accuracy: 77.77777777777777\n",
      "Epoch 121/1000 Loss: 0.2831270694732666 num_feat: 28/38 Reg Loss: 0.06482675671577454 Val Accuracy: 75.0\n",
      "Epoch 122/1000 Loss: 0.8789234161376953 num_feat: 28/38 Reg Loss: 0.06415028870105743 Val Accuracy: 80.55555555555556\n",
      "Epoch 123/1000 Loss: 0.38448548316955566 num_feat: 29/38 Reg Loss: 0.06365401297807693 Val Accuracy: 91.66666666666667\n",
      "Epoch 124/1000 Loss: 0.28045302629470825 num_feat: 29/38 Reg Loss: 0.06322458386421204 Val Accuracy: 75.0\n",
      "Epoch 125/1000 Loss: 0.4237721264362335 num_feat: 27/38 Reg Loss: 0.06283283233642578 Val Accuracy: 75.0\n",
      "Epoch 126/1000 Loss: 0.5131252408027649 num_feat: 27/38 Reg Loss: 0.06246746703982353 Val Accuracy: 88.88888888888889\n",
      "Epoch 127/1000 Loss: 0.252164363861084 num_feat: 27/38 Reg Loss: 0.06210977956652641 Val Accuracy: 94.44444444444444\n",
      "Epoch 128/1000 Loss: 0.2248261570930481 num_feat: 27/38 Reg Loss: 0.061779141426086426 Val Accuracy: 83.33333333333333\n",
      "Epoch 129/1000 Loss: 0.21089980006217957 num_feat: 26/38 Reg Loss: 0.06148102506995201 Val Accuracy: 86.11111111111111\n",
      "Epoch 130/1000 Loss: 0.30990535020828247 num_feat: 26/38 Reg Loss: 0.06121599301695824 Val Accuracy: 86.11111111111111\n",
      "Epoch 131/1000 Loss: 0.30681338906288147 num_feat: 25/38 Reg Loss: 0.06099303439259529 Val Accuracy: 94.44444444444444\n",
      "Epoch 132/1000 Loss: 0.17339183390140533 num_feat: 25/38 Reg Loss: 0.06083804368972778 Val Accuracy: 91.66666666666667\n",
      "Epoch 133/1000 Loss: 0.1734723299741745 num_feat: 25/38 Reg Loss: 0.06066518649458885 Val Accuracy: 86.11111111111111\n",
      "Epoch 134/1000 Loss: 0.3751733899116516 num_feat: 25/38 Reg Loss: 0.06048628315329552 Val Accuracy: 94.44444444444444\n",
      "Epoch 135/1000 Loss: 0.14742229878902435 num_feat: 25/38 Reg Loss: 0.06022268533706665 Val Accuracy: 80.55555555555556\n",
      "Epoch 136/1000 Loss: 0.25421270728111267 num_feat: 25/38 Reg Loss: 0.059978682547807693 Val Accuracy: 86.11111111111111\n",
      "Epoch 137/1000 Loss: 0.1558302640914917 num_feat: 25/38 Reg Loss: 0.05966071039438248 Val Accuracy: 86.11111111111111\n",
      "Epoch 138/1000 Loss: 0.33507996797561646 num_feat: 24/38 Reg Loss: 0.05937705561518669 Val Accuracy: 86.11111111111111\n",
      "Epoch 139/1000 Loss: 0.1424407958984375 num_feat: 23/38 Reg Loss: 0.05919559672474861 Val Accuracy: 91.66666666666667\n",
      "Epoch 140/1000 Loss: 0.1274934560060501 num_feat: 22/38 Reg Loss: 0.05904601141810417 Val Accuracy: 86.11111111111111\n",
      "Epoch 141/1000 Loss: 0.12634967267513275 num_feat: 22/38 Reg Loss: 0.05890309810638428 Val Accuracy: 86.11111111111111\n",
      "Epoch 142/1000 Loss: 0.1325833797454834 num_feat: 22/38 Reg Loss: 0.05876389890909195 Val Accuracy: 91.66666666666667\n",
      "Epoch 143/1000 Loss: 0.15558089315891266 num_feat: 22/38 Reg Loss: 0.05860558897256851 Val Accuracy: 91.66666666666667\n",
      "Epoch 144/1000 Loss: 0.22194315493106842 num_feat: 22/38 Reg Loss: 0.05842730030417442 Val Accuracy: 94.44444444444444\n",
      "Epoch 145/1000 Loss: 0.11397425085306168 num_feat: 22/38 Reg Loss: 0.05815126374363899 Val Accuracy: 94.44444444444444\n",
      "Epoch 146/1000 Loss: 0.11842057853937149 num_feat: 21/38 Reg Loss: 0.05787171795964241 Val Accuracy: 88.88888888888889\n",
      "Epoch 147/1000 Loss: 0.12731030583381653 num_feat: 20/38 Reg Loss: 0.057632654905319214 Val Accuracy: 91.66666666666667\n",
      "Epoch 148/1000 Loss: 0.1395697295665741 num_feat: 20/38 Reg Loss: 0.057414907962083817 Val Accuracy: 88.88888888888889\n",
      "Epoch 149/1000 Loss: 0.21126967668533325 num_feat: 20/38 Reg Loss: 0.05723743513226509 Val Accuracy: 88.88888888888889\n",
      "Epoch 150/1000 Loss: 0.37375083565711975 num_feat: 19/38 Reg Loss: 0.05693916231393814 Val Accuracy: 86.11111111111111\n",
      "Epoch 151/1000 Loss: 0.10072818398475647 num_feat: 19/38 Reg Loss: 0.05669901892542839 Val Accuracy: 94.44444444444444\n",
      "Epoch 152/1000 Loss: 0.11012204736471176 num_feat: 19/38 Reg Loss: 0.056458521634340286 Val Accuracy: 94.44444444444444\n",
      "Epoch 153/1000 Loss: 0.10776567459106445 num_feat: 19/38 Reg Loss: 0.056216008961200714 Val Accuracy: 97.22222222222223\n",
      "Epoch 154/1000 Loss: 0.13320167362689972 num_feat: 19/38 Reg Loss: 0.05595818907022476 Val Accuracy: 94.44444444444444\n",
      "Epoch 155/1000 Loss: 0.10342651605606079 num_feat: 19/38 Reg Loss: 0.05567488074302673 Val Accuracy: 94.44444444444444\n",
      "Epoch 156/1000 Loss: 0.12302316725254059 num_feat: 19/38 Reg Loss: 0.05541059374809265 Val Accuracy: 94.44444444444444\n",
      "Epoch 157/1000 Loss: 0.08968666940927505 num_feat: 19/38 Reg Loss: 0.05512138828635216 Val Accuracy: 83.33333333333333\n",
      "Epoch 158/1000 Loss: 0.43032360076904297 num_feat: 19/38 Reg Loss: 0.0548345148563385 Val Accuracy: 88.88888888888889\n",
      "Epoch 159/1000 Loss: 0.17408643662929535 num_feat: 19/38 Reg Loss: 0.05472617968916893 Val Accuracy: 94.44444444444444\n",
      "Epoch 160/1000 Loss: 0.11267029494047165 num_feat: 19/38 Reg Loss: 0.05469128489494324 Val Accuracy: 88.88888888888889\n",
      "Epoch 161/1000 Loss: 0.09564924240112305 num_feat: 20/38 Reg Loss: 0.054666418582201004 Val Accuracy: 83.33333333333333\n",
      "Epoch 162/1000 Loss: 0.4958045184612274 num_feat: 20/38 Reg Loss: 0.05462181195616722 Val Accuracy: 97.22222222222223\n",
      "Epoch 163/1000 Loss: 0.11519409716129303 num_feat: 20/38 Reg Loss: 0.05441848561167717 Val Accuracy: 94.44444444444444\n",
      "Epoch 164/1000 Loss: 0.09809496253728867 num_feat: 20/38 Reg Loss: 0.05420380458235741 Val Accuracy: 88.88888888888889\n",
      "Epoch 165/1000 Loss: 0.10391618311405182 num_feat: 20/38 Reg Loss: 0.053975995630025864 Val Accuracy: 91.66666666666667\n",
      "Epoch 166/1000 Loss: 0.3256649971008301 num_feat: 20/38 Reg Loss: 0.05380138382315636 Val Accuracy: 94.44444444444444\n",
      "Epoch 167/1000 Loss: 0.09090331941843033 num_feat: 20/38 Reg Loss: 0.053705859929323196 Val Accuracy: 88.88888888888889\n",
      "Epoch 168/1000 Loss: 0.16510780155658722 num_feat: 20/38 Reg Loss: 0.05363400653004646 Val Accuracy: 80.55555555555556\n",
      "Epoch 169/1000 Loss: 0.09816846251487732 num_feat: 20/38 Reg Loss: 0.053613174706697464 Val Accuracy: 83.33333333333333\n",
      "Epoch 170/1000 Loss: 0.10933507233858109 num_feat: 20/38 Reg Loss: 0.05359285697340965 Val Accuracy: 69.44444444444444\n",
      "Epoch 171/1000 Loss: 0.16843490302562714 num_feat: 20/38 Reg Loss: 0.05353813245892525 Val Accuracy: 83.33333333333333\n",
      "Epoch 172/1000 Loss: 0.37777382135391235 num_feat: 20/38 Reg Loss: 0.05342578887939453 Val Accuracy: 94.44444444444444\n",
      "Epoch 173/1000 Loss: 0.08801501989364624 num_feat: 20/38 Reg Loss: 0.05313754081726074 Val Accuracy: 94.44444444444444\n",
      "Epoch 174/1000 Loss: 0.0765642300248146 num_feat: 19/38 Reg Loss: 0.05285785719752312 Val Accuracy: 86.11111111111111\n",
      "Epoch 175/1000 Loss: 0.13192236423492432 num_feat: 19/38 Reg Loss: 0.052600037306547165 Val Accuracy: 86.11111111111111\n",
      "Epoch 176/1000 Loss: 0.31370753049850464 num_feat: 20/38 Reg Loss: 0.052383966743946075 Val Accuracy: 91.66666666666667\n",
      "Epoch 177/1000 Loss: 0.09390651434659958 num_feat: 20/38 Reg Loss: 0.05225599557161331 Val Accuracy: 94.44444444444444\n",
      "Epoch 178/1000 Loss: 0.092854805290699 num_feat: 20/38 Reg Loss: 0.05214889720082283 Val Accuracy: 86.11111111111111\n",
      "Epoch 179/1000 Loss: 0.09578271955251694 num_feat: 20/38 Reg Loss: 0.052011437714099884 Val Accuracy: 91.66666666666667\n",
      "Epoch 180/1000 Loss: 0.1298491656780243 num_feat: 19/38 Reg Loss: 0.05184867978096008 Val Accuracy: 86.11111111111111\n",
      "Epoch 181/1000 Loss: 0.0837441086769104 num_feat: 19/38 Reg Loss: 0.051651716232299805 Val Accuracy: 91.66666666666667\n",
      "Epoch 182/1000 Loss: 0.0674089789390564 num_feat: 19/38 Reg Loss: 0.05148259922862053 Val Accuracy: 94.44444444444444\n",
      "Epoch 183/1000 Loss: 0.07164693623781204 num_feat: 19/38 Reg Loss: 0.05131847411394119 Val Accuracy: 94.44444444444444\n",
      "Epoch 184/1000 Loss: 0.0782250314950943 num_feat: 19/38 Reg Loss: 0.05116474628448486 Val Accuracy: 86.11111111111111\n",
      "Epoch 185/1000 Loss: 0.07978049665689468 num_feat: 19/38 Reg Loss: 0.05104084685444832 Val Accuracy: 94.44444444444444\n",
      "Epoch 186/1000 Loss: 0.08157958090305328 num_feat: 19/38 Reg Loss: 0.050936438143253326 Val Accuracy: 94.44444444444444\n",
      "Epoch 187/1000 Loss: 0.07465606927871704 num_feat: 19/38 Reg Loss: 0.05084927752614021 Val Accuracy: 97.22222222222223\n",
      "Epoch 188/1000 Loss: 0.10464975982904434 num_feat: 19/38 Reg Loss: 0.05076662451028824 Val Accuracy: 91.66666666666667\n",
      "Epoch 189/1000 Loss: 0.1161450669169426 num_feat: 19/38 Reg Loss: 0.05063491687178612 Val Accuracy: 80.55555555555556\n",
      "Epoch 190/1000 Loss: 0.07349023967981339 num_feat: 19/38 Reg Loss: 0.05049430951476097 Val Accuracy: 94.44444444444444\n",
      "Epoch 191/1000 Loss: 0.07745539397001266 num_feat: 19/38 Reg Loss: 0.050356388092041016 Val Accuracy: 91.66666666666667\n",
      "Epoch 192/1000 Loss: 0.06975948065519333 num_feat: 19/38 Reg Loss: 0.05021944269537926 Val Accuracy: 94.44444444444444\n",
      "Epoch 193/1000 Loss: 0.062872514128685 num_feat: 19/38 Reg Loss: 0.05007713660597801 Val Accuracy: 94.44444444444444\n",
      "Epoch 194/1000 Loss: 0.06414882838726044 num_feat: 19/38 Reg Loss: 0.04993921518325806 Val Accuracy: 91.66666666666667\n",
      "Epoch 195/1000 Loss: 0.12262050062417984 num_feat: 18/38 Reg Loss: 0.049808960407972336 Val Accuracy: 91.66666666666667\n",
      "Epoch 196/1000 Loss: 0.11652829498052597 num_feat: 17/38 Reg Loss: 0.049728333950042725 Val Accuracy: 94.44444444444444\n",
      "Epoch 197/1000 Loss: 0.10746769607067108 num_feat: 17/38 Reg Loss: 0.04967338219285011 Val Accuracy: 97.22222222222223\n",
      "Epoch 198/1000 Loss: 0.08566305786371231 num_feat: 17/38 Reg Loss: 0.04964834451675415 Val Accuracy: 94.44444444444444\n",
      "Epoch 199/1000 Loss: 0.12031550705432892 num_feat: 17/38 Reg Loss: 0.049589645117521286 Val Accuracy: 86.11111111111111\n",
      "Epoch 200/1000 Loss: 0.2150920033454895 num_feat: 17/38 Reg Loss: 0.04949502274394035 Val Accuracy: 94.44444444444444\n",
      "Epoch 201/1000 Loss: 0.13795237243175507 num_feat: 17/38 Reg Loss: 0.04929641634225845 Val Accuracy: 94.44444444444444\n",
      "Epoch 202/1000 Loss: 0.0877019390463829 num_feat: 17/38 Reg Loss: 0.04906388744711876 Val Accuracy: 86.11111111111111\n",
      "Epoch 203/1000 Loss: 0.10636933892965317 num_feat: 16/38 Reg Loss: 0.0488271489739418 Val Accuracy: 86.11111111111111\n",
      "Epoch 204/1000 Loss: 0.43917566537857056 num_feat: 17/38 Reg Loss: 0.04864557832479477 Val Accuracy: 94.44444444444444\n",
      "Epoch 205/1000 Loss: 0.12362056970596313 num_feat: 17/38 Reg Loss: 0.0485958606004715 Val Accuracy: 94.44444444444444\n",
      "Epoch 206/1000 Loss: 0.06496883928775787 num_feat: 16/38 Reg Loss: 0.04857878014445305 Val Accuracy: 94.44444444444444\n",
      "Epoch 207/1000 Loss: 0.07002171128988266 num_feat: 16/38 Reg Loss: 0.04855908825993538 Val Accuracy: 94.44444444444444\n",
      "Epoch 208/1000 Loss: 0.3463391065597534 num_feat: 16/38 Reg Loss: 0.048541974276304245 Val Accuracy: 94.44444444444444\n",
      "Epoch 209/1000 Loss: 0.05846153944730759 num_feat: 16/38 Reg Loss: 0.048437561839818954 Val Accuracy: 94.44444444444444\n",
      "Epoch 210/1000 Loss: 0.20153896510601044 num_feat: 16/38 Reg Loss: 0.04833696037530899 Val Accuracy: 94.44444444444444\n",
      "Epoch 211/1000 Loss: 0.06611520797014236 num_feat: 16/38 Reg Loss: 0.04813447222113609 Val Accuracy: 91.66666666666667\n",
      "Epoch 212/1000 Loss: 0.41629642248153687 num_feat: 16/38 Reg Loss: 0.04794549569487572 Val Accuracy: 94.44444444444444\n",
      "Epoch 213/1000 Loss: 0.28906410932540894 num_feat: 16/38 Reg Loss: 0.04784513637423515 Val Accuracy: 94.44444444444444\n",
      "Epoch 214/1000 Loss: 0.10071875154972076 num_feat: 16/38 Reg Loss: 0.047831229865550995 Val Accuracy: 94.44444444444444\n",
      "Epoch 215/1000 Loss: 0.24498258531093597 num_feat: 16/38 Reg Loss: 0.04777228832244873 Val Accuracy: 97.22222222222223\n",
      "Epoch 216/1000 Loss: 0.09078910201787949 num_feat: 16/38 Reg Loss: 0.047630827873945236 Val Accuracy: 97.22222222222223\n",
      "Epoch 217/1000 Loss: 0.07151838392019272 num_feat: 16/38 Reg Loss: 0.0474790595471859 Val Accuracy: 97.22222222222223\n",
      "Epoch 218/1000 Loss: 0.07821118086576462 num_feat: 16/38 Reg Loss: 0.04732749983668327 Val Accuracy: 91.66666666666667\n",
      "Epoch 219/1000 Loss: 0.07533463090658188 num_feat: 16/38 Reg Loss: 0.04716849699616432 Val Accuracy: 91.66666666666667\n",
      "Epoch 220/1000 Loss: 0.18146571516990662 num_feat: 16/38 Reg Loss: 0.04701909050345421 Val Accuracy: 94.44444444444444\n",
      "Epoch 221/1000 Loss: 0.0794965848326683 num_feat: 16/38 Reg Loss: 0.04687615856528282 Val Accuracy: 91.66666666666667\n",
      "Epoch 222/1000 Loss: 0.08251375705003738 num_feat: 15/38 Reg Loss: 0.04674721881747246 Val Accuracy: 94.44444444444444\n",
      "Epoch 223/1000 Loss: 0.06890691816806793 num_feat: 15/38 Reg Loss: 0.04662732407450676 Val Accuracy: 94.44444444444444\n",
      "Epoch 224/1000 Loss: 0.07278437912464142 num_feat: 15/38 Reg Loss: 0.046507205814123154 Val Accuracy: 94.44444444444444\n",
      "Epoch 225/1000 Loss: 0.08675097674131393 num_feat: 14/38 Reg Loss: 0.046386443078517914 Val Accuracy: 94.44444444444444\n",
      "Epoch 226/1000 Loss: 0.0647779256105423 num_feat: 14/38 Reg Loss: 0.04625099152326584 Val Accuracy: 97.22222222222223\n",
      "Epoch 227/1000 Loss: 0.12362173944711685 num_feat: 14/38 Reg Loss: 0.0461217425763607 Val Accuracy: 94.44444444444444\n",
      "Epoch 228/1000 Loss: 0.07959014177322388 num_feat: 14/38 Reg Loss: 0.045959100127220154 Val Accuracy: 94.44444444444444\n",
      "Epoch 229/1000 Loss: 0.17319689691066742 num_feat: 14/38 Reg Loss: 0.04578559845685959 Val Accuracy: 94.44444444444444\n",
      "Epoch 230/1000 Loss: 0.0795789361000061 num_feat: 14/38 Reg Loss: 0.045635465532541275 Val Accuracy: 94.44444444444444\n",
      "Epoch 231/1000 Loss: 0.08901780098676682 num_feat: 14/38 Reg Loss: 0.045503389090299606 Val Accuracy: 94.44444444444444\n",
      "Epoch 232/1000 Loss: 0.08840938657522202 num_feat: 14/38 Reg Loss: 0.04537962004542351 Val Accuracy: 94.44444444444444\n",
      "Epoch 233/1000 Loss: 0.06860367208719254 num_feat: 14/38 Reg Loss: 0.045280322432518005 Val Accuracy: 94.44444444444444\n",
      "Epoch 234/1000 Loss: 0.09476610273122787 num_feat: 14/38 Reg Loss: 0.04518319293856621 Val Accuracy: 97.22222222222223\n",
      "Epoch 235/1000 Loss: 0.11905144155025482 num_feat: 14/38 Reg Loss: 0.04506584256887436 Val Accuracy: 100.0\n",
      "Epoch 236/1000 Loss: 0.08307836204767227 num_feat: 14/38 Reg Loss: 0.044923994690179825 Val Accuracy: 91.66666666666667\n",
      "Epoch 237/1000 Loss: 0.09141065180301666 num_feat: 14/38 Reg Loss: 0.0447688102722168 Val Accuracy: 94.44444444444444\n",
      "Epoch 238/1000 Loss: 0.08474322408437729 num_feat: 14/38 Reg Loss: 0.044603604823350906 Val Accuracy: 94.44444444444444\n",
      "Epoch 239/1000 Loss: 0.06303603947162628 num_feat: 14/38 Reg Loss: 0.04446316510438919 Val Accuracy: 94.44444444444444\n",
      "Epoch 240/1000 Loss: 0.10308269411325455 num_feat: 14/38 Reg Loss: 0.044331882148981094 Val Accuracy: 91.66666666666667\n",
      "Epoch 241/1000 Loss: 0.11181896924972534 num_feat: 14/38 Reg Loss: 0.04421674460172653 Val Accuracy: 94.44444444444444\n",
      "Epoch 242/1000 Loss: 0.14155469834804535 num_feat: 14/38 Reg Loss: 0.04411322996020317 Val Accuracy: 94.44444444444444\n",
      "Epoch 243/1000 Loss: 0.07176624238491058 num_feat: 14/38 Reg Loss: 0.04404853656888008 Val Accuracy: 94.44444444444444\n",
      "Epoch 244/1000 Loss: 0.07679694890975952 num_feat: 13/38 Reg Loss: 0.043989166617393494 Val Accuracy: 97.22222222222223\n",
      "Epoch 245/1000 Loss: 0.27676284313201904 num_feat: 13/38 Reg Loss: 0.043920304626226425 Val Accuracy: 97.22222222222223\n",
      "Epoch 246/1000 Loss: 0.14273792505264282 num_feat: 12/38 Reg Loss: 0.043808337301015854 Val Accuracy: 100.0\n",
      "Epoch 247/1000 Loss: 0.11670611798763275 num_feat: 12/38 Reg Loss: 0.043669164180755615 Val Accuracy: 94.44444444444444\n",
      "Epoch 248/1000 Loss: 0.09145648032426834 num_feat: 12/38 Reg Loss: 0.043495044112205505 Val Accuracy: 91.66666666666667\n",
      "Epoch 249/1000 Loss: 0.13469147682189941 num_feat: 12/38 Reg Loss: 0.04335124418139458 Val Accuracy: 94.44444444444444\n",
      "Epoch 250/1000 Loss: 0.13381998240947723 num_feat: 11/38 Reg Loss: 0.04322097450494766 Val Accuracy: 94.44444444444444\n",
      "Epoch 251/1000 Loss: 0.08674165606498718 num_feat: 11/38 Reg Loss: 0.043131470680236816 Val Accuracy: 94.44444444444444\n",
      "Epoch 252/1000 Loss: 0.07518652081489563 num_feat: 11/38 Reg Loss: 0.04305562376976013 Val Accuracy: 97.22222222222223\n",
      "Epoch 253/1000 Loss: 0.1269194334745407 num_feat: 11/38 Reg Loss: 0.0429832749068737 Val Accuracy: 100.0\n",
      "Epoch 254/1000 Loss: 0.09475535154342651 num_feat: 11/38 Reg Loss: 0.042864840477705 Val Accuracy: 97.22222222222223\n",
      "Epoch 255/1000 Loss: 0.08345919847488403 num_feat: 11/38 Reg Loss: 0.042731400579214096 Val Accuracy: 94.44444444444444\n",
      "Epoch 256/1000 Loss: 0.0992259532213211 num_feat: 11/38 Reg Loss: 0.04259008169174194 Val Accuracy: 94.44444444444444\n",
      "Epoch 257/1000 Loss: 0.057578153908252716 num_feat: 11/38 Reg Loss: 0.04243312403559685 Val Accuracy: 94.44444444444444\n",
      "Epoch 258/1000 Loss: 0.09784968942403793 num_feat: 11/38 Reg Loss: 0.04228520020842552 Val Accuracy: 91.66666666666667\n",
      "Epoch 259/1000 Loss: 0.10048417001962662 num_feat: 11/38 Reg Loss: 0.04213613271713257 Val Accuracy: 88.88888888888889\n",
      "Epoch 260/1000 Loss: 0.2099846601486206 num_feat: 11/38 Reg Loss: 0.04200545325875282 Val Accuracy: 91.66666666666667\n",
      "Epoch 261/1000 Loss: 0.050691258162260056 num_feat: 10/38 Reg Loss: 0.04193495213985443 Val Accuracy: 94.44444444444444\n",
      "Epoch 262/1000 Loss: 0.08412114530801773 num_feat: 10/38 Reg Loss: 0.04187546297907829 Val Accuracy: 94.44444444444444\n",
      "Epoch 263/1000 Loss: 0.051694244146347046 num_feat: 10/38 Reg Loss: 0.041826121509075165 Val Accuracy: 94.44444444444444\n",
      "Epoch 264/1000 Loss: 0.057598214596509933 num_feat: 10/38 Reg Loss: 0.04177822917699814 Val Accuracy: 100.0\n",
      "Epoch 265/1000 Loss: 0.06604379415512085 num_feat: 10/38 Reg Loss: 0.04172810912132263 Val Accuracy: 97.22222222222223\n",
      "Epoch 266/1000 Loss: 0.11300602555274963 num_feat: 10/38 Reg Loss: 0.04166612774133682 Val Accuracy: 97.22222222222223\n",
      "Epoch 267/1000 Loss: 0.11879149824380875 num_feat: 10/38 Reg Loss: 0.04158203303813934 Val Accuracy: 94.44444444444444\n",
      "Epoch 268/1000 Loss: 0.059016693383455276 num_feat: 10/38 Reg Loss: 0.041487209498882294 Val Accuracy: 94.44444444444444\n",
      "Epoch 269/1000 Loss: 0.050224121659994125 num_feat: 10/38 Reg Loss: 0.04139084741473198 Val Accuracy: 94.44444444444444\n",
      "Epoch 270/1000 Loss: 0.06960482150316238 num_feat: 10/38 Reg Loss: 0.041298430413007736 Val Accuracy: 94.44444444444444\n",
      "Epoch 271/1000 Loss: 0.10336893796920776 num_feat: 10/38 Reg Loss: 0.041214894503355026 Val Accuracy: 94.44444444444444\n",
      "Epoch 272/1000 Loss: 0.0722673237323761 num_feat: 10/38 Reg Loss: 0.041148990392684937 Val Accuracy: 94.44444444444444\n",
      "Epoch 273/1000 Loss: 0.052314020693302155 num_feat: 10/38 Reg Loss: 0.04109693691134453 Val Accuracy: 94.44444444444444\n",
      "Epoch 274/1000 Loss: 0.05849561095237732 num_feat: 10/38 Reg Loss: 0.04104474186897278 Val Accuracy: 94.44444444444444\n",
      "Epoch 275/1000 Loss: 0.13195770978927612 num_feat: 10/38 Reg Loss: 0.040997639298439026 Val Accuracy: 94.44444444444444\n",
      "Epoch 276/1000 Loss: 0.05780655890703201 num_feat: 10/38 Reg Loss: 0.04089837521314621 Val Accuracy: 97.22222222222223\n",
      "Epoch 277/1000 Loss: 0.056154847145080566 num_feat: 10/38 Reg Loss: 0.040807049721479416 Val Accuracy: 94.44444444444444\n",
      "Epoch 278/1000 Loss: 0.049848772585392 num_feat: 10/38 Reg Loss: 0.04071997478604317 Val Accuracy: 94.44444444444444\n",
      "Epoch 279/1000 Loss: 0.05066081881523132 num_feat: 10/38 Reg Loss: 0.040636684745550156 Val Accuracy: 94.44444444444444\n",
      "Epoch 280/1000 Loss: 0.05162392184138298 num_feat: 10/38 Reg Loss: 0.04055316001176834 Val Accuracy: 94.44444444444444\n",
      "Epoch 281/1000 Loss: 0.053407397121191025 num_feat: 10/38 Reg Loss: 0.040468938648700714 Val Accuracy: 94.44444444444444\n",
      "Epoch 282/1000 Loss: 0.0442575104534626 num_feat: 10/38 Reg Loss: 0.040394801646471024 Val Accuracy: 94.44444444444444\n",
      "Epoch 283/1000 Loss: 0.11522182822227478 num_feat: 10/38 Reg Loss: 0.040322404354810715 Val Accuracy: 94.44444444444444\n",
      "Epoch 284/1000 Loss: 0.054126206785440445 num_feat: 10/38 Reg Loss: 0.04026443883776665 Val Accuracy: 97.22222222222223\n",
      "Epoch 285/1000 Loss: 0.05796517804265022 num_feat: 10/38 Reg Loss: 0.04020017012953758 Val Accuracy: 94.44444444444444\n",
      "Epoch 286/1000 Loss: 0.05859298259019852 num_feat: 10/38 Reg Loss: 0.04013273864984512 Val Accuracy: 94.44444444444444\n",
      "Epoch 287/1000 Loss: 0.0618242546916008 num_feat: 9/38 Reg Loss: 0.04007038101553917 Val Accuracy: 94.44444444444444\n",
      "Epoch 288/1000 Loss: 0.06758648157119751 num_feat: 9/38 Reg Loss: 0.039996061474084854 Val Accuracy: 94.44444444444444\n",
      "Epoch 289/1000 Loss: 0.04725091531872749 num_feat: 9/38 Reg Loss: 0.039907488971948624 Val Accuracy: 94.44444444444444\n",
      "Epoch 290/1000 Loss: 0.049905866384506226 num_feat: 9/38 Reg Loss: 0.0398225300014019 Val Accuracy: 94.44444444444444\n",
      "Epoch 291/1000 Loss: 0.0826464518904686 num_feat: 9/38 Reg Loss: 0.039734866470098495 Val Accuracy: 94.44444444444444\n",
      "Epoch 292/1000 Loss: 0.05959206447005272 num_feat: 9/38 Reg Loss: 0.03964826837182045 Val Accuracy: 97.22222222222223\n",
      "Epoch 293/1000 Loss: 0.044428084045648575 num_feat: 9/38 Reg Loss: 0.03957004100084305 Val Accuracy: 97.22222222222223\n",
      "Epoch 294/1000 Loss: 0.04589870199561119 num_feat: 9/38 Reg Loss: 0.03949795663356781 Val Accuracy: 94.44444444444444\n",
      "Epoch 295/1000 Loss: 0.046487610787153244 num_feat: 9/38 Reg Loss: 0.03942713886499405 Val Accuracy: 97.22222222222223\n",
      "Epoch 296/1000 Loss: 0.055832669138908386 num_feat: 9/38 Reg Loss: 0.03935779258608818 Val Accuracy: 97.22222222222223\n",
      "Epoch 297/1000 Loss: 0.06191932410001755 num_feat: 9/38 Reg Loss: 0.0392831414937973 Val Accuracy: 94.44444444444444\n",
      "Epoch 298/1000 Loss: 0.04154977574944496 num_feat: 9/38 Reg Loss: 0.03920513018965721 Val Accuracy: 94.44444444444444\n",
      "Epoch 299/1000 Loss: 0.03747476264834404 num_feat: 9/38 Reg Loss: 0.03912721201777458 Val Accuracy: 94.44444444444444\n",
      "Epoch 300/1000 Loss: 0.04265318438410759 num_feat: 9/38 Reg Loss: 0.03905055299401283 Val Accuracy: 94.44444444444444\n",
      "Epoch 301/1000 Loss: 0.08859340846538544 num_feat: 9/38 Reg Loss: 0.0389770083129406 Val Accuracy: 94.44444444444444\n",
      "Epoch 302/1000 Loss: 0.08873704075813293 num_feat: 9/38 Reg Loss: 0.038902245461940765 Val Accuracy: 94.44444444444444\n",
      "Epoch 303/1000 Loss: 0.041305847465991974 num_feat: 9/38 Reg Loss: 0.03885543346405029 Val Accuracy: 94.44444444444444\n",
      "Epoch 304/1000 Loss: 0.06688781827688217 num_feat: 9/38 Reg Loss: 0.03880415856838226 Val Accuracy: 94.44444444444444\n",
      "Epoch 305/1000 Loss: 0.05937875062227249 num_feat: 9/38 Reg Loss: 0.03874296694993973 Val Accuracy: 94.44444444444444\n",
      "Epoch 306/1000 Loss: 0.039461977779865265 num_feat: 9/38 Reg Loss: 0.03866741806268692 Val Accuracy: 94.44444444444444\n",
      "Epoch 307/1000 Loss: 0.03886820748448372 num_feat: 9/38 Reg Loss: 0.038593169301748276 Val Accuracy: 94.44444444444444\n",
      "Epoch 308/1000 Loss: 0.04817679524421692 num_feat: 9/38 Reg Loss: 0.03851858526468277 Val Accuracy: 94.44444444444444\n",
      "Epoch 309/1000 Loss: 0.0662018284201622 num_feat: 9/38 Reg Loss: 0.03846359625458717 Val Accuracy: 94.44444444444444\n",
      "Epoch 310/1000 Loss: 0.04780860245227814 num_feat: 9/38 Reg Loss: 0.038405891507864 Val Accuracy: 94.44444444444444\n",
      "Epoch 311/1000 Loss: 0.04296344518661499 num_feat: 9/38 Reg Loss: 0.038355179131031036 Val Accuracy: 94.44444444444444\n",
      "Epoch 312/1000 Loss: 0.058318160474300385 num_feat: 9/38 Reg Loss: 0.03830532357096672 Val Accuracy: 94.44444444444444\n",
      "Epoch 313/1000 Loss: 0.05234629660844803 num_feat: 9/38 Reg Loss: 0.03825898841023445 Val Accuracy: 97.22222222222223\n",
      "Epoch 314/1000 Loss: 0.058942005038261414 num_feat: 9/38 Reg Loss: 0.038205455988645554 Val Accuracy: 94.44444444444444\n",
      "Epoch 315/1000 Loss: 0.0606466606259346 num_feat: 9/38 Reg Loss: 0.0381433479487896 Val Accuracy: 94.44444444444444\n",
      "Epoch 316/1000 Loss: 0.03089108131825924 num_feat: 9/38 Reg Loss: 0.03807331994175911 Val Accuracy: 94.44444444444444\n",
      "Epoch 317/1000 Loss: 0.1321958750486374 num_feat: 9/38 Reg Loss: 0.0380023755133152 Val Accuracy: 94.44444444444444\n",
      "Epoch 318/1000 Loss: 0.10373737663030624 num_feat: 9/38 Reg Loss: 0.0379503034055233 Val Accuracy: 94.44444444444444\n",
      "Epoch 319/1000 Loss: 0.04568818584084511 num_feat: 9/38 Reg Loss: 0.03790926933288574 Val Accuracy: 97.22222222222223\n",
      "Epoch 320/1000 Loss: 0.047517694532871246 num_feat: 9/38 Reg Loss: 0.03787234053015709 Val Accuracy: 94.44444444444444\n",
      "Epoch 321/1000 Loss: 0.04109092429280281 num_feat: 9/38 Reg Loss: 0.03781864792108536 Val Accuracy: 97.22222222222223\n",
      "Epoch 322/1000 Loss: 0.05875277519226074 num_feat: 9/38 Reg Loss: 0.03776464983820915 Val Accuracy: 97.22222222222223\n",
      "Epoch 323/1000 Loss: 0.03959953039884567 num_feat: 9/38 Reg Loss: 0.037696246057748795 Val Accuracy: 94.44444444444444\n",
      "Epoch 324/1000 Loss: 0.04622673615813255 num_feat: 9/38 Reg Loss: 0.03762662038207054 Val Accuracy: 94.44444444444444\n",
      "Epoch 325/1000 Loss: 0.03264104947447777 num_feat: 9/38 Reg Loss: 0.03754230961203575 Val Accuracy: 94.44444444444444\n",
      "Epoch 326/1000 Loss: 0.08295990526676178 num_feat: 9/38 Reg Loss: 0.03746200352907181 Val Accuracy: 94.44444444444444\n",
      "Epoch 327/1000 Loss: 0.05601757392287254 num_feat: 9/38 Reg Loss: 0.03738017380237579 Val Accuracy: 94.44444444444444\n",
      "Epoch 328/1000 Loss: 0.0371052622795105 num_feat: 9/38 Reg Loss: 0.037318333983421326 Val Accuracy: 97.22222222222223\n",
      "Epoch 329/1000 Loss: 0.03085215389728546 num_feat: 9/38 Reg Loss: 0.037269290536642075 Val Accuracy: 94.44444444444444\n",
      "Epoch 330/1000 Loss: 0.030618121847510338 num_feat: 9/38 Reg Loss: 0.03721586987376213 Val Accuracy: 94.44444444444444\n",
      "Epoch 331/1000 Loss: 0.0372127965092659 num_feat: 9/38 Reg Loss: 0.037163760513067245 Val Accuracy: 100.0\n",
      "Epoch 332/1000 Loss: 0.04384985566139221 num_feat: 9/38 Reg Loss: 0.03710943087935448 Val Accuracy: 94.44444444444444\n",
      "Epoch 333/1000 Loss: 0.059152036905288696 num_feat: 9/38 Reg Loss: 0.03705572709441185 Val Accuracy: 94.44444444444444\n",
      "Epoch 334/1000 Loss: 0.028694218024611473 num_feat: 9/38 Reg Loss: 0.03700309619307518 Val Accuracy: 97.22222222222223\n",
      "Epoch 335/1000 Loss: 0.03799127787351608 num_feat: 9/38 Reg Loss: 0.03694986552000046 Val Accuracy: 94.44444444444444\n",
      "Epoch 336/1000 Loss: 0.032388657331466675 num_feat: 9/38 Reg Loss: 0.03689567744731903 Val Accuracy: 94.44444444444444\n",
      "Epoch 337/1000 Loss: 0.03417978808283806 num_feat: 9/38 Reg Loss: 0.03684046491980553 Val Accuracy: 94.44444444444444\n",
      "Epoch 338/1000 Loss: 0.03998996317386627 num_feat: 9/38 Reg Loss: 0.03677685558795929 Val Accuracy: 94.44444444444444\n",
      "Epoch 339/1000 Loss: 0.03658466786146164 num_feat: 9/38 Reg Loss: 0.0367240272462368 Val Accuracy: 94.44444444444444\n",
      "Epoch 340/1000 Loss: 0.06566285341978073 num_feat: 9/38 Reg Loss: 0.03667697310447693 Val Accuracy: 94.44444444444444\n",
      "Epoch 341/1000 Loss: 0.03904291242361069 num_feat: 9/38 Reg Loss: 0.03662680834531784 Val Accuracy: 94.44444444444444\n",
      "Epoch 342/1000 Loss: 0.030279191210865974 num_feat: 9/38 Reg Loss: 0.036582302302122116 Val Accuracy: 97.22222222222223\n",
      "Epoch 343/1000 Loss: 0.03986891359090805 num_feat: 9/38 Reg Loss: 0.036539558321237564 Val Accuracy: 97.22222222222223\n",
      "Epoch 344/1000 Loss: 0.07544004172086716 num_feat: 9/38 Reg Loss: 0.03649691864848137 Val Accuracy: 94.44444444444444\n",
      "Epoch 345/1000 Loss: 0.029584646224975586 num_feat: 9/38 Reg Loss: 0.03642258420586586 Val Accuracy: 94.44444444444444\n",
      "Epoch 346/1000 Loss: 0.028740255162119865 num_feat: 9/38 Reg Loss: 0.03634992241859436 Val Accuracy: 94.44444444444444\n",
      "Epoch 347/1000 Loss: 0.08440210670232773 num_feat: 9/38 Reg Loss: 0.036281757056713104 Val Accuracy: 94.44444444444444\n",
      "Epoch 348/1000 Loss: 0.029474491253495216 num_feat: 9/38 Reg Loss: 0.03622720390558243 Val Accuracy: 94.44444444444444\n",
      "Epoch 349/1000 Loss: 0.02209087461233139 num_feat: 9/38 Reg Loss: 0.036174651235342026 Val Accuracy: 94.44444444444444\n",
      "Epoch 350/1000 Loss: 0.03466808423399925 num_feat: 9/38 Reg Loss: 0.03612440824508667 Val Accuracy: 97.22222222222223\n",
      "Epoch 351/1000 Loss: 0.06109500676393509 num_feat: 9/38 Reg Loss: 0.03607667610049248 Val Accuracy: 94.44444444444444\n",
      "Epoch 352/1000 Loss: 0.04527025297284126 num_feat: 9/38 Reg Loss: 0.03602001070976257 Val Accuracy: 94.44444444444444\n",
      "Epoch 353/1000 Loss: 0.025242159143090248 num_feat: 9/38 Reg Loss: 0.03595105931162834 Val Accuracy: 94.44444444444444\n",
      "Epoch 354/1000 Loss: 0.04321124777197838 num_feat: 8/38 Reg Loss: 0.035884175449609756 Val Accuracy: 94.44444444444444\n",
      "Epoch 355/1000 Loss: 0.05819792300462723 num_feat: 8/38 Reg Loss: 0.03582270070910454 Val Accuracy: 94.44444444444444\n",
      "Epoch 356/1000 Loss: 0.05238525569438934 num_feat: 8/38 Reg Loss: 0.03575890138745308 Val Accuracy: 94.44444444444444\n",
      "Epoch 357/1000 Loss: 0.026015011593699455 num_feat: 8/38 Reg Loss: 0.03569905087351799 Val Accuracy: 94.44444444444444\n",
      "Epoch 358/1000 Loss: 0.03052639588713646 num_feat: 8/38 Reg Loss: 0.035639308393001556 Val Accuracy: 94.44444444444444\n",
      "Epoch 359/1000 Loss: 0.028281982988119125 num_feat: 8/38 Reg Loss: 0.035578422248363495 Val Accuracy: 97.22222222222223\n",
      "Epoch 360/1000 Loss: 0.03310965746641159 num_feat: 8/38 Reg Loss: 0.035519570112228394 Val Accuracy: 97.22222222222223\n",
      "Epoch 361/1000 Loss: 0.029976705089211464 num_feat: 8/38 Reg Loss: 0.03546490892767906 Val Accuracy: 94.44444444444444\n",
      "Epoch 362/1000 Loss: 0.027585478499531746 num_feat: 8/38 Reg Loss: 0.0354059599339962 Val Accuracy: 94.44444444444444\n",
      "Epoch 363/1000 Loss: 0.02139691449701786 num_feat: 8/38 Reg Loss: 0.035348519682884216 Val Accuracy: 94.44444444444444\n",
      "Epoch 364/1000 Loss: 0.027293989434838295 num_feat: 8/38 Reg Loss: 0.035292185842990875 Val Accuracy: 94.44444444444444\n",
      "Epoch 365/1000 Loss: 0.05312957242131233 num_feat: 8/38 Reg Loss: 0.03524012118577957 Val Accuracy: 94.44444444444444\n",
      "Epoch 366/1000 Loss: 0.025816738605499268 num_feat: 8/38 Reg Loss: 0.03519795089960098 Val Accuracy: 94.44444444444444\n",
      "Epoch 367/1000 Loss: 0.029977723956108093 num_feat: 8/38 Reg Loss: 0.03515470027923584 Val Accuracy: 94.44444444444444\n",
      "Epoch 368/1000 Loss: 0.02271188050508499 num_feat: 8/38 Reg Loss: 0.03511909767985344 Val Accuracy: 94.44444444444444\n",
      "Epoch 369/1000 Loss: 0.024266764521598816 num_feat: 8/38 Reg Loss: 0.03508482873439789 Val Accuracy: 94.44444444444444\n",
      "Epoch 370/1000 Loss: 0.025514336302876472 num_feat: 8/38 Reg Loss: 0.03505045920610428 Val Accuracy: 94.44444444444444\n",
      "Epoch 371/1000 Loss: 0.023396316915750504 num_feat: 8/38 Reg Loss: 0.03501301631331444 Val Accuracy: 94.44444444444444\n",
      "Epoch 372/1000 Loss: 0.04814092442393303 num_feat: 8/38 Reg Loss: 0.03497719019651413 Val Accuracy: 94.44444444444444\n",
      "Epoch 373/1000 Loss: 0.025308016687631607 num_feat: 8/38 Reg Loss: 0.03492410108447075 Val Accuracy: 94.44444444444444\n",
      "Epoch 374/1000 Loss: 0.025882139801979065 num_feat: 8/38 Reg Loss: 0.03486209362745285 Val Accuracy: 94.44444444444444\n",
      "Epoch 375/1000 Loss: 0.0581764318048954 num_feat: 8/38 Reg Loss: 0.034802936017513275 Val Accuracy: 94.44444444444444\n",
      "Epoch 376/1000 Loss: 0.019892603158950806 num_feat: 8/38 Reg Loss: 0.03474144637584686 Val Accuracy: 94.44444444444444\n",
      "Epoch 377/1000 Loss: 0.028297631070017815 num_feat: 8/38 Reg Loss: 0.03468184545636177 Val Accuracy: 94.44444444444444\n",
      "Epoch 378/1000 Loss: 0.026235954836010933 num_feat: 8/38 Reg Loss: 0.03461477532982826 Val Accuracy: 94.44444444444444\n",
      "Epoch 379/1000 Loss: 0.02164744958281517 num_feat: 8/38 Reg Loss: 0.034548114985227585 Val Accuracy: 94.44444444444444\n",
      "Epoch 380/1000 Loss: 0.021665044128894806 num_feat: 8/38 Reg Loss: 0.034487538039684296 Val Accuracy: 94.44444444444444\n",
      "Epoch 381/1000 Loss: 0.04477473348379135 num_feat: 8/38 Reg Loss: 0.03442373126745224 Val Accuracy: 94.44444444444444\n",
      "Epoch 382/1000 Loss: 0.03738544508814812 num_feat: 8/38 Reg Loss: 0.03434842452406883 Val Accuracy: 97.22222222222223\n",
      "Epoch 383/1000 Loss: 0.03289486840367317 num_feat: 8/38 Reg Loss: 0.034261416643857956 Val Accuracy: 94.44444444444444\n",
      "Epoch 384/1000 Loss: 0.03565702959895134 num_feat: 8/38 Reg Loss: 0.03418298810720444 Val Accuracy: 94.44444444444444\n",
      "Epoch 385/1000 Loss: 0.023907426744699478 num_feat: 8/38 Reg Loss: 0.034112635999917984 Val Accuracy: 94.44444444444444\n",
      "Epoch 386/1000 Loss: 0.01630035974085331 num_feat: 8/38 Reg Loss: 0.034050602465867996 Val Accuracy: 94.44444444444444\n",
      "Epoch 387/1000 Loss: 0.021025653928518295 num_feat: 8/38 Reg Loss: 0.03398975357413292 Val Accuracy: 94.44444444444444\n",
      "Epoch 388/1000 Loss: 0.024469880387187004 num_feat: 8/38 Reg Loss: 0.03392970189452171 Val Accuracy: 94.44444444444444\n",
      "Epoch 389/1000 Loss: 0.01724916882812977 num_feat: 8/38 Reg Loss: 0.03386526554822922 Val Accuracy: 97.22222222222223\n",
      "Epoch 390/1000 Loss: 0.09283099323511124 num_feat: 8/38 Reg Loss: 0.033804114907979965 Val Accuracy: 94.44444444444444\n",
      "Epoch 391/1000 Loss: 0.021619753912091255 num_feat: 8/38 Reg Loss: 0.033725555986166 Val Accuracy: 94.44444444444444\n",
      "Epoch 392/1000 Loss: 0.036356259137392044 num_feat: 8/38 Reg Loss: 0.033652182668447495 Val Accuracy: 94.44444444444444\n",
      "Epoch 393/1000 Loss: 0.025606544688344002 num_feat: 8/38 Reg Loss: 0.0335879884660244 Val Accuracy: 94.44444444444444\n",
      "Epoch 394/1000 Loss: 0.02699930965900421 num_feat: 8/38 Reg Loss: 0.033532094210386276 Val Accuracy: 94.44444444444444\n",
      "Epoch 395/1000 Loss: 0.026433447375893593 num_feat: 8/38 Reg Loss: 0.03348594531416893 Val Accuracy: 94.44444444444444\n",
      "Epoch 396/1000 Loss: 0.018241826444864273 num_feat: 8/38 Reg Loss: 0.03343920782208443 Val Accuracy: 94.44444444444444\n",
      "Epoch 397/1000 Loss: 0.051743585616350174 num_feat: 8/38 Reg Loss: 0.03339448198676109 Val Accuracy: 94.44444444444444\n",
      "Epoch 398/1000 Loss: 0.09404624998569489 num_feat: 8/38 Reg Loss: 0.033333610743284225 Val Accuracy: 94.44444444444444\n",
      "Epoch 399/1000 Loss: 0.06343007832765579 num_feat: 8/38 Reg Loss: 0.03325511887669563 Val Accuracy: 94.44444444444444\n",
      "Epoch 400/1000 Loss: 0.027529194951057434 num_feat: 8/38 Reg Loss: 0.033189695328474045 Val Accuracy: 94.44444444444444\n",
      "Epoch 401/1000 Loss: 0.02109554037451744 num_feat: 8/38 Reg Loss: 0.03313290700316429 Val Accuracy: 94.44444444444444\n",
      "Epoch 402/1000 Loss: 0.02507440187036991 num_feat: 8/38 Reg Loss: 0.03308401256799698 Val Accuracy: 94.44444444444444\n",
      "Epoch 403/1000 Loss: 0.04869236797094345 num_feat: 8/38 Reg Loss: 0.033036407083272934 Val Accuracy: 94.44444444444444\n",
      "Epoch 404/1000 Loss: 0.01921682618558407 num_feat: 8/38 Reg Loss: 0.032980259507894516 Val Accuracy: 94.44444444444444\n",
      "Epoch 405/1000 Loss: 0.028504017740488052 num_feat: 8/38 Reg Loss: 0.0329248271882534 Val Accuracy: 94.44444444444444\n",
      "Epoch 406/1000 Loss: 0.020757993683218956 num_feat: 8/38 Reg Loss: 0.03286551684141159 Val Accuracy: 94.44444444444444\n",
      "Epoch 407/1000 Loss: 0.028204599395394325 num_feat: 8/38 Reg Loss: 0.03280855342745781 Val Accuracy: 94.44444444444444\n",
      "Epoch 408/1000 Loss: 0.042000748217105865 num_feat: 8/38 Reg Loss: 0.03275279328227043 Val Accuracy: 94.44444444444444\n",
      "Epoch 409/1000 Loss: 0.01802457682788372 num_feat: 8/38 Reg Loss: 0.032704975455999374 Val Accuracy: 94.44444444444444\n",
      "Epoch 410/1000 Loss: 0.03057064302265644 num_feat: 8/38 Reg Loss: 0.03265805169939995 Val Accuracy: 94.44444444444444\n",
      "Epoch 411/1000 Loss: 0.018075136467814445 num_feat: 8/38 Reg Loss: 0.03260655328631401 Val Accuracy: 94.44444444444444\n",
      "Epoch 412/1000 Loss: 0.01961888186633587 num_feat: 8/38 Reg Loss: 0.032556306570768356 Val Accuracy: 97.22222222222223\n",
      "Epoch 413/1000 Loss: 0.02140122279524803 num_feat: 8/38 Reg Loss: 0.03250642493367195 Val Accuracy: 94.44444444444444\n",
      "Epoch 414/1000 Loss: 0.016529282554984093 num_feat: 8/38 Reg Loss: 0.03245631977915764 Val Accuracy: 94.44444444444444\n",
      "Epoch 415/1000 Loss: 0.02030637487769127 num_feat: 8/38 Reg Loss: 0.032407037913799286 Val Accuracy: 94.44444444444444\n",
      "Epoch 416/1000 Loss: 0.03847382590174675 num_feat: 8/38 Reg Loss: 0.03235825523734093 Val Accuracy: 94.44444444444444\n",
      "Epoch 417/1000 Loss: 0.03569091483950615 num_feat: 8/38 Reg Loss: 0.03230678290128708 Val Accuracy: 94.44444444444444\n",
      "Epoch 418/1000 Loss: 0.02103947475552559 num_feat: 8/38 Reg Loss: 0.032253384590148926 Val Accuracy: 94.44444444444444\n",
      "Epoch 419/1000 Loss: 0.044652074575424194 num_feat: 8/38 Reg Loss: 0.032200440764427185 Val Accuracy: 94.44444444444444\n",
      "Epoch 420/1000 Loss: 0.02477119117975235 num_feat: 8/38 Reg Loss: 0.03214207664132118 Val Accuracy: 94.44444444444444\n",
      "Epoch 421/1000 Loss: 0.01640312373638153 num_feat: 8/38 Reg Loss: 0.0320848748087883 Val Accuracy: 94.44444444444444\n",
      "Epoch 422/1000 Loss: 0.01541694812476635 num_feat: 8/38 Reg Loss: 0.032029565423727036 Val Accuracy: 94.44444444444444\n",
      "Epoch 423/1000 Loss: 0.015235141851007938 num_feat: 8/38 Reg Loss: 0.03197542205452919 Val Accuracy: 94.44444444444444\n",
      "Epoch 424/1000 Loss: 0.022693371400237083 num_feat: 8/38 Reg Loss: 0.03192144259810448 Val Accuracy: 94.44444444444444\n",
      "Epoch 425/1000 Loss: 0.016865139827132225 num_feat: 8/38 Reg Loss: 0.03186715766787529 Val Accuracy: 94.44444444444444\n",
      "Epoch 426/1000 Loss: 0.01890433207154274 num_feat: 8/38 Reg Loss: 0.03181426599621773 Val Accuracy: 94.44444444444444\n",
      "Epoch 427/1000 Loss: 0.01786573976278305 num_feat: 8/38 Reg Loss: 0.03176623955368996 Val Accuracy: 94.44444444444444\n",
      "Epoch 428/1000 Loss: 0.01460363157093525 num_feat: 8/38 Reg Loss: 0.031716495752334595 Val Accuracy: 94.44444444444444\n",
      "Epoch 429/1000 Loss: 0.017939135432243347 num_feat: 8/38 Reg Loss: 0.03166848048567772 Val Accuracy: 94.44444444444444\n",
      "Epoch 430/1000 Loss: 0.020652785897254944 num_feat: 8/38 Reg Loss: 0.031622543931007385 Val Accuracy: 94.44444444444444\n",
      "Epoch 431/1000 Loss: 0.015257783234119415 num_feat: 8/38 Reg Loss: 0.03157863765954971 Val Accuracy: 94.44444444444444\n",
      "Epoch 432/1000 Loss: 0.012363720685243607 num_feat: 8/38 Reg Loss: 0.03153436258435249 Val Accuracy: 94.44444444444444\n",
      "Epoch 433/1000 Loss: 0.013386398553848267 num_feat: 8/38 Reg Loss: 0.031490784138441086 Val Accuracy: 94.44444444444444\n",
      "Epoch 434/1000 Loss: 0.014807023108005524 num_feat: 8/38 Reg Loss: 0.03144853189587593 Val Accuracy: 94.44444444444444\n",
      "Epoch 435/1000 Loss: 0.018594840541481972 num_feat: 8/38 Reg Loss: 0.031406257301568985 Val Accuracy: 94.44444444444444\n",
      "Epoch 436/1000 Loss: 0.028491463512182236 num_feat: 8/38 Reg Loss: 0.031362976878881454 Val Accuracy: 94.44444444444444\n",
      "Epoch 437/1000 Loss: 0.014654682017862797 num_feat: 8/38 Reg Loss: 0.0313166044652462 Val Accuracy: 94.44444444444444\n",
      "Epoch 438/1000 Loss: 0.031057581305503845 num_feat: 8/38 Reg Loss: 0.031270720064640045 Val Accuracy: 94.44444444444444\n",
      "Epoch 439/1000 Loss: 0.01420918945223093 num_feat: 8/38 Reg Loss: 0.03121882490813732 Val Accuracy: 94.44444444444444\n",
      "Epoch 440/1000 Loss: 0.012735382653772831 num_feat: 8/38 Reg Loss: 0.031166940927505493 Val Accuracy: 94.44444444444444\n",
      "Epoch 441/1000 Loss: 0.018586283549666405 num_feat: 8/38 Reg Loss: 0.03111676312983036 Val Accuracy: 94.44444444444444\n",
      "Epoch 442/1000 Loss: 0.026411661878228188 num_feat: 8/38 Reg Loss: 0.031067049130797386 Val Accuracy: 94.44444444444444\n",
      "Epoch 443/1000 Loss: 0.011706069111824036 num_feat: 8/38 Reg Loss: 0.031018784269690514 Val Accuracy: 94.44444444444444\n",
      "Epoch 444/1000 Loss: 0.013522269204258919 num_feat: 8/38 Reg Loss: 0.03097163699567318 Val Accuracy: 94.44444444444444\n",
      "Epoch 445/1000 Loss: 0.014284290373325348 num_feat: 8/38 Reg Loss: 0.03092542663216591 Val Accuracy: 94.44444444444444\n",
      "Epoch 446/1000 Loss: 0.03854973986744881 num_feat: 8/38 Reg Loss: 0.03088105283677578 Val Accuracy: 94.44444444444444\n",
      "Epoch 447/1000 Loss: 0.010907013900578022 num_feat: 8/38 Reg Loss: 0.03083953633904457 Val Accuracy: 94.44444444444444\n",
      "Epoch 448/1000 Loss: 0.011820374988019466 num_feat: 8/38 Reg Loss: 0.030799364671111107 Val Accuracy: 94.44444444444444\n",
      "Epoch 449/1000 Loss: 0.019191144034266472 num_feat: 8/38 Reg Loss: 0.030760420486330986 Val Accuracy: 94.44444444444444\n",
      "Epoch 450/1000 Loss: 0.010394503362476826 num_feat: 8/38 Reg Loss: 0.030722681432962418 Val Accuracy: 94.44444444444444\n",
      "Epoch 451/1000 Loss: 0.010866322554647923 num_feat: 8/38 Reg Loss: 0.030686650425195694 Val Accuracy: 94.44444444444444\n",
      "Epoch 452/1000 Loss: 0.01370923686772585 num_feat: 8/38 Reg Loss: 0.030651355162262917 Val Accuracy: 94.44444444444444\n",
      "Epoch 453/1000 Loss: 0.01265315804630518 num_feat: 8/38 Reg Loss: 0.0306178480386734 Val Accuracy: 94.44444444444444\n",
      "Epoch 454/1000 Loss: 0.013364361599087715 num_feat: 8/38 Reg Loss: 0.030584553256630898 Val Accuracy: 94.44444444444444\n",
      "Epoch 455/1000 Loss: 0.01561155915260315 num_feat: 8/38 Reg Loss: 0.030550966039299965 Val Accuracy: 94.44444444444444\n",
      "Epoch 456/1000 Loss: 0.011994118802249432 num_feat: 8/38 Reg Loss: 0.030515605583786964 Val Accuracy: 94.44444444444444\n",
      "Epoch 457/1000 Loss: 0.012442399747669697 num_feat: 8/38 Reg Loss: 0.03048027493059635 Val Accuracy: 94.44444444444444\n",
      "Epoch 458/1000 Loss: 0.010722939856350422 num_feat: 8/38 Reg Loss: 0.030444195494055748 Val Accuracy: 94.44444444444444\n",
      "Epoch 459/1000 Loss: 0.02620595693588257 num_feat: 8/38 Reg Loss: 0.030408678576350212 Val Accuracy: 94.44444444444444\n",
      "Epoch 460/1000 Loss: 0.011124487966299057 num_feat: 8/38 Reg Loss: 0.03037162497639656 Val Accuracy: 94.44444444444444\n",
      "Epoch 461/1000 Loss: 0.02150573953986168 num_feat: 8/38 Reg Loss: 0.030335664749145508 Val Accuracy: 94.44444444444444\n",
      "Epoch 462/1000 Loss: 0.009738313034176826 num_feat: 8/38 Reg Loss: 0.030295392498373985 Val Accuracy: 94.44444444444444\n",
      "Epoch 463/1000 Loss: 0.01224503107368946 num_feat: 8/38 Reg Loss: 0.03025655448436737 Val Accuracy: 94.44444444444444\n",
      "Epoch 464/1000 Loss: 0.009940393269062042 num_feat: 8/38 Reg Loss: 0.030216673389077187 Val Accuracy: 94.44444444444444\n",
      "Epoch 465/1000 Loss: 0.010947014205157757 num_feat: 8/38 Reg Loss: 0.03017744980752468 Val Accuracy: 94.44444444444444\n",
      "Epoch 466/1000 Loss: 0.009827862493693829 num_feat: 8/38 Reg Loss: 0.03013947606086731 Val Accuracy: 94.44444444444444\n",
      "Epoch 467/1000 Loss: 0.010858910158276558 num_feat: 8/38 Reg Loss: 0.030101895332336426 Val Accuracy: 94.44444444444444\n",
      "Epoch 468/1000 Loss: 0.061251409351825714 num_feat: 8/38 Reg Loss: 0.030065003782510757 Val Accuracy: 94.44444444444444\n",
      "Epoch 469/1000 Loss: 0.01094132848083973 num_feat: 8/38 Reg Loss: 0.03003556840121746 Val Accuracy: 94.44444444444444\n",
      "Epoch 470/1000 Loss: 0.033130042254924774 num_feat: 8/38 Reg Loss: 0.030005618929862976 Val Accuracy: 94.44444444444444\n",
      "Epoch 471/1000 Loss: 0.02390446327626705 num_feat: 8/38 Reg Loss: 0.029973287135362625 Val Accuracy: 94.44444444444444\n",
      "Epoch 472/1000 Loss: 0.011849894188344479 num_feat: 8/38 Reg Loss: 0.02994101122021675 Val Accuracy: 94.44444444444444\n",
      "Epoch 473/1000 Loss: 0.009237143211066723 num_feat: 8/38 Reg Loss: 0.029908476397395134 Val Accuracy: 94.44444444444444\n",
      "Epoch 474/1000 Loss: 0.026384444907307625 num_feat: 8/38 Reg Loss: 0.029876787215471268 Val Accuracy: 94.44444444444444\n",
      "Epoch 475/1000 Loss: 0.0299462229013443 num_feat: 8/38 Reg Loss: 0.029844334349036217 Val Accuracy: 94.44444444444444\n",
      "Epoch 476/1000 Loss: 0.1259884387254715 num_feat: 8/38 Reg Loss: 0.029817471280694008 Val Accuracy: 94.44444444444444\n",
      "Epoch 477/1000 Loss: 0.031528886407613754 num_feat: 8/38 Reg Loss: 0.029792889952659607 Val Accuracy: 94.44444444444444\n",
      "Epoch 478/1000 Loss: 0.08344610780477524 num_feat: 8/38 Reg Loss: 0.02976447157561779 Val Accuracy: 94.44444444444444\n",
      "Epoch 479/1000 Loss: 0.0764179602265358 num_feat: 8/38 Reg Loss: 0.029734691604971886 Val Accuracy: 94.44444444444444\n",
      "Epoch 480/1000 Loss: 0.010902856476604939 num_feat: 8/38 Reg Loss: 0.02969902567565441 Val Accuracy: 94.44444444444444\n",
      "Epoch 481/1000 Loss: 0.02063804678618908 num_feat: 8/38 Reg Loss: 0.029664119705557823 Val Accuracy: 94.44444444444444\n",
      "Epoch 482/1000 Loss: 0.052700456231832504 num_feat: 8/38 Reg Loss: 0.029629239812493324 Val Accuracy: 94.44444444444444\n",
      "Epoch 483/1000 Loss: 0.058764632791280746 num_feat: 8/38 Reg Loss: 0.029599064961075783 Val Accuracy: 94.44444444444444\n",
      "Epoch 484/1000 Loss: 0.01134142279624939 num_feat: 8/38 Reg Loss: 0.02957754395902157 Val Accuracy: 94.44444444444444\n",
      "Epoch 485/1000 Loss: 0.0331558957695961 num_feat: 8/38 Reg Loss: 0.029554450884461403 Val Accuracy: 94.44444444444444\n",
      "Epoch 486/1000 Loss: 0.03654918819665909 num_feat: 8/38 Reg Loss: 0.02953001670539379 Val Accuracy: 94.44444444444444\n",
      "Epoch 487/1000 Loss: 0.06388428062200546 num_feat: 8/38 Reg Loss: 0.02950328029692173 Val Accuracy: 94.44444444444444\n",
      "Epoch 488/1000 Loss: 0.011460547335445881 num_feat: 8/38 Reg Loss: 0.029470214620232582 Val Accuracy: 94.44444444444444\n",
      "Epoch 489/1000 Loss: 0.018717674538493156 num_feat: 8/38 Reg Loss: 0.02943650260567665 Val Accuracy: 94.44444444444444\n",
      "Epoch 490/1000 Loss: 0.03646137937903404 num_feat: 8/38 Reg Loss: 0.02940252423286438 Val Accuracy: 94.44444444444444\n",
      "Epoch 491/1000 Loss: 0.04783669486641884 num_feat: 8/38 Reg Loss: 0.02936405874788761 Val Accuracy: 94.44444444444444\n",
      "Epoch 492/1000 Loss: 0.01049207616597414 num_feat: 8/38 Reg Loss: 0.0293246041983366 Val Accuracy: 94.44444444444444\n",
      "Epoch 493/1000 Loss: 0.021221712231636047 num_feat: 8/38 Reg Loss: 0.02928626537322998 Val Accuracy: 94.44444444444444\n",
      "Epoch 494/1000 Loss: 0.03360350802540779 num_feat: 8/38 Reg Loss: 0.029248986393213272 Val Accuracy: 94.44444444444444\n",
      "Epoch 495/1000 Loss: 0.017965367063879967 num_feat: 8/38 Reg Loss: 0.0292107742279768 Val Accuracy: 94.44444444444444\n",
      "Epoch 496/1000 Loss: 0.021478381007909775 num_feat: 8/38 Reg Loss: 0.02917826548218727 Val Accuracy: 94.44444444444444\n",
      "Epoch 497/1000 Loss: 0.011165371164679527 num_feat: 8/38 Reg Loss: 0.029145140200853348 Val Accuracy: 94.44444444444444\n",
      "Epoch 498/1000 Loss: 0.033327750861644745 num_feat: 8/38 Reg Loss: 0.029111986979842186 Val Accuracy: 94.44444444444444\n",
      "Epoch 499/1000 Loss: 0.020549628883600235 num_feat: 8/38 Reg Loss: 0.02908051386475563 Val Accuracy: 94.44444444444444\n",
      "Epoch 500/1000 Loss: 0.007488109637051821 num_feat: 8/38 Reg Loss: 0.02904984913766384 Val Accuracy: 94.44444444444444\n",
      "Epoch 501/1000 Loss: 0.009214379824697971 num_feat: 8/38 Reg Loss: 0.029020128771662712 Val Accuracy: 94.44444444444444\n",
      "Epoch 502/1000 Loss: 0.014443915337324142 num_feat: 8/38 Reg Loss: 0.02899058535695076 Val Accuracy: 94.44444444444444\n",
      "Epoch 503/1000 Loss: 0.02167549543082714 num_feat: 8/38 Reg Loss: 0.028965095058083534 Val Accuracy: 94.44444444444444\n",
      "Epoch 504/1000 Loss: 0.028122946619987488 num_feat: 8/38 Reg Loss: 0.02894190326333046 Val Accuracy: 94.44444444444444\n",
      "Epoch 505/1000 Loss: 0.017031241208314896 num_feat: 8/38 Reg Loss: 0.028915762901306152 Val Accuracy: 94.44444444444444\n",
      "Epoch 506/1000 Loss: 0.009695632383227348 num_feat: 8/38 Reg Loss: 0.028889214619994164 Val Accuracy: 94.44444444444444\n",
      "Epoch 507/1000 Loss: 0.01933949626982212 num_feat: 8/38 Reg Loss: 0.028863167390227318 Val Accuracy: 94.44444444444444\n",
      "Epoch 508/1000 Loss: 0.005897713825106621 num_feat: 8/38 Reg Loss: 0.028836464509367943 Val Accuracy: 94.44444444444444\n",
      "Epoch 509/1000 Loss: 0.037880364805459976 num_feat: 8/38 Reg Loss: 0.02880929782986641 Val Accuracy: 94.44444444444444\n",
      "Epoch 510/1000 Loss: 0.008108776062726974 num_feat: 8/38 Reg Loss: 0.02878624200820923 Val Accuracy: 94.44444444444444\n",
      "Epoch 511/1000 Loss: 0.007785140071064234 num_feat: 8/38 Reg Loss: 0.028764307498931885 Val Accuracy: 94.44444444444444\n",
      "Epoch 512/1000 Loss: 0.017189348116517067 num_feat: 8/38 Reg Loss: 0.02874184586107731 Val Accuracy: 94.44444444444444\n",
      "Epoch 513/1000 Loss: 0.023289088159799576 num_feat: 8/38 Reg Loss: 0.028718581423163414 Val Accuracy: 94.44444444444444\n",
      "Epoch 514/1000 Loss: 0.017601020634174347 num_feat: 8/38 Reg Loss: 0.028694579377770424 Val Accuracy: 94.44444444444444\n",
      "Epoch 515/1000 Loss: 0.012782945297658443 num_feat: 8/38 Reg Loss: 0.028670473024249077 Val Accuracy: 94.44444444444444\n",
      "Epoch 516/1000 Loss: 0.010136035270988941 num_feat: 8/38 Reg Loss: 0.028642917051911354 Val Accuracy: 94.44444444444444\n",
      "Epoch 517/1000 Loss: 0.00883922167122364 num_feat: 8/38 Reg Loss: 0.028615731745958328 Val Accuracy: 94.44444444444444\n",
      "Epoch 518/1000 Loss: 0.04389192909002304 num_feat: 8/38 Reg Loss: 0.028593946248292923 Val Accuracy: 94.44444444444444\n",
      "Epoch 519/1000 Loss: 0.01118245255202055 num_feat: 8/38 Reg Loss: 0.028569787740707397 Val Accuracy: 94.44444444444444\n",
      "Epoch 520/1000 Loss: 0.007635508198291063 num_feat: 8/38 Reg Loss: 0.028547927737236023 Val Accuracy: 94.44444444444444\n",
      "Epoch 521/1000 Loss: 0.011521292850375175 num_feat: 8/38 Reg Loss: 0.028525978326797485 Val Accuracy: 94.44444444444444\n",
      "Epoch 522/1000 Loss: 0.062056370079517365 num_feat: 8/38 Reg Loss: 0.028505221009254456 Val Accuracy: 94.44444444444444\n",
      "Epoch 523/1000 Loss: 0.016883207485079765 num_feat: 8/38 Reg Loss: 0.028493015095591545 Val Accuracy: 94.44444444444444\n",
      "Epoch 524/1000 Loss: 0.02323838137090206 num_feat: 8/38 Reg Loss: 0.028474656865000725 Val Accuracy: 94.44444444444444\n",
      "Epoch 525/1000 Loss: 0.02406041882932186 num_feat: 8/38 Reg Loss: 0.028454428538680077 Val Accuracy: 94.44444444444444\n",
      "Epoch 526/1000 Loss: 0.02196233905851841 num_feat: 8/38 Reg Loss: 0.0284364465624094 Val Accuracy: 94.44444444444444\n",
      "Epoch 527/1000 Loss: 0.011006551794707775 num_feat: 8/38 Reg Loss: 0.02842198871076107 Val Accuracy: 94.44444444444444\n",
      "Epoch 528/1000 Loss: 0.00787352491170168 num_feat: 8/38 Reg Loss: 0.02840648591518402 Val Accuracy: 94.44444444444444\n",
      "Epoch 529/1000 Loss: 0.034600816667079926 num_feat: 8/38 Reg Loss: 0.028390010818839073 Val Accuracy: 94.44444444444444\n",
      "Epoch 530/1000 Loss: 0.011097751557826996 num_feat: 8/38 Reg Loss: 0.028364552184939384 Val Accuracy: 94.44444444444444\n",
      "Epoch 531/1000 Loss: 0.009312096051871777 num_feat: 8/38 Reg Loss: 0.02833729051053524 Val Accuracy: 94.44444444444444\n",
      "Epoch 532/1000 Loss: 0.006412227172404528 num_feat: 8/38 Reg Loss: 0.028310006484389305 Val Accuracy: 94.44444444444444\n",
      "Epoch 533/1000 Loss: 0.01128709688782692 num_feat: 8/38 Reg Loss: 0.028283188119530678 Val Accuracy: 94.44444444444444\n",
      "Epoch 534/1000 Loss: 0.0055580404587090015 num_feat: 8/38 Reg Loss: 0.028259415179491043 Val Accuracy: 94.44444444444444\n",
      "Epoch 535/1000 Loss: 0.018483953550457954 num_feat: 8/38 Reg Loss: 0.02823563478887081 Val Accuracy: 94.44444444444444\n",
      "Epoch 536/1000 Loss: 0.01490184385329485 num_feat: 8/38 Reg Loss: 0.028212985023856163 Val Accuracy: 94.44444444444444\n",
      "Epoch 537/1000 Loss: 0.007783976849168539 num_feat: 8/38 Reg Loss: 0.028194358572363853 Val Accuracy: 94.44444444444444\n",
      "Epoch 538/1000 Loss: 0.0070123011246323586 num_feat: 8/38 Reg Loss: 0.02817402593791485 Val Accuracy: 94.44444444444444\n",
      "Epoch 539/1000 Loss: 0.010768494568765163 num_feat: 8/38 Reg Loss: 0.028153030201792717 Val Accuracy: 94.44444444444444\n",
      "Epoch 540/1000 Loss: 0.023274680599570274 num_feat: 8/38 Reg Loss: 0.02813166193664074 Val Accuracy: 94.44444444444444\n",
      "Epoch 541/1000 Loss: 0.010126891545951366 num_feat: 8/38 Reg Loss: 0.028110161423683167 Val Accuracy: 94.44444444444444\n",
      "Epoch 542/1000 Loss: 0.006362601183354855 num_feat: 8/38 Reg Loss: 0.02808837592601776 Val Accuracy: 94.44444444444444\n",
      "Epoch 543/1000 Loss: 0.009137757122516632 num_feat: 8/38 Reg Loss: 0.028066489845514297 Val Accuracy: 94.44444444444444\n",
      "Epoch 544/1000 Loss: 0.00558527372777462 num_feat: 8/38 Reg Loss: 0.02804410457611084 Val Accuracy: 94.44444444444444\n",
      "Epoch 545/1000 Loss: 0.014395509846508503 num_feat: 8/38 Reg Loss: 0.02802184782922268 Val Accuracy: 94.44444444444444\n",
      "Epoch 546/1000 Loss: 0.015311076305806637 num_feat: 8/38 Reg Loss: 0.027995770797133446 Val Accuracy: 94.44444444444444\n",
      "Epoch 547/1000 Loss: 0.006289208307862282 num_feat: 8/38 Reg Loss: 0.0279696024954319 Val Accuracy: 94.44444444444444\n",
      "Epoch 548/1000 Loss: 0.00655319495126605 num_feat: 8/38 Reg Loss: 0.02794358693063259 Val Accuracy: 94.44444444444444\n",
      "Epoch 549/1000 Loss: 0.007533606141805649 num_feat: 8/38 Reg Loss: 0.02791784703731537 Val Accuracy: 94.44444444444444\n",
      "Epoch 550/1000 Loss: 0.034966226667165756 num_feat: 8/38 Reg Loss: 0.02789382077753544 Val Accuracy: 94.44444444444444\n",
      "Epoch 551/1000 Loss: 0.006119637284427881 num_feat: 8/38 Reg Loss: 0.027870064601302147 Val Accuracy: 94.44444444444444\n",
      "Epoch 552/1000 Loss: 0.007351086940616369 num_feat: 8/38 Reg Loss: 0.027845842763781548 Val Accuracy: 94.44444444444444\n",
      "Epoch 553/1000 Loss: 0.006996763404458761 num_feat: 8/38 Reg Loss: 0.027821218594908714 Val Accuracy: 94.44444444444444\n",
      "Epoch 554/1000 Loss: 0.02552037313580513 num_feat: 8/38 Reg Loss: 0.0277975145727396 Val Accuracy: 94.44444444444444\n",
      "Epoch 555/1000 Loss: 0.00477722380310297 num_feat: 8/38 Reg Loss: 0.0277766864746809 Val Accuracy: 94.44444444444444\n",
      "Epoch 556/1000 Loss: 0.0063071479089558125 num_feat: 8/38 Reg Loss: 0.027756202965974808 Val Accuracy: 94.44444444444444\n",
      "Epoch 557/1000 Loss: 0.005149601958692074 num_feat: 8/38 Reg Loss: 0.02773652970790863 Val Accuracy: 91.66666666666667\n",
      "Epoch 558/1000 Loss: 0.0055783712305128574 num_feat: 8/38 Reg Loss: 0.02771744504570961 Val Accuracy: 94.44444444444444\n",
      "Epoch 559/1000 Loss: 0.01052487175911665 num_feat: 8/38 Reg Loss: 0.027698159217834473 Val Accuracy: 94.44444444444444\n",
      "Epoch 560/1000 Loss: 0.009603407233953476 num_feat: 8/38 Reg Loss: 0.027676716446876526 Val Accuracy: 94.44444444444444\n",
      "Epoch 561/1000 Loss: 0.005213975440710783 num_feat: 8/38 Reg Loss: 0.027654705569148064 Val Accuracy: 94.44444444444444\n",
      "Epoch 562/1000 Loss: 0.0053925588726997375 num_feat: 8/38 Reg Loss: 0.027632692828774452 Val Accuracy: 94.44444444444444\n",
      "Epoch 563/1000 Loss: 0.007469541858881712 num_feat: 8/38 Reg Loss: 0.027610287070274353 Val Accuracy: 94.44444444444444\n",
      "Epoch 564/1000 Loss: 0.0052779666148126125 num_feat: 8/38 Reg Loss: 0.02758779563009739 Val Accuracy: 94.44444444444444\n",
      "Epoch 565/1000 Loss: 0.0042986273765563965 num_feat: 8/38 Reg Loss: 0.027565360069274902 Val Accuracy: 94.44444444444444\n",
      "Epoch 566/1000 Loss: 0.004895422141999006 num_feat: 8/38 Reg Loss: 0.02754283882677555 Val Accuracy: 94.44444444444444\n",
      "Epoch 567/1000 Loss: 0.005628292914479971 num_feat: 8/38 Reg Loss: 0.027520379051566124 Val Accuracy: 94.44444444444444\n",
      "Epoch 568/1000 Loss: 0.0038538959342986345 num_feat: 8/38 Reg Loss: 0.02749798633158207 Val Accuracy: 94.44444444444444\n",
      "Epoch 569/1000 Loss: 0.005016318056732416 num_feat: 8/38 Reg Loss: 0.027475619688630104 Val Accuracy: 94.44444444444444\n",
      "Epoch 570/1000 Loss: 0.00671797851100564 num_feat: 8/38 Reg Loss: 0.02745382860302925 Val Accuracy: 94.44444444444444\n",
      "Epoch 571/1000 Loss: 0.004776179790496826 num_feat: 8/38 Reg Loss: 0.02743024006485939 Val Accuracy: 94.44444444444444\n",
      "Epoch 572/1000 Loss: 0.005422843620181084 num_feat: 8/38 Reg Loss: 0.027406955137848854 Val Accuracy: 94.44444444444444\n",
      "Epoch 573/1000 Loss: 0.02278302237391472 num_feat: 8/38 Reg Loss: 0.027383744716644287 Val Accuracy: 94.44444444444444\n",
      "Epoch 574/1000 Loss: 0.007083856035023928 num_feat: 8/38 Reg Loss: 0.02735951542854309 Val Accuracy: 94.44444444444444\n",
      "Epoch 575/1000 Loss: 0.010576844215393066 num_feat: 8/38 Reg Loss: 0.027333958074450493 Val Accuracy: 94.44444444444444\n",
      "Epoch 576/1000 Loss: 0.020173093304038048 num_feat: 8/38 Reg Loss: 0.02731161378324032 Val Accuracy: 94.44444444444444\n",
      "Epoch 577/1000 Loss: 0.0049756732769310474 num_feat: 8/38 Reg Loss: 0.02728533186018467 Val Accuracy: 94.44444444444444\n",
      "Epoch 578/1000 Loss: 0.007313873153179884 num_feat: 8/38 Reg Loss: 0.02725975215435028 Val Accuracy: 94.44444444444444\n",
      "Epoch 579/1000 Loss: 0.015378412790596485 num_feat: 8/38 Reg Loss: 0.02723541297018528 Val Accuracy: 94.44444444444444\n",
      "Epoch 580/1000 Loss: 0.014568030834197998 num_feat: 8/38 Reg Loss: 0.027211833745241165 Val Accuracy: 94.44444444444444\n",
      "Epoch 581/1000 Loss: 0.005843495484441519 num_feat: 8/38 Reg Loss: 0.027190038934350014 Val Accuracy: 94.44444444444444\n",
      "Epoch 582/1000 Loss: 0.009846114553511143 num_feat: 8/38 Reg Loss: 0.02716728486120701 Val Accuracy: 94.44444444444444\n",
      "Epoch 583/1000 Loss: 0.028578465804457664 num_feat: 8/38 Reg Loss: 0.027146680280566216 Val Accuracy: 94.44444444444444\n",
      "Epoch 584/1000 Loss: 0.00867646187543869 num_feat: 8/38 Reg Loss: 0.027125302702188492 Val Accuracy: 94.44444444444444\n",
      "Epoch 585/1000 Loss: 0.0045066154561936855 num_feat: 8/38 Reg Loss: 0.02710607461631298 Val Accuracy: 94.44444444444444\n",
      "Epoch 586/1000 Loss: 0.007726481184363365 num_feat: 8/38 Reg Loss: 0.027086982503533363 Val Accuracy: 94.44444444444444\n",
      "Epoch 587/1000 Loss: 0.011093361303210258 num_feat: 8/38 Reg Loss: 0.02706724777817726 Val Accuracy: 94.44444444444444\n",
      "Epoch 588/1000 Loss: 0.024590006098151207 num_feat: 8/38 Reg Loss: 0.02704719640314579 Val Accuracy: 94.44444444444444\n",
      "Epoch 589/1000 Loss: 0.004173999186605215 num_feat: 8/38 Reg Loss: 0.027027327567338943 Val Accuracy: 94.44444444444444\n",
      "Epoch 590/1000 Loss: 0.02102718874812126 num_feat: 8/38 Reg Loss: 0.027007509022951126 Val Accuracy: 94.44444444444444\n",
      "Epoch 591/1000 Loss: 0.012145689688622952 num_feat: 8/38 Reg Loss: 0.02698679268360138 Val Accuracy: 94.44444444444444\n",
      "Epoch 592/1000 Loss: 0.008285753428936005 num_feat: 8/38 Reg Loss: 0.0269655529409647 Val Accuracy: 94.44444444444444\n",
      "Epoch 593/1000 Loss: 0.003930882550776005 num_feat: 8/38 Reg Loss: 0.026943042874336243 Val Accuracy: 94.44444444444444\n",
      "Epoch 594/1000 Loss: 0.005472085438668728 num_feat: 8/38 Reg Loss: 0.02692072093486786 Val Accuracy: 94.44444444444444\n",
      "Epoch 595/1000 Loss: 0.0146725969389081 num_feat: 8/38 Reg Loss: 0.026899972930550575 Val Accuracy: 94.44444444444444\n",
      "Epoch 596/1000 Loss: 0.009141163900494576 num_feat: 8/38 Reg Loss: 0.026877863332629204 Val Accuracy: 94.44444444444444\n",
      "Epoch 597/1000 Loss: 0.006049867253750563 num_feat: 8/38 Reg Loss: 0.02685619331896305 Val Accuracy: 94.44444444444444\n",
      "Epoch 598/1000 Loss: 0.003765016095712781 num_feat: 8/38 Reg Loss: 0.026835203170776367 Val Accuracy: 94.44444444444444\n",
      "Epoch 599/1000 Loss: 0.009046492166817188 num_feat: 8/38 Reg Loss: 0.026814419776201248 Val Accuracy: 94.44444444444444\n",
      "Epoch 600/1000 Loss: 0.01236655842512846 num_feat: 8/38 Reg Loss: 0.02679416909813881 Val Accuracy: 94.44444444444444\n",
      "Epoch 601/1000 Loss: 0.006099885329604149 num_feat: 8/38 Reg Loss: 0.026773277670145035 Val Accuracy: 94.44444444444444\n",
      "Epoch 602/1000 Loss: 0.0036008208990097046 num_feat: 8/38 Reg Loss: 0.026751821860671043 Val Accuracy: 94.44444444444444\n",
      "Epoch 603/1000 Loss: 0.005066659767180681 num_feat: 8/38 Reg Loss: 0.026730766519904137 Val Accuracy: 94.44444444444444\n",
      "Epoch 604/1000 Loss: 0.004731557331979275 num_feat: 8/38 Reg Loss: 0.026709798723459244 Val Accuracy: 94.44444444444444\n",
      "Epoch 605/1000 Loss: 0.023639366030693054 num_feat: 8/38 Reg Loss: 0.026689518243074417 Val Accuracy: 94.44444444444444\n",
      "Epoch 606/1000 Loss: 0.005222237203270197 num_feat: 8/38 Reg Loss: 0.02666947804391384 Val Accuracy: 94.44444444444444\n",
      "Epoch 607/1000 Loss: 0.003922661300748587 num_feat: 8/38 Reg Loss: 0.026650210842490196 Val Accuracy: 94.44444444444444\n",
      "Epoch 608/1000 Loss: 0.010645531117916107 num_feat: 8/38 Reg Loss: 0.026630952954292297 Val Accuracy: 94.44444444444444\n",
      "Epoch 609/1000 Loss: 0.022702999413013458 num_feat: 8/38 Reg Loss: 0.026608748361468315 Val Accuracy: 94.44444444444444\n",
      "Epoch 610/1000 Loss: 0.003714157035574317 num_feat: 8/38 Reg Loss: 0.026583531871438026 Val Accuracy: 94.44444444444444\n",
      "Epoch 611/1000 Loss: 0.003960221074521542 num_feat: 8/38 Reg Loss: 0.0265593733638525 Val Accuracy: 94.44444444444444\n",
      "Epoch 612/1000 Loss: 0.013555668294429779 num_feat: 8/38 Reg Loss: 0.026535868644714355 Val Accuracy: 94.44444444444444\n",
      "Epoch 613/1000 Loss: 0.006687747780233622 num_feat: 8/38 Reg Loss: 0.02651160955429077 Val Accuracy: 94.44444444444444\n",
      "Epoch 614/1000 Loss: 0.007137155160307884 num_feat: 8/38 Reg Loss: 0.026489434763789177 Val Accuracy: 94.44444444444444\n",
      "Epoch 615/1000 Loss: 0.0037155349273234606 num_feat: 8/38 Reg Loss: 0.026467567309737206 Val Accuracy: 94.44444444444444\n",
      "Epoch 616/1000 Loss: 0.011204628273844719 num_feat: 8/38 Reg Loss: 0.026446176692843437 Val Accuracy: 94.44444444444444\n",
      "Epoch 617/1000 Loss: 0.003199277212843299 num_feat: 8/38 Reg Loss: 0.026424294337630272 Val Accuracy: 94.44444444444444\n",
      "Epoch 618/1000 Loss: 0.003660285845398903 num_feat: 8/38 Reg Loss: 0.02640305459499359 Val Accuracy: 94.44444444444444\n",
      "Epoch 619/1000 Loss: 0.003158221021294594 num_feat: 8/38 Reg Loss: 0.026382043957710266 Val Accuracy: 94.44444444444444\n",
      "Epoch 620/1000 Loss: 0.0034647760912775993 num_feat: 8/38 Reg Loss: 0.026361456140875816 Val Accuracy: 94.44444444444444\n",
      "Epoch 621/1000 Loss: 0.00489825988188386 num_feat: 8/38 Reg Loss: 0.02634136751294136 Val Accuracy: 94.44444444444444\n",
      "Epoch 622/1000 Loss: 0.0035353966522961855 num_feat: 8/38 Reg Loss: 0.026321036741137505 Val Accuracy: 94.44444444444444\n",
      "Epoch 623/1000 Loss: 0.002917488105595112 num_feat: 8/38 Reg Loss: 0.026301002129912376 Val Accuracy: 94.44444444444444\n",
      "Epoch 624/1000 Loss: 0.0033606968354433775 num_feat: 8/38 Reg Loss: 0.026281634345650673 Val Accuracy: 94.44444444444444\n",
      "Epoch 625/1000 Loss: 0.0032474277541041374 num_feat: 8/38 Reg Loss: 0.02626243233680725 Val Accuracy: 94.44444444444444\n",
      "Epoch 626/1000 Loss: 0.005981932859867811 num_feat: 8/38 Reg Loss: 0.026243511587381363 Val Accuracy: 94.44444444444444\n",
      "Epoch 627/1000 Loss: 0.0027915448881685734 num_feat: 8/38 Reg Loss: 0.026224924251437187 Val Accuracy: 94.44444444444444\n",
      "Epoch 628/1000 Loss: 0.003814807627350092 num_feat: 8/38 Reg Loss: 0.02620656229555607 Val Accuracy: 94.44444444444444\n",
      "Epoch 629/1000 Loss: 0.003833932103589177 num_feat: 8/38 Reg Loss: 0.026188278570771217 Val Accuracy: 94.44444444444444\n",
      "Epoch 630/1000 Loss: 0.003630327992141247 num_feat: 8/38 Reg Loss: 0.026170093566179276 Val Accuracy: 94.44444444444444\n",
      "Epoch 631/1000 Loss: 0.0030538667924702168 num_feat: 8/38 Reg Loss: 0.02615188993513584 Val Accuracy: 94.44444444444444\n",
      "Epoch 632/1000 Loss: 0.0191023088991642 num_feat: 8/38 Reg Loss: 0.0261338260024786 Val Accuracy: 94.44444444444444\n",
      "Epoch 633/1000 Loss: 0.003023103578016162 num_feat: 8/38 Reg Loss: 0.02611321024596691 Val Accuracy: 94.44444444444444\n",
      "Epoch 634/1000 Loss: 0.007011589594185352 num_feat: 8/38 Reg Loss: 0.026093140244483948 Val Accuracy: 94.44444444444444\n",
      "Epoch 635/1000 Loss: 0.007012271322309971 num_feat: 8/38 Reg Loss: 0.026072880253195763 Val Accuracy: 94.44444444444444\n",
      "Epoch 636/1000 Loss: 0.0034288831520825624 num_feat: 8/38 Reg Loss: 0.026053136214613914 Val Accuracy: 94.44444444444444\n",
      "Epoch 637/1000 Loss: 0.0032007363624870777 num_feat: 8/38 Reg Loss: 0.026034031063318253 Val Accuracy: 94.44444444444444\n",
      "Epoch 638/1000 Loss: 0.002646963344886899 num_feat: 8/38 Reg Loss: 0.026015296578407288 Val Accuracy: 94.44444444444444\n",
      "Epoch 639/1000 Loss: 0.0031083684880286455 num_feat: 8/38 Reg Loss: 0.025996917858719826 Val Accuracy: 94.44444444444444\n",
      "Epoch 640/1000 Loss: 0.0029315706342458725 num_feat: 8/38 Reg Loss: 0.025978757068514824 Val Accuracy: 94.44444444444444\n",
      "Epoch 641/1000 Loss: 0.00287603004835546 num_feat: 8/38 Reg Loss: 0.02596081607043743 Val Accuracy: 94.44444444444444\n",
      "Epoch 642/1000 Loss: 0.0030749610159546137 num_feat: 8/38 Reg Loss: 0.025943025946617126 Val Accuracy: 94.44444444444444\n",
      "Epoch 643/1000 Loss: 0.004207887686789036 num_feat: 8/38 Reg Loss: 0.025925645604729652 Val Accuracy: 94.44444444444444\n",
      "Epoch 644/1000 Loss: 0.002966280560940504 num_feat: 8/38 Reg Loss: 0.02590889111161232 Val Accuracy: 94.44444444444444\n",
      "Epoch 645/1000 Loss: 0.005728390067815781 num_feat: 8/38 Reg Loss: 0.025892529636621475 Val Accuracy: 94.44444444444444\n",
      "Epoch 646/1000 Loss: 0.0028285670559853315 num_feat: 8/38 Reg Loss: 0.025875864550471306 Val Accuracy: 94.44444444444444\n",
      "Epoch 647/1000 Loss: 0.004587159492075443 num_feat: 8/38 Reg Loss: 0.025859316810965538 Val Accuracy: 94.44444444444444\n",
      "Epoch 648/1000 Loss: 0.0036091399379074574 num_feat: 8/38 Reg Loss: 0.02584199048578739 Val Accuracy: 94.44444444444444\n",
      "Epoch 649/1000 Loss: 0.002811660524457693 num_feat: 8/38 Reg Loss: 0.025824755430221558 Val Accuracy: 94.44444444444444\n",
      "Epoch 650/1000 Loss: 0.0030202926136553288 num_feat: 8/38 Reg Loss: 0.025807736441493034 Val Accuracy: 94.44444444444444\n",
      "Epoch 651/1000 Loss: 0.002996182069182396 num_feat: 8/38 Reg Loss: 0.025790859013795853 Val Accuracy: 94.44444444444444\n",
      "Epoch 652/1000 Loss: 0.0029621741268783808 num_feat: 8/38 Reg Loss: 0.025774046778678894 Val Accuracy: 94.44444444444444\n",
      "Epoch 653/1000 Loss: 0.0037854888942092657 num_feat: 8/38 Reg Loss: 0.025757377967238426 Val Accuracy: 94.44444444444444\n",
      "Epoch 654/1000 Loss: 0.002772375475615263 num_feat: 8/38 Reg Loss: 0.025740861892700195 Val Accuracy: 94.44444444444444\n",
      "Epoch 655/1000 Loss: 0.0027867299504578114 num_feat: 8/38 Reg Loss: 0.02572455070912838 Val Accuracy: 94.44444444444444\n",
      "Epoch 656/1000 Loss: 0.0027651169802993536 num_feat: 8/38 Reg Loss: 0.025708645582199097 Val Accuracy: 94.44444444444444\n",
      "Epoch 657/1000 Loss: 0.0027955840341746807 num_feat: 8/38 Reg Loss: 0.025692883878946304 Val Accuracy: 94.44444444444444\n",
      "Epoch 658/1000 Loss: 0.002571035409346223 num_feat: 8/38 Reg Loss: 0.025677194818854332 Val Accuracy: 94.44444444444444\n",
      "Epoch 659/1000 Loss: 0.002903165528550744 num_feat: 8/38 Reg Loss: 0.025661660358309746 Val Accuracy: 94.44444444444444\n",
      "Epoch 660/1000 Loss: 0.002662820043042302 num_feat: 8/38 Reg Loss: 0.025645950809121132 Val Accuracy: 94.44444444444444\n",
      "Epoch 661/1000 Loss: 0.002846594899892807 num_feat: 8/38 Reg Loss: 0.025630442425608635 Val Accuracy: 94.44444444444444\n",
      "Epoch 662/1000 Loss: 0.0027102725580334663 num_feat: 8/38 Reg Loss: 0.025615086778998375 Val Accuracy: 94.44444444444444\n",
      "Epoch 663/1000 Loss: 0.002712034620344639 num_feat: 8/38 Reg Loss: 0.025599753484129906 Val Accuracy: 94.44444444444444\n",
      "Epoch 664/1000 Loss: 0.002950032241642475 num_feat: 8/38 Reg Loss: 0.02558443322777748 Val Accuracy: 94.44444444444444\n",
      "Epoch 665/1000 Loss: 0.0025967182591557503 num_feat: 8/38 Reg Loss: 0.025569206103682518 Val Accuracy: 94.44444444444444\n",
      "Epoch 666/1000 Loss: 0.0046785795129835606 num_feat: 8/38 Reg Loss: 0.025553930550813675 Val Accuracy: 94.44444444444444\n",
      "Epoch 667/1000 Loss: 0.002527558244764805 num_feat: 8/38 Reg Loss: 0.02553769014775753 Val Accuracy: 94.44444444444444\n",
      "Epoch 668/1000 Loss: 0.0026352982968091965 num_feat: 8/38 Reg Loss: 0.02552166022360325 Val Accuracy: 94.44444444444444\n",
      "Epoch 669/1000 Loss: 0.002494456246495247 num_feat: 8/38 Reg Loss: 0.02550572156906128 Val Accuracy: 94.44444444444444\n",
      "Epoch 670/1000 Loss: 0.002930694492533803 num_feat: 8/38 Reg Loss: 0.025489939376711845 Val Accuracy: 94.44444444444444\n",
      "Epoch 671/1000 Loss: 0.002878100611269474 num_feat: 8/38 Reg Loss: 0.025474226102232933 Val Accuracy: 94.44444444444444\n",
      "Epoch 672/1000 Loss: 0.0027841597329825163 num_feat: 8/38 Reg Loss: 0.025458553805947304 Val Accuracy: 94.44444444444444\n",
      "Epoch 673/1000 Loss: 0.002506012562662363 num_feat: 8/38 Reg Loss: 0.0254429429769516 Val Accuracy: 94.44444444444444\n",
      "Epoch 674/1000 Loss: 0.0025344137102365494 num_feat: 8/38 Reg Loss: 0.025427479296922684 Val Accuracy: 94.44444444444444\n",
      "Epoch 675/1000 Loss: 0.00302415294572711 num_feat: 8/38 Reg Loss: 0.025412091985344887 Val Accuracy: 94.44444444444444\n",
      "Epoch 676/1000 Loss: 0.0027756416238844395 num_feat: 8/38 Reg Loss: 0.025396710261702538 Val Accuracy: 94.44444444444444\n",
      "Epoch 677/1000 Loss: 0.0027506647165864706 num_feat: 8/38 Reg Loss: 0.02538159303367138 Val Accuracy: 94.44444444444444\n",
      "Epoch 678/1000 Loss: 0.0024737496860325336 num_feat: 8/38 Reg Loss: 0.025366520509123802 Val Accuracy: 94.44444444444444\n",
      "Epoch 679/1000 Loss: 0.0025438687298446894 num_feat: 8/38 Reg Loss: 0.02535143867135048 Val Accuracy: 94.44444444444444\n",
      "Epoch 680/1000 Loss: 0.0038791049737483263 num_feat: 8/38 Reg Loss: 0.025336408987641335 Val Accuracy: 94.44444444444444\n",
      "Epoch 681/1000 Loss: 0.0026202108711004257 num_feat: 8/38 Reg Loss: 0.02532118931412697 Val Accuracy: 94.44444444444444\n",
      "Epoch 682/1000 Loss: 0.002452349988743663 num_feat: 8/38 Reg Loss: 0.0253059770911932 Val Accuracy: 94.44444444444444\n",
      "Epoch 683/1000 Loss: 0.0028815227560698986 num_feat: 8/38 Reg Loss: 0.025290871039032936 Val Accuracy: 94.44444444444444\n",
      "Epoch 684/1000 Loss: 0.0028727632015943527 num_feat: 8/38 Reg Loss: 0.02527536451816559 Val Accuracy: 94.44444444444444\n",
      "Epoch 685/1000 Loss: 0.0035468521527945995 num_feat: 8/38 Reg Loss: 0.02526041865348816 Val Accuracy: 94.44444444444444\n",
      "Epoch 686/1000 Loss: 0.0034264815039932728 num_feat: 8/38 Reg Loss: 0.02524559572339058 Val Accuracy: 94.44444444444444\n",
      "Epoch 687/1000 Loss: 0.0026343860663473606 num_feat: 8/38 Reg Loss: 0.025230903178453445 Val Accuracy: 94.44444444444444\n",
      "Epoch 688/1000 Loss: 0.0024194635916501284 num_feat: 8/38 Reg Loss: 0.02521628513932228 Val Accuracy: 94.44444444444444\n",
      "Epoch 689/1000 Loss: 0.002389055909588933 num_feat: 8/38 Reg Loss: 0.025201711803674698 Val Accuracy: 94.44444444444444\n",
      "Epoch 690/1000 Loss: 0.0025975899770855904 num_feat: 8/38 Reg Loss: 0.025187212973833084 Val Accuracy: 94.44444444444444\n",
      "Epoch 691/1000 Loss: 0.0024965086486190557 num_feat: 8/38 Reg Loss: 0.025172799825668335 Val Accuracy: 94.44444444444444\n",
      "Epoch 692/1000 Loss: 0.012053452432155609 num_feat: 8/38 Reg Loss: 0.025158453732728958 Val Accuracy: 94.44444444444444\n",
      "Epoch 693/1000 Loss: 0.0027335721533745527 num_feat: 8/38 Reg Loss: 0.025143567472696304 Val Accuracy: 94.44444444444444\n",
      "Epoch 694/1000 Loss: 0.003455718047916889 num_feat: 8/38 Reg Loss: 0.02512880600988865 Val Accuracy: 94.44444444444444\n",
      "Epoch 695/1000 Loss: 0.007041329517960548 num_feat: 8/38 Reg Loss: 0.025114629417657852 Val Accuracy: 94.44444444444444\n",
      "Epoch 696/1000 Loss: 0.006647811271250248 num_feat: 8/38 Reg Loss: 0.0251005869358778 Val Accuracy: 94.44444444444444\n",
      "Epoch 697/1000 Loss: 0.0027913826052099466 num_feat: 8/38 Reg Loss: 0.025085845962166786 Val Accuracy: 94.44444444444444\n",
      "Epoch 698/1000 Loss: 0.0022111099679023027 num_feat: 8/38 Reg Loss: 0.025071311742067337 Val Accuracy: 94.44444444444444\n",
      "Epoch 699/1000 Loss: 0.002660032594576478 num_feat: 8/38 Reg Loss: 0.02505689673125744 Val Accuracy: 94.44444444444444\n",
      "Epoch 700/1000 Loss: 0.0032857987098395824 num_feat: 8/38 Reg Loss: 0.02504255808889866 Val Accuracy: 94.44444444444444\n",
      "Epoch 701/1000 Loss: 0.0018706565024331212 num_feat: 8/38 Reg Loss: 0.025028208270668983 Val Accuracy: 94.44444444444444\n",
      "Epoch 702/1000 Loss: 0.0026794893201440573 num_feat: 8/38 Reg Loss: 0.025014027953147888 Val Accuracy: 94.44444444444444\n",
      "Epoch 703/1000 Loss: 0.006287287455052137 num_feat: 8/38 Reg Loss: 0.025000063702464104 Val Accuracy: 94.44444444444444\n",
      "Epoch 704/1000 Loss: 0.002495201537385583 num_feat: 8/38 Reg Loss: 0.024985985830426216 Val Accuracy: 94.44444444444444\n",
      "Epoch 705/1000 Loss: 0.00261591630987823 num_feat: 8/38 Reg Loss: 0.02497209794819355 Val Accuracy: 94.44444444444444\n",
      "Epoch 706/1000 Loss: 0.0038835371378809214 num_feat: 8/38 Reg Loss: 0.02495829574763775 Val Accuracy: 94.44444444444444\n",
      "Epoch 707/1000 Loss: 0.00438475888222456 num_feat: 8/38 Reg Loss: 0.024944601580500603 Val Accuracy: 94.44444444444444\n",
      "Epoch 708/1000 Loss: 0.009715399704873562 num_feat: 8/38 Reg Loss: 0.024930911138653755 Val Accuracy: 94.44444444444444\n",
      "Epoch 709/1000 Loss: 0.0024186144582927227 num_feat: 8/38 Reg Loss: 0.024916227906942368 Val Accuracy: 94.44444444444444\n",
      "Epoch 710/1000 Loss: 0.002997958566993475 num_feat: 8/38 Reg Loss: 0.024901648983359337 Val Accuracy: 94.44444444444444\n",
      "Epoch 711/1000 Loss: 0.004262594040483236 num_feat: 8/38 Reg Loss: 0.024887243285775185 Val Accuracy: 94.44444444444444\n",
      "Epoch 712/1000 Loss: 0.0046971566043794155 num_feat: 8/38 Reg Loss: 0.024872982874512672 Val Accuracy: 94.44444444444444\n",
      "Epoch 713/1000 Loss: 0.004060236271470785 num_feat: 8/38 Reg Loss: 0.02485954388976097 Val Accuracy: 94.44444444444444\n",
      "Epoch 714/1000 Loss: 0.002149623353034258 num_feat: 8/38 Reg Loss: 0.024846132844686508 Val Accuracy: 94.44444444444444\n",
      "Epoch 715/1000 Loss: 0.0020782779902219772 num_feat: 8/38 Reg Loss: 0.024832865223288536 Val Accuracy: 94.44444444444444\n",
      "Epoch 716/1000 Loss: 0.0037290100008249283 num_feat: 8/38 Reg Loss: 0.024819646030664444 Val Accuracy: 94.44444444444444\n",
      "Epoch 717/1000 Loss: 0.00410928949713707 num_feat: 8/38 Reg Loss: 0.024805987253785133 Val Accuracy: 94.44444444444444\n",
      "Epoch 718/1000 Loss: 0.0036363648250699043 num_feat: 8/38 Reg Loss: 0.02479194663465023 Val Accuracy: 94.44444444444444\n",
      "Epoch 719/1000 Loss: 0.001902673626318574 num_feat: 8/38 Reg Loss: 0.024777742102742195 Val Accuracy: 94.44444444444444\n",
      "Epoch 720/1000 Loss: 0.0019326163455843925 num_feat: 8/38 Reg Loss: 0.02476373501121998 Val Accuracy: 94.44444444444444\n",
      "Epoch 721/1000 Loss: 0.0020153403747826815 num_feat: 8/38 Reg Loss: 0.0247498769313097 Val Accuracy: 94.44444444444444\n",
      "Epoch 722/1000 Loss: 0.002089605201035738 num_feat: 8/38 Reg Loss: 0.02473617158830166 Val Accuracy: 91.66666666666667\n",
      "Epoch 723/1000 Loss: 0.00382565101608634 num_feat: 8/38 Reg Loss: 0.024722594767808914 Val Accuracy: 94.44444444444444\n",
      "Epoch 724/1000 Loss: 0.003135988023132086 num_feat: 8/38 Reg Loss: 0.02470906637609005 Val Accuracy: 94.44444444444444\n",
      "Epoch 725/1000 Loss: 0.003619436640292406 num_feat: 8/38 Reg Loss: 0.024695973843336105 Val Accuracy: 94.44444444444444\n",
      "Epoch 726/1000 Loss: 0.0019444441422820091 num_feat: 8/38 Reg Loss: 0.02468232437968254 Val Accuracy: 94.44444444444444\n",
      "Epoch 727/1000 Loss: 0.002315159887075424 num_feat: 8/38 Reg Loss: 0.02466881088912487 Val Accuracy: 94.44444444444444\n",
      "Epoch 728/1000 Loss: 0.0025094598531723022 num_feat: 8/38 Reg Loss: 0.024655325338244438 Val Accuracy: 94.44444444444444\n",
      "Epoch 729/1000 Loss: 0.0015024496242403984 num_feat: 8/38 Reg Loss: 0.024642091244459152 Val Accuracy: 94.44444444444444\n",
      "Epoch 730/1000 Loss: 0.0020894117187708616 num_feat: 8/38 Reg Loss: 0.024629002436995506 Val Accuracy: 94.44444444444444\n",
      "Epoch 731/1000 Loss: 0.005245408974587917 num_feat: 8/38 Reg Loss: 0.02461613528430462 Val Accuracy: 94.44444444444444\n",
      "Epoch 732/1000 Loss: 0.002020144835114479 num_feat: 8/38 Reg Loss: 0.02460309863090515 Val Accuracy: 94.44444444444444\n",
      "Epoch 733/1000 Loss: 0.0017561388667672873 num_feat: 8/38 Reg Loss: 0.024590155109763145 Val Accuracy: 94.44444444444444\n",
      "Epoch 734/1000 Loss: 0.001912481035105884 num_feat: 8/38 Reg Loss: 0.02457730658352375 Val Accuracy: 94.44444444444444\n",
      "Epoch 735/1000 Loss: 0.0027837965171784163 num_feat: 8/38 Reg Loss: 0.024564532563090324 Val Accuracy: 94.44444444444444\n",
      "Epoch 736/1000 Loss: 0.002269565360620618 num_feat: 8/38 Reg Loss: 0.0245518796145916 Val Accuracy: 94.44444444444444\n",
      "Epoch 737/1000 Loss: 0.002819362562149763 num_feat: 8/38 Reg Loss: 0.02453930303454399 Val Accuracy: 94.44444444444444\n",
      "Epoch 738/1000 Loss: 0.0024072222877293825 num_feat: 8/38 Reg Loss: 0.02452671341598034 Val Accuracy: 94.44444444444444\n",
      "Epoch 739/1000 Loss: 0.001693488797172904 num_feat: 8/38 Reg Loss: 0.0245142113417387 Val Accuracy: 94.44444444444444\n",
      "Epoch 740/1000 Loss: 0.005257951095700264 num_feat: 8/38 Reg Loss: 0.024501819163560867 Val Accuracy: 94.44444444444444\n",
      "Epoch 741/1000 Loss: 0.0019137191120535135 num_feat: 8/38 Reg Loss: 0.024489019066095352 Val Accuracy: 94.44444444444444\n",
      "Epoch 742/1000 Loss: 0.0018272793386131525 num_feat: 8/38 Reg Loss: 0.024476410821080208 Val Accuracy: 94.44444444444444\n",
      "Epoch 743/1000 Loss: 0.0017423102399334311 num_feat: 8/38 Reg Loss: 0.024463746696710587 Val Accuracy: 94.44444444444444\n",
      "Epoch 744/1000 Loss: 0.001895654248073697 num_feat: 8/38 Reg Loss: 0.024451179429888725 Val Accuracy: 94.44444444444444\n",
      "Epoch 745/1000 Loss: 0.0017674637492746115 num_feat: 8/38 Reg Loss: 0.024438558146357536 Val Accuracy: 94.44444444444444\n",
      "Epoch 746/1000 Loss: 0.0041146897710859776 num_feat: 8/38 Reg Loss: 0.02442602813243866 Val Accuracy: 94.44444444444444\n",
      "Epoch 747/1000 Loss: 0.001694943173788488 num_feat: 8/38 Reg Loss: 0.02441365271806717 Val Accuracy: 94.44444444444444\n",
      "Epoch 748/1000 Loss: 0.002062304178252816 num_feat: 8/38 Reg Loss: 0.024401402100920677 Val Accuracy: 94.44444444444444\n",
      "Epoch 749/1000 Loss: 0.007050391286611557 num_feat: 8/38 Reg Loss: 0.02438936196267605 Val Accuracy: 94.44444444444444\n",
      "Epoch 750/1000 Loss: 0.0017922460101544857 num_feat: 8/38 Reg Loss: 0.024376505985856056 Val Accuracy: 94.44444444444444\n",
      "Epoch 751/1000 Loss: 0.0017044125124812126 num_feat: 8/38 Reg Loss: 0.024363864213228226 Val Accuracy: 94.44444444444444\n",
      "Epoch 752/1000 Loss: 0.002748704282566905 num_feat: 8/38 Reg Loss: 0.024351395666599274 Val Accuracy: 94.44444444444444\n",
      "Epoch 753/1000 Loss: 0.002390572102740407 num_feat: 8/38 Reg Loss: 0.024339038878679276 Val Accuracy: 94.44444444444444\n",
      "Epoch 754/1000 Loss: 0.0022289243061095476 num_feat: 8/38 Reg Loss: 0.024326739832758904 Val Accuracy: 94.44444444444444\n",
      "Epoch 755/1000 Loss: 0.002155614085495472 num_feat: 8/38 Reg Loss: 0.024314459413290024 Val Accuracy: 94.44444444444444\n",
      "Epoch 756/1000 Loss: 0.0017613122472539544 num_feat: 8/38 Reg Loss: 0.024302292615175247 Val Accuracy: 94.44444444444444\n",
      "Epoch 757/1000 Loss: 0.001711432822048664 num_feat: 8/38 Reg Loss: 0.02429015561938286 Val Accuracy: 94.44444444444444\n",
      "Epoch 758/1000 Loss: 0.0017699880991131067 num_feat: 8/38 Reg Loss: 0.02427808754146099 Val Accuracy: 94.44444444444444\n",
      "Epoch 759/1000 Loss: 0.0020364588126540184 num_feat: 8/38 Reg Loss: 0.024266047403216362 Val Accuracy: 94.44444444444444\n",
      "Epoch 760/1000 Loss: 0.0026130082551389933 num_feat: 8/38 Reg Loss: 0.024254312738776207 Val Accuracy: 94.44444444444444\n",
      "Epoch 761/1000 Loss: 0.002045948524028063 num_feat: 8/38 Reg Loss: 0.024242481216788292 Val Accuracy: 94.44444444444444\n",
      "Epoch 762/1000 Loss: 0.002308115130290389 num_feat: 8/38 Reg Loss: 0.024230755865573883 Val Accuracy: 94.44444444444444\n",
      "Epoch 763/1000 Loss: 0.0016274815425276756 num_feat: 8/38 Reg Loss: 0.02421918325126171 Val Accuracy: 94.44444444444444\n",
      "Epoch 764/1000 Loss: 0.001902446849271655 num_feat: 8/38 Reg Loss: 0.024207698181271553 Val Accuracy: 94.44444444444444\n",
      "Epoch 765/1000 Loss: 0.0028497425373643637 num_feat: 8/38 Reg Loss: 0.024196190759539604 Val Accuracy: 94.44444444444444\n",
      "Epoch 766/1000 Loss: 0.002346367808058858 num_feat: 8/38 Reg Loss: 0.024184370413422585 Val Accuracy: 94.44444444444444\n",
      "Epoch 767/1000 Loss: 0.0016545791877433658 num_feat: 8/38 Reg Loss: 0.02417229674756527 Val Accuracy: 94.44444444444444\n",
      "Epoch 768/1000 Loss: 0.0016347173368558288 num_feat: 8/38 Reg Loss: 0.024160318076610565 Val Accuracy: 94.44444444444444\n",
      "Epoch 769/1000 Loss: 0.0016690113116055727 num_feat: 8/38 Reg Loss: 0.02414841391146183 Val Accuracy: 94.44444444444444\n",
      "Epoch 770/1000 Loss: 0.0017667877255007625 num_feat: 8/38 Reg Loss: 0.024136584252119064 Val Accuracy: 94.44444444444444\n",
      "Epoch 771/1000 Loss: 0.0015861520078033209 num_feat: 8/38 Reg Loss: 0.024124857038259506 Val Accuracy: 94.44444444444444\n",
      "Epoch 772/1000 Loss: 0.0015878044068813324 num_feat: 8/38 Reg Loss: 0.024113213643431664 Val Accuracy: 94.44444444444444\n",
      "Epoch 773/1000 Loss: 0.001898746588267386 num_feat: 8/38 Reg Loss: 0.024101674556732178 Val Accuracy: 94.44444444444444\n",
      "Epoch 774/1000 Loss: 0.0019657015800476074 num_feat: 8/38 Reg Loss: 0.024090122431516647 Val Accuracy: 94.44444444444444\n",
      "Epoch 775/1000 Loss: 0.0016689995536580682 num_feat: 8/38 Reg Loss: 0.024078721180558205 Val Accuracy: 94.44444444444444\n",
      "Epoch 776/1000 Loss: 0.0015440058195963502 num_feat: 8/38 Reg Loss: 0.02406725101172924 Val Accuracy: 94.44444444444444\n",
      "Epoch 777/1000 Loss: 0.0024702067021280527 num_feat: 8/38 Reg Loss: 0.024055885151028633 Val Accuracy: 94.44444444444444\n",
      "Epoch 778/1000 Loss: 0.001629807986319065 num_feat: 8/38 Reg Loss: 0.024044327437877655 Val Accuracy: 94.44444444444444\n",
      "Epoch 779/1000 Loss: 0.003832502756267786 num_feat: 8/38 Reg Loss: 0.024032820016145706 Val Accuracy: 94.44444444444444\n",
      "Epoch 780/1000 Loss: 0.003671195823699236 num_feat: 8/38 Reg Loss: 0.024020932614803314 Val Accuracy: 94.44444444444444\n",
      "Epoch 781/1000 Loss: 0.0017112226923927665 num_feat: 8/38 Reg Loss: 0.02400892786681652 Val Accuracy: 94.44444444444444\n",
      "Epoch 782/1000 Loss: 0.0019980212673544884 num_feat: 8/38 Reg Loss: 0.02399708330631256 Val Accuracy: 94.44444444444444\n",
      "Epoch 783/1000 Loss: 0.0018710559234023094 num_feat: 8/38 Reg Loss: 0.023985333740711212 Val Accuracy: 94.44444444444444\n",
      "Epoch 784/1000 Loss: 0.005112720187753439 num_feat: 8/38 Reg Loss: 0.023973781615495682 Val Accuracy: 94.44444444444444\n",
      "Epoch 785/1000 Loss: 0.0017091089393943548 num_feat: 8/38 Reg Loss: 0.023961974307894707 Val Accuracy: 94.44444444444444\n",
      "Epoch 786/1000 Loss: 0.0013974857283756137 num_feat: 8/38 Reg Loss: 0.02395021729171276 Val Accuracy: 94.44444444444444\n",
      "Epoch 787/1000 Loss: 0.0014368273550644517 num_feat: 8/38 Reg Loss: 0.023938575759530067 Val Accuracy: 94.44444444444444\n",
      "Epoch 788/1000 Loss: 0.003987594041973352 num_feat: 8/38 Reg Loss: 0.023927081376314163 Val Accuracy: 94.44444444444444\n",
      "Epoch 789/1000 Loss: 0.0014678327133879066 num_feat: 8/38 Reg Loss: 0.023915499448776245 Val Accuracy: 94.44444444444444\n",
      "Epoch 790/1000 Loss: 0.0018238095799461007 num_feat: 8/38 Reg Loss: 0.02390400506556034 Val Accuracy: 94.44444444444444\n",
      "Epoch 791/1000 Loss: 0.0013126522535458207 num_feat: 8/38 Reg Loss: 0.023892579600214958 Val Accuracy: 94.44444444444444\n",
      "Epoch 792/1000 Loss: 0.0014734393917024136 num_feat: 8/38 Reg Loss: 0.02388126775622368 Val Accuracy: 94.44444444444444\n",
      "Epoch 793/1000 Loss: 0.0016979423817247152 num_feat: 8/38 Reg Loss: 0.023870069533586502 Val Accuracy: 94.44444444444444\n",
      "Epoch 794/1000 Loss: 0.0015716698253527284 num_feat: 8/38 Reg Loss: 0.02385895512998104 Val Accuracy: 94.44444444444444\n",
      "Epoch 795/1000 Loss: 0.0013469968689605594 num_feat: 8/38 Reg Loss: 0.023847920820116997 Val Accuracy: 94.44444444444444\n",
      "Epoch 796/1000 Loss: 0.0015801434637978673 num_feat: 8/38 Reg Loss: 0.023836921900510788 Val Accuracy: 94.44444444444444\n",
      "Epoch 797/1000 Loss: 0.0014782946091145277 num_feat: 8/38 Reg Loss: 0.023826027289032936 Val Accuracy: 94.44444444444444\n",
      "Epoch 798/1000 Loss: 0.001465721637941897 num_feat: 8/38 Reg Loss: 0.02381526492536068 Val Accuracy: 94.44444444444444\n",
      "Epoch 799/1000 Loss: 0.0019569287542253733 num_feat: 8/38 Reg Loss: 0.023804526776075363 Val Accuracy: 94.44444444444444\n",
      "Epoch 800/1000 Loss: 0.0014247616054490209 num_feat: 8/38 Reg Loss: 0.023793572559952736 Val Accuracy: 94.44444444444444\n",
      "Epoch 801/1000 Loss: 0.001617217669263482 num_feat: 8/38 Reg Loss: 0.023782700300216675 Val Accuracy: 94.44444444444444\n",
      "Epoch 802/1000 Loss: 0.0017525566508993506 num_feat: 8/38 Reg Loss: 0.023771870881319046 Val Accuracy: 94.44444444444444\n",
      "Epoch 803/1000 Loss: 0.0014277721056714654 num_feat: 8/38 Reg Loss: 0.02376106195151806 Val Accuracy: 94.44444444444444\n",
      "Epoch 804/1000 Loss: 0.0016196814831346273 num_feat: 8/38 Reg Loss: 0.023750323802232742 Val Accuracy: 94.44444444444444\n",
      "Epoch 805/1000 Loss: 0.002256399719044566 num_feat: 8/38 Reg Loss: 0.02373957633972168 Val Accuracy: 94.44444444444444\n",
      "Epoch 806/1000 Loss: 0.0014778070617467165 num_feat: 8/38 Reg Loss: 0.023728741332888603 Val Accuracy: 94.44444444444444\n",
      "Epoch 807/1000 Loss: 0.0013646032894030213 num_feat: 8/38 Reg Loss: 0.0237179733812809 Val Accuracy: 94.44444444444444\n",
      "Epoch 808/1000 Loss: 0.0014156310353428125 num_feat: 8/38 Reg Loss: 0.023707231506705284 Val Accuracy: 94.44444444444444\n",
      "Epoch 809/1000 Loss: 0.0013965927064418793 num_feat: 8/38 Reg Loss: 0.0236965399235487 Val Accuracy: 94.44444444444444\n",
      "Epoch 810/1000 Loss: 0.0017210956430062652 num_feat: 8/38 Reg Loss: 0.02368588000535965 Val Accuracy: 94.44444444444444\n",
      "Epoch 811/1000 Loss: 0.0013052629074081779 num_feat: 8/38 Reg Loss: 0.023675229400396347 Val Accuracy: 94.44444444444444\n",
      "Epoch 812/1000 Loss: 0.0014461694518104196 num_feat: 8/38 Reg Loss: 0.023664653301239014 Val Accuracy: 94.44444444444444\n",
      "Epoch 813/1000 Loss: 0.0013155614724382758 num_feat: 8/38 Reg Loss: 0.023654161021113396 Val Accuracy: 94.44444444444444\n",
      "Epoch 814/1000 Loss: 0.001290378044359386 num_feat: 8/38 Reg Loss: 0.023643730208277702 Val Accuracy: 94.44444444444444\n",
      "Epoch 815/1000 Loss: 0.001302589662373066 num_feat: 8/38 Reg Loss: 0.0236333180218935 Val Accuracy: 94.44444444444444\n",
      "Epoch 816/1000 Loss: 0.0013584386324509978 num_feat: 8/38 Reg Loss: 0.023622913286089897 Val Accuracy: 94.44444444444444\n",
      "Epoch 817/1000 Loss: 0.0013635237701237202 num_feat: 8/38 Reg Loss: 0.02361254207789898 Val Accuracy: 94.44444444444444\n",
      "Epoch 818/1000 Loss: 0.0013994107721373439 num_feat: 8/38 Reg Loss: 0.023602163419127464 Val Accuracy: 94.44444444444444\n",
      "Epoch 819/1000 Loss: 0.0023390324786305428 num_feat: 8/38 Reg Loss: 0.023591849952936172 Val Accuracy: 94.44444444444444\n",
      "Epoch 820/1000 Loss: 0.0013218475505709648 num_feat: 8/38 Reg Loss: 0.02358154021203518 Val Accuracy: 94.44444444444444\n",
      "Epoch 821/1000 Loss: 0.001316406182013452 num_feat: 8/38 Reg Loss: 0.023571310564875603 Val Accuracy: 94.44444444444444\n",
      "Epoch 822/1000 Loss: 0.0013660019030794501 num_feat: 8/38 Reg Loss: 0.02356107346713543 Val Accuracy: 94.44444444444444\n",
      "Epoch 823/1000 Loss: 0.0015926212072372437 num_feat: 8/38 Reg Loss: 0.023550955578684807 Val Accuracy: 94.44444444444444\n",
      "Epoch 824/1000 Loss: 0.0013429166283458471 num_feat: 8/38 Reg Loss: 0.023540901020169258 Val Accuracy: 94.44444444444444\n",
      "Epoch 825/1000 Loss: 0.0024985901545733213 num_feat: 8/38 Reg Loss: 0.023530907928943634 Val Accuracy: 94.44444444444444\n",
      "Epoch 826/1000 Loss: 0.0013905118685215712 num_feat: 8/38 Reg Loss: 0.023520730435848236 Val Accuracy: 94.44444444444444\n",
      "Epoch 827/1000 Loss: 0.0018000166164711118 num_feat: 8/38 Reg Loss: 0.023510532453656197 Val Accuracy: 94.44444444444444\n",
      "Epoch 828/1000 Loss: 0.0014476008946076035 num_feat: 8/38 Reg Loss: 0.02350016124546528 Val Accuracy: 94.44444444444444\n",
      "Epoch 829/1000 Loss: 0.0023897949140518904 num_feat: 8/38 Reg Loss: 0.02348983846604824 Val Accuracy: 94.44444444444444\n",
      "Epoch 830/1000 Loss: 0.0025517705362290144 num_feat: 8/38 Reg Loss: 0.02347947657108307 Val Accuracy: 94.44444444444444\n",
      "Epoch 831/1000 Loss: 0.002733186585828662 num_feat: 8/38 Reg Loss: 0.0234691072255373 Val Accuracy: 94.44444444444444\n",
      "Epoch 832/1000 Loss: 0.0014880598755553365 num_feat: 8/38 Reg Loss: 0.02345861680805683 Val Accuracy: 94.44444444444444\n",
      "Epoch 833/1000 Loss: 0.001382523332722485 num_feat: 8/38 Reg Loss: 0.023448185995221138 Val Accuracy: 94.44444444444444\n",
      "Epoch 834/1000 Loss: 0.0013328741770237684 num_feat: 8/38 Reg Loss: 0.023437783122062683 Val Accuracy: 94.44444444444444\n",
      "Epoch 835/1000 Loss: 0.0016397049184888601 num_feat: 8/38 Reg Loss: 0.023427417501807213 Val Accuracy: 94.44444444444444\n",
      "Epoch 836/1000 Loss: 0.0019081911304965615 num_feat: 8/38 Reg Loss: 0.023417124524712563 Val Accuracy: 94.44444444444444\n",
      "Epoch 837/1000 Loss: 0.002256992505863309 num_feat: 8/38 Reg Loss: 0.02340681664645672 Val Accuracy: 94.44444444444444\n",
      "Epoch 838/1000 Loss: 0.0012064927723258734 num_feat: 8/38 Reg Loss: 0.023396452888846397 Val Accuracy: 94.44444444444444\n",
      "Epoch 839/1000 Loss: 0.002769614802673459 num_feat: 8/38 Reg Loss: 0.023386193439364433 Val Accuracy: 94.44444444444444\n",
      "Epoch 840/1000 Loss: 0.0012126388028264046 num_feat: 8/38 Reg Loss: 0.023375732824206352 Val Accuracy: 94.44444444444444\n",
      "Epoch 841/1000 Loss: 0.001601288910023868 num_feat: 8/38 Reg Loss: 0.023365318775177002 Val Accuracy: 94.44444444444444\n",
      "Epoch 842/1000 Loss: 0.0018328005680814385 num_feat: 8/38 Reg Loss: 0.023355035111308098 Val Accuracy: 94.44444444444444\n",
      "Epoch 843/1000 Loss: 0.001936946646310389 num_feat: 8/38 Reg Loss: 0.02334478497505188 Val Accuracy: 94.44444444444444\n",
      "Epoch 844/1000 Loss: 0.0021209251135587692 num_feat: 8/38 Reg Loss: 0.023334603756666183 Val Accuracy: 94.44444444444444\n",
      "Epoch 845/1000 Loss: 0.0015880530700087547 num_feat: 8/38 Reg Loss: 0.023324185982346535 Val Accuracy: 94.44444444444444\n",
      "Epoch 846/1000 Loss: 0.0011768988333642483 num_feat: 8/38 Reg Loss: 0.023313753306865692 Val Accuracy: 94.44444444444444\n",
      "Epoch 847/1000 Loss: 0.00105206947773695 num_feat: 8/38 Reg Loss: 0.023303378373384476 Val Accuracy: 94.44444444444444\n",
      "Epoch 848/1000 Loss: 0.0015010657953098416 num_feat: 8/38 Reg Loss: 0.023293055593967438 Val Accuracy: 94.44444444444444\n",
      "Epoch 849/1000 Loss: 0.0017702995100989938 num_feat: 8/38 Reg Loss: 0.023282749578356743 Val Accuracy: 94.44444444444444\n",
      "Epoch 850/1000 Loss: 0.004105233121663332 num_feat: 8/38 Reg Loss: 0.02327253483235836 Val Accuracy: 94.44444444444444\n",
      "Epoch 851/1000 Loss: 0.0014284923672676086 num_feat: 8/38 Reg Loss: 0.023261701688170433 Val Accuracy: 94.44444444444444\n",
      "Epoch 852/1000 Loss: 0.0011388115817680955 num_feat: 8/38 Reg Loss: 0.02325109764933586 Val Accuracy: 94.44444444444444\n",
      "Epoch 853/1000 Loss: 0.0014054197818040848 num_feat: 8/38 Reg Loss: 0.023240571841597557 Val Accuracy: 94.44444444444444\n",
      "Epoch 854/1000 Loss: 0.0013394082197919488 num_feat: 8/38 Reg Loss: 0.02323010191321373 Val Accuracy: 94.44444444444444\n",
      "Epoch 855/1000 Loss: 0.0012948166113346815 num_feat: 8/38 Reg Loss: 0.02321973256766796 Val Accuracy: 94.44444444444444\n",
      "Epoch 856/1000 Loss: 0.0021126428619027138 num_feat: 8/38 Reg Loss: 0.023209458217024803 Val Accuracy: 94.44444444444444\n",
      "Epoch 857/1000 Loss: 0.0019206060096621513 num_feat: 8/38 Reg Loss: 0.023199260234832764 Val Accuracy: 94.44444444444444\n",
      "Epoch 858/1000 Loss: 0.0013991823652759194 num_feat: 8/38 Reg Loss: 0.023189127445220947 Val Accuracy: 94.44444444444444\n",
      "Epoch 859/1000 Loss: 0.001971908612176776 num_feat: 8/38 Reg Loss: 0.023179080337285995 Val Accuracy: 94.44444444444444\n",
      "Epoch 860/1000 Loss: 0.00115046591963619 num_feat: 8/38 Reg Loss: 0.02316894941031933 Val Accuracy: 94.44444444444444\n",
      "Epoch 861/1000 Loss: 0.0012306671123951674 num_feat: 8/38 Reg Loss: 0.023158904165029526 Val Accuracy: 94.44444444444444\n",
      "Epoch 862/1000 Loss: 0.0017973908688873053 num_feat: 8/38 Reg Loss: 0.023148981854319572 Val Accuracy: 94.44444444444444\n",
      "Epoch 863/1000 Loss: 0.0014246637001633644 num_feat: 8/38 Reg Loss: 0.023139048367738724 Val Accuracy: 94.44444444444444\n",
      "Epoch 864/1000 Loss: 0.0020484267733991146 num_feat: 8/38 Reg Loss: 0.023129209876060486 Val Accuracy: 94.44444444444444\n",
      "Epoch 865/1000 Loss: 0.0012870148057118058 num_feat: 8/38 Reg Loss: 0.02311938814818859 Val Accuracy: 94.44444444444444\n",
      "Epoch 866/1000 Loss: 0.0011486735893413424 num_feat: 8/38 Reg Loss: 0.02310960739850998 Val Accuracy: 94.44444444444444\n",
      "Epoch 867/1000 Loss: 0.0018153516575694084 num_feat: 8/38 Reg Loss: 0.02309992164373398 Val Accuracy: 94.44444444444444\n",
      "Epoch 868/1000 Loss: 0.0011636167764663696 num_feat: 8/38 Reg Loss: 0.023090295493602753 Val Accuracy: 94.44444444444444\n",
      "Epoch 869/1000 Loss: 0.0010609342716634274 num_feat: 8/38 Reg Loss: 0.02308061346411705 Val Accuracy: 94.44444444444444\n",
      "Epoch 870/1000 Loss: 0.0011197486892342567 num_feat: 8/38 Reg Loss: 0.023070966824889183 Val Accuracy: 94.44444444444444\n",
      "Epoch 871/1000 Loss: 0.0014450831804424524 num_feat: 8/38 Reg Loss: 0.023061342537403107 Val Accuracy: 94.44444444444444\n",
      "Epoch 872/1000 Loss: 0.0012213251320645213 num_feat: 8/38 Reg Loss: 0.023051651194691658 Val Accuracy: 94.44444444444444\n",
      "Epoch 873/1000 Loss: 0.001499176025390625 num_feat: 8/38 Reg Loss: 0.023041944950819016 Val Accuracy: 94.44444444444444\n",
      "Epoch 874/1000 Loss: 0.0012270684819668531 num_feat: 8/38 Reg Loss: 0.02303217351436615 Val Accuracy: 94.44444444444444\n",
      "Epoch 875/1000 Loss: 0.001272403635084629 num_feat: 8/38 Reg Loss: 0.023022426292300224 Val Accuracy: 94.44444444444444\n",
      "Epoch 876/1000 Loss: 0.001206034328788519 num_feat: 8/38 Reg Loss: 0.023012695834040642 Val Accuracy: 94.44444444444444\n",
      "Epoch 877/1000 Loss: 0.0011530545307323337 num_feat: 8/38 Reg Loss: 0.02300299145281315 Val Accuracy: 94.44444444444444\n",
      "Epoch 878/1000 Loss: 0.03486640006303787 num_feat: 8/38 Reg Loss: 0.022993290796875954 Val Accuracy: 94.44444444444444\n",
      "Epoch 879/1000 Loss: 0.007590000983327627 num_feat: 8/38 Reg Loss: 0.022983385249972343 Val Accuracy: 94.44444444444444\n",
      "Epoch 880/1000 Loss: 0.03261727839708328 num_feat: 8/38 Reg Loss: 0.02297350950539112 Val Accuracy: 94.44444444444444\n",
      "Epoch 881/1000 Loss: 0.008831878192722797 num_feat: 8/38 Reg Loss: 0.022963671013712883 Val Accuracy: 94.44444444444444\n",
      "Epoch 882/1000 Loss: 0.0011152327060699463 num_feat: 8/38 Reg Loss: 0.022954419255256653 Val Accuracy: 94.44444444444444\n",
      "Epoch 883/1000 Loss: 0.00606209272518754 num_feat: 8/38 Reg Loss: 0.022945178672671318 Val Accuracy: 91.66666666666667\n",
      "Epoch 884/1000 Loss: 0.018471691757440567 num_feat: 8/38 Reg Loss: 0.022935761138796806 Val Accuracy: 91.66666666666667\n",
      "Epoch 885/1000 Loss: 0.11435071378946304 num_feat: 8/38 Reg Loss: 0.022927340120077133 Val Accuracy: 94.44444444444444\n",
      "Epoch 886/1000 Loss: 0.0011844453401863575 num_feat: 8/38 Reg Loss: 0.02291613630950451 Val Accuracy: 94.44444444444444\n",
      "Epoch 887/1000 Loss: 0.032194752246141434 num_feat: 8/38 Reg Loss: 0.022905226796865463 Val Accuracy: 94.44444444444444\n",
      "Epoch 888/1000 Loss: 0.0681748241186142 num_feat: 8/38 Reg Loss: 0.022892791777849197 Val Accuracy: 94.44444444444444\n",
      "Epoch 889/1000 Loss: 0.005093039013445377 num_feat: 8/38 Reg Loss: 0.02288009040057659 Val Accuracy: 94.44444444444444\n",
      "Epoch 890/1000 Loss: 0.003545850981026888 num_feat: 8/38 Reg Loss: 0.022867558524012566 Val Accuracy: 91.66666666666667\n",
      "Epoch 891/1000 Loss: 0.0675121322274208 num_feat: 8/38 Reg Loss: 0.022855376824736595 Val Accuracy: 94.44444444444444\n",
      "Epoch 892/1000 Loss: 0.0035536307841539383 num_feat: 8/38 Reg Loss: 0.02284042350947857 Val Accuracy: 94.44444444444444\n",
      "Epoch 893/1000 Loss: 0.0027398744132369757 num_feat: 8/38 Reg Loss: 0.022826028987765312 Val Accuracy: 94.44444444444444\n",
      "Epoch 894/1000 Loss: 0.025093598291277885 num_feat: 8/38 Reg Loss: 0.02281220816075802 Val Accuracy: 94.44444444444444\n",
      "Epoch 895/1000 Loss: 0.02123025432229042 num_feat: 8/38 Reg Loss: 0.022799616679549217 Val Accuracy: 94.44444444444444\n",
      "Epoch 896/1000 Loss: 0.0034143254160881042 num_feat: 8/38 Reg Loss: 0.02278800494968891 Val Accuracy: 94.44444444444444\n",
      "Epoch 897/1000 Loss: 0.0023413891904056072 num_feat: 8/38 Reg Loss: 0.022776449099183083 Val Accuracy: 94.44444444444444\n",
      "Epoch 898/1000 Loss: 0.013305477797985077 num_feat: 8/38 Reg Loss: 0.02276510000228882 Val Accuracy: 91.66666666666667\n",
      "Epoch 899/1000 Loss: 0.08894138783216476 num_feat: 8/38 Reg Loss: 0.02275398001074791 Val Accuracy: 94.44444444444444\n",
      "Epoch 900/1000 Loss: 0.0068810745142400265 num_feat: 8/38 Reg Loss: 0.022740283980965614 Val Accuracy: 94.44444444444444\n",
      "Epoch 901/1000 Loss: 0.14151056110858917 num_feat: 8/38 Reg Loss: 0.02272697538137436 Val Accuracy: 94.44444444444444\n",
      "Epoch 902/1000 Loss: 0.0316837839782238 num_feat: 8/38 Reg Loss: 0.02271057479083538 Val Accuracy: 94.44444444444444\n",
      "Epoch 903/1000 Loss: 0.0018739604856818914 num_feat: 8/38 Reg Loss: 0.022695589810609818 Val Accuracy: 91.66666666666667\n",
      "Epoch 904/1000 Loss: 0.062077466398477554 num_feat: 8/38 Reg Loss: 0.02268119715154171 Val Accuracy: 91.66666666666667\n",
      "Epoch 905/1000 Loss: 0.029345382004976273 num_feat: 8/38 Reg Loss: 0.022667357698082924 Val Accuracy: 94.44444444444444\n",
      "Epoch 906/1000 Loss: 0.001455560326576233 num_feat: 8/38 Reg Loss: 0.022654011845588684 Val Accuracy: 94.44444444444444\n",
      "Epoch 907/1000 Loss: 0.033812470734119415 num_feat: 8/38 Reg Loss: 0.022641105577349663 Val Accuracy: 94.44444444444444\n",
      "Epoch 908/1000 Loss: 0.04972410202026367 num_feat: 8/38 Reg Loss: 0.02262824773788452 Val Accuracy: 94.44444444444444\n",
      "Epoch 909/1000 Loss: 0.0022128622513264418 num_feat: 8/38 Reg Loss: 0.022615762427449226 Val Accuracy: 94.44444444444444\n",
      "Epoch 910/1000 Loss: 0.001780209131538868 num_feat: 8/38 Reg Loss: 0.022603577002882957 Val Accuracy: 94.44444444444444\n",
      "Epoch 911/1000 Loss: 0.019573114812374115 num_feat: 8/38 Reg Loss: 0.02259170450270176 Val Accuracy: 94.44444444444444\n",
      "Epoch 912/1000 Loss: 0.02026151306927204 num_feat: 8/38 Reg Loss: 0.02257906086742878 Val Accuracy: 94.44444444444444\n",
      "Epoch 913/1000 Loss: 0.002130369422957301 num_feat: 8/38 Reg Loss: 0.022566862404346466 Val Accuracy: 94.44444444444444\n",
      "Epoch 914/1000 Loss: 0.0021756074856966734 num_feat: 8/38 Reg Loss: 0.02255498804152012 Val Accuracy: 94.44444444444444\n",
      "Epoch 915/1000 Loss: 0.023591268807649612 num_feat: 8/38 Reg Loss: 0.022543510422110558 Val Accuracy: 94.44444444444444\n",
      "Epoch 916/1000 Loss: 0.010065493173897266 num_feat: 8/38 Reg Loss: 0.022529948502779007 Val Accuracy: 94.44444444444444\n",
      "Epoch 917/1000 Loss: 0.0018152493285015225 num_feat: 8/38 Reg Loss: 0.022515881806612015 Val Accuracy: 94.44444444444444\n",
      "Epoch 918/1000 Loss: 0.00117120158392936 num_feat: 8/38 Reg Loss: 0.022502291947603226 Val Accuracy: 94.44444444444444\n",
      "Epoch 919/1000 Loss: 0.003951132297515869 num_feat: 8/38 Reg Loss: 0.022489188238978386 Val Accuracy: 94.44444444444444\n",
      "Epoch 920/1000 Loss: 0.007993225008249283 num_feat: 8/38 Reg Loss: 0.02247687615454197 Val Accuracy: 94.44444444444444\n",
      "Epoch 921/1000 Loss: 0.006711187772452831 num_feat: 8/38 Reg Loss: 0.022465717047452927 Val Accuracy: 94.44444444444444\n",
      "Epoch 922/1000 Loss: 0.0020414898172020912 num_feat: 8/38 Reg Loss: 0.022455431520938873 Val Accuracy: 94.44444444444444\n",
      "Epoch 923/1000 Loss: 0.001037477981299162 num_feat: 8/38 Reg Loss: 0.0224455539137125 Val Accuracy: 94.44444444444444\n",
      "Epoch 924/1000 Loss: 0.0015532920369878411 num_feat: 8/38 Reg Loss: 0.022435858845710754 Val Accuracy: 94.44444444444444\n",
      "Epoch 925/1000 Loss: 0.0031285618897527456 num_feat: 8/38 Reg Loss: 0.022426242008805275 Val Accuracy: 94.44444444444444\n",
      "Epoch 926/1000 Loss: 0.00376433739438653 num_feat: 8/38 Reg Loss: 0.0224168598651886 Val Accuracy: 94.44444444444444\n",
      "Epoch 927/1000 Loss: 0.004853823687881231 num_feat: 8/38 Reg Loss: 0.022407997399568558 Val Accuracy: 94.44444444444444\n",
      "Epoch 928/1000 Loss: 0.0016729026101529598 num_feat: 8/38 Reg Loss: 0.02239842899143696 Val Accuracy: 94.44444444444444\n",
      "Epoch 929/1000 Loss: 0.002023919951170683 num_feat: 8/38 Reg Loss: 0.022388821467757225 Val Accuracy: 94.44444444444444\n",
      "Epoch 930/1000 Loss: 0.0011198832653462887 num_feat: 8/38 Reg Loss: 0.022378867492079735 Val Accuracy: 94.44444444444444\n",
      "Epoch 931/1000 Loss: 0.0017295652069151402 num_feat: 8/38 Reg Loss: 0.022369015961885452 Val Accuracy: 94.44444444444444\n",
      "Epoch 932/1000 Loss: 0.0021340239327400923 num_feat: 8/38 Reg Loss: 0.02235926128923893 Val Accuracy: 94.44444444444444\n",
      "Epoch 933/1000 Loss: 0.002334429882466793 num_feat: 8/38 Reg Loss: 0.022349655628204346 Val Accuracy: 94.44444444444444\n",
      "Epoch 934/1000 Loss: 0.003421289613470435 num_feat: 8/38 Reg Loss: 0.02234048582613468 Val Accuracy: 94.44444444444444\n",
      "Epoch 935/1000 Loss: 0.002856555860489607 num_feat: 8/38 Reg Loss: 0.022331304848194122 Val Accuracy: 94.44444444444444\n",
      "Epoch 936/1000 Loss: 0.0010111161973327398 num_feat: 8/38 Reg Loss: 0.0223222803324461 Val Accuracy: 94.44444444444444\n",
      "Epoch 937/1000 Loss: 0.0010351886739954352 num_feat: 8/38 Reg Loss: 0.022313328459858894 Val Accuracy: 94.44444444444444\n",
      "Epoch 938/1000 Loss: 0.001437692204490304 num_feat: 8/38 Reg Loss: 0.022304417565464973 Val Accuracy: 94.44444444444444\n",
      "Epoch 939/1000 Loss: 0.0017344511579722166 num_feat: 8/38 Reg Loss: 0.022295497357845306 Val Accuracy: 94.44444444444444\n",
      "Epoch 940/1000 Loss: 0.0022834520787000656 num_feat: 8/38 Reg Loss: 0.02228640392422676 Val Accuracy: 94.44444444444444\n",
      "Epoch 941/1000 Loss: 0.0032642437145113945 num_feat: 8/38 Reg Loss: 0.022277338430285454 Val Accuracy: 94.44444444444444\n",
      "Epoch 942/1000 Loss: 0.0014233568217605352 num_feat: 8/38 Reg Loss: 0.022268062457442284 Val Accuracy: 94.44444444444444\n",
      "Epoch 943/1000 Loss: 0.0010616822401061654 num_feat: 8/38 Reg Loss: 0.022258838638663292 Val Accuracy: 94.44444444444444\n",
      "Epoch 944/1000 Loss: 0.000990324537269771 num_feat: 8/38 Reg Loss: 0.02224966324865818 Val Accuracy: 94.44444444444444\n",
      "Epoch 945/1000 Loss: 0.001462597749195993 num_feat: 8/38 Reg Loss: 0.02224053628742695 Val Accuracy: 94.44444444444444\n",
      "Epoch 946/1000 Loss: 0.0015642009675502777 num_feat: 8/38 Reg Loss: 0.022231416776776314 Val Accuracy: 94.44444444444444\n",
      "Epoch 947/1000 Loss: 0.002245707903057337 num_feat: 8/38 Reg Loss: 0.02222231775522232 Val Accuracy: 94.44444444444444\n",
      "Epoch 948/1000 Loss: 0.000986924977041781 num_feat: 8/38 Reg Loss: 0.0222135242074728 Val Accuracy: 94.44444444444444\n",
      "Epoch 949/1000 Loss: 0.0011223730398342013 num_feat: 8/38 Reg Loss: 0.02220485918223858 Val Accuracy: 94.44444444444444\n",
      "Epoch 950/1000 Loss: 0.001000714022666216 num_feat: 8/38 Reg Loss: 0.022196194157004356 Val Accuracy: 94.44444444444444\n",
      "Epoch 951/1000 Loss: 0.0009542425977997482 num_feat: 8/38 Reg Loss: 0.022187530994415283 Val Accuracy: 94.44444444444444\n",
      "Epoch 952/1000 Loss: 0.0009587510139681399 num_feat: 8/38 Reg Loss: 0.02217886969447136 Val Accuracy: 94.44444444444444\n",
      "Epoch 953/1000 Loss: 0.0009826194727793336 num_feat: 8/38 Reg Loss: 0.022170085459947586 Val Accuracy: 94.44444444444444\n",
      "Epoch 954/1000 Loss: 0.001063130097463727 num_feat: 8/38 Reg Loss: 0.0221613310277462 Val Accuracy: 94.44444444444444\n",
      "Epoch 955/1000 Loss: 0.0010760308941826224 num_feat: 8/38 Reg Loss: 0.022152593359351158 Val Accuracy: 94.44444444444444\n",
      "Epoch 956/1000 Loss: 0.001962674781680107 num_feat: 8/38 Reg Loss: 0.02214386686682701 Val Accuracy: 94.44444444444444\n",
      "Epoch 957/1000 Loss: 0.0008881368557922542 num_feat: 8/38 Reg Loss: 0.02213515155017376 Val Accuracy: 94.44444444444444\n",
      "Epoch 958/1000 Loss: 0.0009277596836909652 num_feat: 8/38 Reg Loss: 0.0221264511346817 Val Accuracy: 94.44444444444444\n",
      "Epoch 959/1000 Loss: 0.001240273704752326 num_feat: 7/38 Reg Loss: 0.022117769345641136 Val Accuracy: 94.44444444444444\n",
      "Epoch 960/1000 Loss: 0.0008564310846850276 num_feat: 7/38 Reg Loss: 0.022108877077698708 Val Accuracy: 94.44444444444444\n",
      "Epoch 961/1000 Loss: 0.0012222295626997948 num_feat: 7/38 Reg Loss: 0.02210000716149807 Val Accuracy: 94.44444444444444\n",
      "Epoch 962/1000 Loss: 0.0012785757426172495 num_feat: 7/38 Reg Loss: 0.02209116891026497 Val Accuracy: 94.44444444444444\n",
      "Epoch 963/1000 Loss: 0.001218923251144588 num_feat: 7/38 Reg Loss: 0.022082494571805 Val Accuracy: 94.44444444444444\n",
      "Epoch 964/1000 Loss: 0.008660971187055111 num_feat: 7/38 Reg Loss: 0.022074004635214806 Val Accuracy: 94.44444444444444\n",
      "Epoch 965/1000 Loss: 0.0009378008544445038 num_feat: 7/38 Reg Loss: 0.02206551842391491 Val Accuracy: 94.44444444444444\n",
      "Epoch 966/1000 Loss: 0.003910887986421585 num_feat: 7/38 Reg Loss: 0.022057032212615013 Val Accuracy: 94.44444444444444\n",
      "Epoch 967/1000 Loss: 0.0035561479162424803 num_feat: 7/38 Reg Loss: 0.022048546001315117 Val Accuracy: 94.44444444444444\n",
      "Epoch 968/1000 Loss: 0.005544870160520077 num_feat: 7/38 Reg Loss: 0.022039948031306267 Val Accuracy: 94.44444444444444\n",
      "Epoch 969/1000 Loss: 0.001500162878073752 num_feat: 7/38 Reg Loss: 0.02203141525387764 Val Accuracy: 94.44444444444444\n",
      "Epoch 970/1000 Loss: 0.0009285231935791671 num_feat: 7/38 Reg Loss: 0.022022893652319908 Val Accuracy: 94.44444444444444\n",
      "Epoch 971/1000 Loss: 0.0010191143956035376 num_feat: 7/38 Reg Loss: 0.022014383226633072 Val Accuracy: 72.22222222222223\n",
      "Epoch 972/1000 Loss: 0.0015704771503806114 num_feat: 7/38 Reg Loss: 0.02200588770210743 Val Accuracy: 94.44444444444444\n",
      "Epoch 973/1000 Loss: 0.0016954990569502115 num_feat: 7/38 Reg Loss: 0.02199740521609783 Val Accuracy: 94.44444444444444\n",
      "Epoch 974/1000 Loss: 0.0016453309217467904 num_feat: 7/38 Reg Loss: 0.02198917046189308 Val Accuracy: 94.44444444444444\n",
      "Epoch 975/1000 Loss: 0.0023137640673667192 num_feat: 7/38 Reg Loss: 0.0219810139387846 Val Accuracy: 94.44444444444444\n",
      "Epoch 976/1000 Loss: 0.003035882720723748 num_feat: 7/38 Reg Loss: 0.021972961723804474 Val Accuracy: 94.44444444444444\n",
      "Epoch 977/1000 Loss: 0.000980660319328308 num_feat: 7/38 Reg Loss: 0.021964946761727333 Val Accuracy: 94.44444444444444\n",
      "Epoch 978/1000 Loss: 0.0009451930527575314 num_feat: 7/38 Reg Loss: 0.02195693552494049 Val Accuracy: 94.44444444444444\n",
      "Epoch 979/1000 Loss: 0.0010546762496232986 num_feat: 7/38 Reg Loss: 0.021948834881186485 Val Accuracy: 94.44444444444444\n",
      "Epoch 980/1000 Loss: 0.0008571454090997577 num_feat: 7/38 Reg Loss: 0.02194071374833584 Val Accuracy: 94.44444444444444\n",
      "Epoch 981/1000 Loss: 0.0018588636303320527 num_feat: 7/38 Reg Loss: 0.021932631731033325 Val Accuracy: 94.44444444444444\n",
      "Epoch 982/1000 Loss: 0.001683249487541616 num_feat: 7/38 Reg Loss: 0.021924538537859917 Val Accuracy: 94.44444444444444\n",
      "Epoch 983/1000 Loss: 0.0017812317237257957 num_feat: 7/38 Reg Loss: 0.021916482597589493 Val Accuracy: 94.44444444444444\n",
      "Epoch 984/1000 Loss: 0.0013417481677606702 num_feat: 7/38 Reg Loss: 0.021908430382609367 Val Accuracy: 94.44444444444444\n",
      "Epoch 985/1000 Loss: 1.1876550912857056 num_feat: 7/38 Reg Loss: 0.02190038375556469 Val Accuracy: 91.66666666666667\n",
      "Epoch 986/1000 Loss: 0.6133225560188293 num_feat: 7/38 Reg Loss: 0.021898919716477394 Val Accuracy: 91.66666666666667\n",
      "Epoch 987/1000 Loss: 0.78526371717453 num_feat: 7/38 Reg Loss: 0.021895533427596092 Val Accuracy: 94.44444444444444\n",
      "Epoch 988/1000 Loss: 0.14738251268863678 num_feat: 7/38 Reg Loss: 0.02189096249639988 Val Accuracy: 91.66666666666667\n",
      "Epoch 989/1000 Loss: 0.07931007444858551 num_feat: 7/38 Reg Loss: 0.021887699142098427 Val Accuracy: 94.44444444444444\n",
      "Epoch 990/1000 Loss: 0.39174774289131165 num_feat: 7/38 Reg Loss: 0.021884111687541008 Val Accuracy: 97.22222222222223\n",
      "Epoch 991/1000 Loss: 0.183510422706604 num_feat: 7/38 Reg Loss: 0.02188296616077423 Val Accuracy: 94.44444444444444\n",
      "Epoch 992/1000 Loss: 0.012694195844233036 num_feat: 7/38 Reg Loss: 0.021891649812459946 Val Accuracy: 94.44444444444444\n",
      "Epoch 993/1000 Loss: 0.2527550756931305 num_feat: 7/38 Reg Loss: 0.02189641259610653 Val Accuracy: 94.44444444444444\n",
      "Epoch 994/1000 Loss: 0.2563869059085846 num_feat: 7/38 Reg Loss: 0.021880026906728745 Val Accuracy: 94.44444444444444\n",
      "Epoch 995/1000 Loss: 0.027352426201105118 num_feat: 7/38 Reg Loss: 0.021840155124664307 Val Accuracy: 94.44444444444444\n",
      "Epoch 996/1000 Loss: 0.03469843044877052 num_feat: 7/38 Reg Loss: 0.0218033604323864 Val Accuracy: 97.22222222222223\n",
      "Epoch 997/1000 Loss: 0.20166584849357605 num_feat: 7/38 Reg Loss: 0.021778568625450134 Val Accuracy: 97.22222222222223\n",
      "Epoch 998/1000 Loss: 0.10713846236467361 num_feat: 7/38 Reg Loss: 0.02176719158887863 Val Accuracy: 94.44444444444444\n",
      "Epoch 999/1000 Loss: 0.010934455320239067 num_feat: 7/38 Reg Loss: 0.02175723947584629 Val Accuracy: 94.44444444444444\n",
      "Epoch 1000/1000 Loss: 0.10667768120765686 num_feat: 7/38 Reg Loss: 0.021748395636677742 Val Accuracy: 94.44444444444444\n",
      "Accuracy = 96.66666666666667%\n",
      "96.66666666666667%, 34 features\n"
     ]
    }
   ],
   "source": [
    "X, y = shap.datasets.iris()\n",
    "accs = []\n",
    "total_feature = []\n",
    "warnings.simplefilter(action='ignore')\n",
    "X_train, X_test, Y_train, Y_test = insert_feature_noise(X, y, \n",
    "            num_random_noise=num_normal[-1],\n",
    "            num_overwhelemed=num_overwhelmed[-1], \n",
    "            num_shortcut=num_shortcut[-1])\n",
    "X_train, y_train = X_train.to_numpy().astype(np.float32), Y_train.astype(np.int64)\n",
    "X_test, y_test = X_test.to_numpy().astype(np.float32), Y_test.astype(np.int64)\n",
    "clf = skw.ScikitClfWrapper(\n",
    "        input_dim=X_train.shape[1],\n",
    "        number_of_classes=3,\n",
    "        hidden_dims=(16, 16), lam=0.1, epochs=1000, sigma=0.5,\n",
    "        freeze_till=0, lr=0.1, verbose=True,\n",
    "        device='cpu')\n",
    "clf.fit(X_train, Y_train)\n",
    "acc = print_accuracy(clf.predict, X_test, Y_test)\n",
    "total_feature = 2*num_normal[i] + num_overwhelmed[i] + num_shortcut[i]\n",
    "print(f\"{acc}%, {total_feature} features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epoch</th>\n",
       "      <th>valid_acc</th>\n",
       "      <th># feature</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>27.777778</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>27.777778</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>27.777778</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>27.777778</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>27.777778</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   epoch  valid_acc  # feature\n",
       "0      0  27.777778         38\n",
       "1      1  27.777778         38\n",
       "2      2  27.777778         38\n",
       "3      3  27.777778         38\n",
       "4      4  27.777778         38"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stg_history = clf.get_history()\n",
    "stg_history.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x22b94c60d60>]"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\r\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\r\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\r\n<svg height=\"248.518125pt\" version=\"1.1\" viewBox=\"0 0 368.925 248.518125\" width=\"368.925pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\r\n <metadata>\r\n  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\r\n   <cc:Work>\r\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\r\n    <dc:date>2022-03-29T21:23:24.292825</dc:date>\r\n    <dc:format>image/svg+xml</dc:format>\r\n    <dc:creator>\r\n     <cc:Agent>\r\n      <dc:title>Matplotlib v3.4.3, https://matplotlib.org/</dc:title>\r\n     </cc:Agent>\r\n    </dc:creator>\r\n   </cc:Work>\r\n  </rdf:RDF>\r\n </metadata>\r\n <defs>\r\n  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\r\n </defs>\r\n <g id=\"figure_1\">\r\n  <g id=\"patch_1\">\r\n   <path d=\"M -0 248.518125 \r\nL 368.925 248.518125 \r\nL 368.925 0 \r\nL -0 0 \r\nz\r\n\" style=\"fill:none;\"/>\r\n  </g>\r\n  <g id=\"axes_1\">\r\n   <g id=\"patch_2\">\r\n    <path d=\"M 26.925 224.64 \r\nL 361.725 224.64 \r\nL 361.725 7.2 \r\nL 26.925 7.2 \r\nz\r\n\" style=\"fill:#ffffff;\"/>\r\n   </g>\r\n   <g id=\"matplotlib.axis_1\">\r\n    <g id=\"xtick_1\">\r\n     <g id=\"line2d_1\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL 0 3.5 \r\n\" id=\"m39939ae381\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"42.143182\" xlink:href=\"#m39939ae381\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_1\">\r\n      <!-- 0 -->\r\n      <g transform=\"translate(38.961932 239.238438)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 2034 4250 \r\nQ 1547 4250 1301 3770 \r\nQ 1056 3291 1056 2328 \r\nQ 1056 1369 1301 889 \r\nQ 1547 409 2034 409 \r\nQ 2525 409 2770 889 \r\nQ 3016 1369 3016 2328 \r\nQ 3016 3291 2770 3770 \r\nQ 2525 4250 2034 4250 \r\nz\r\nM 2034 4750 \r\nQ 2819 4750 3233 4129 \r\nQ 3647 3509 3647 2328 \r\nQ 3647 1150 3233 529 \r\nQ 2819 -91 2034 -91 \r\nQ 1250 -91 836 529 \r\nQ 422 1150 422 2328 \r\nQ 422 3509 836 4129 \r\nQ 1250 4750 2034 4750 \r\nz\r\n\" id=\"DejaVuSans-30\" transform=\"scale(0.015625)\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_2\">\r\n     <g id=\"line2d_2\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"103.076843\" xlink:href=\"#m39939ae381\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_2\">\r\n      <!-- 200 -->\r\n      <g transform=\"translate(93.533093 239.238438)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 1228 531 \r\nL 3431 531 \r\nL 3431 0 \r\nL 469 0 \r\nL 469 531 \r\nQ 828 903 1448 1529 \r\nQ 2069 2156 2228 2338 \r\nQ 2531 2678 2651 2914 \r\nQ 2772 3150 2772 3378 \r\nQ 2772 3750 2511 3984 \r\nQ 2250 4219 1831 4219 \r\nQ 1534 4219 1204 4116 \r\nQ 875 4013 500 3803 \r\nL 500 4441 \r\nQ 881 4594 1212 4672 \r\nQ 1544 4750 1819 4750 \r\nQ 2544 4750 2975 4387 \r\nQ 3406 4025 3406 3419 \r\nQ 3406 3131 3298 2873 \r\nQ 3191 2616 2906 2266 \r\nQ 2828 2175 2409 1742 \r\nQ 1991 1309 1228 531 \r\nz\r\n\" id=\"DejaVuSans-32\" transform=\"scale(0.015625)\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-32\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_3\">\r\n     <g id=\"line2d_3\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"164.010504\" xlink:href=\"#m39939ae381\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_3\">\r\n      <!-- 400 -->\r\n      <g transform=\"translate(154.466754 239.238438)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 2419 4116 \r\nL 825 1625 \r\nL 2419 1625 \r\nL 2419 4116 \r\nz\r\nM 2253 4666 \r\nL 3047 4666 \r\nL 3047 1625 \r\nL 3713 1625 \r\nL 3713 1100 \r\nL 3047 1100 \r\nL 3047 0 \r\nL 2419 0 \r\nL 2419 1100 \r\nL 313 1100 \r\nL 313 1709 \r\nL 2253 4666 \r\nz\r\n\" id=\"DejaVuSans-34\" transform=\"scale(0.015625)\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-34\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_4\">\r\n     <g id=\"line2d_4\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"224.944165\" xlink:href=\"#m39939ae381\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_4\">\r\n      <!-- 600 -->\r\n      <g transform=\"translate(215.400415 239.238438)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 2113 2584 \r\nQ 1688 2584 1439 2293 \r\nQ 1191 2003 1191 1497 \r\nQ 1191 994 1439 701 \r\nQ 1688 409 2113 409 \r\nQ 2538 409 2786 701 \r\nQ 3034 994 3034 1497 \r\nQ 3034 2003 2786 2293 \r\nQ 2538 2584 2113 2584 \r\nz\r\nM 3366 4563 \r\nL 3366 3988 \r\nQ 3128 4100 2886 4159 \r\nQ 2644 4219 2406 4219 \r\nQ 1781 4219 1451 3797 \r\nQ 1122 3375 1075 2522 \r\nQ 1259 2794 1537 2939 \r\nQ 1816 3084 2150 3084 \r\nQ 2853 3084 3261 2657 \r\nQ 3669 2231 3669 1497 \r\nQ 3669 778 3244 343 \r\nQ 2819 -91 2113 -91 \r\nQ 1303 -91 875 529 \r\nQ 447 1150 447 2328 \r\nQ 447 3434 972 4092 \r\nQ 1497 4750 2381 4750 \r\nQ 2619 4750 2861 4703 \r\nQ 3103 4656 3366 4563 \r\nz\r\n\" id=\"DejaVuSans-36\" transform=\"scale(0.015625)\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-36\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_5\">\r\n     <g id=\"line2d_5\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"285.877826\" xlink:href=\"#m39939ae381\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_5\">\r\n      <!-- 800 -->\r\n      <g transform=\"translate(276.334076 239.238438)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 2034 2216 \r\nQ 1584 2216 1326 1975 \r\nQ 1069 1734 1069 1313 \r\nQ 1069 891 1326 650 \r\nQ 1584 409 2034 409 \r\nQ 2484 409 2743 651 \r\nQ 3003 894 3003 1313 \r\nQ 3003 1734 2745 1975 \r\nQ 2488 2216 2034 2216 \r\nz\r\nM 1403 2484 \r\nQ 997 2584 770 2862 \r\nQ 544 3141 544 3541 \r\nQ 544 4100 942 4425 \r\nQ 1341 4750 2034 4750 \r\nQ 2731 4750 3128 4425 \r\nQ 3525 4100 3525 3541 \r\nQ 3525 3141 3298 2862 \r\nQ 3072 2584 2669 2484 \r\nQ 3125 2378 3379 2068 \r\nQ 3634 1759 3634 1313 \r\nQ 3634 634 3220 271 \r\nQ 2806 -91 2034 -91 \r\nQ 1263 -91 848 271 \r\nQ 434 634 434 1313 \r\nQ 434 1759 690 2068 \r\nQ 947 2378 1403 2484 \r\nz\r\nM 1172 3481 \r\nQ 1172 3119 1398 2916 \r\nQ 1625 2713 2034 2713 \r\nQ 2441 2713 2670 2916 \r\nQ 2900 3119 2900 3481 \r\nQ 2900 3844 2670 4047 \r\nQ 2441 4250 2034 4250 \r\nQ 1625 4250 1398 4047 \r\nQ 1172 3844 1172 3481 \r\nz\r\n\" id=\"DejaVuSans-38\" transform=\"scale(0.015625)\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-38\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_6\">\r\n     <g id=\"line2d_6\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"346.811486\" xlink:href=\"#m39939ae381\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_6\">\r\n      <!-- 1000 -->\r\n      <g transform=\"translate(334.086486 239.238438)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 794 531 \r\nL 1825 531 \r\nL 1825 4091 \r\nL 703 3866 \r\nL 703 4441 \r\nL 1819 4666 \r\nL 2450 4666 \r\nL 2450 531 \r\nL 3481 531 \r\nL 3481 0 \r\nL 794 0 \r\nL 794 531 \r\nz\r\n\" id=\"DejaVuSans-31\" transform=\"scale(0.015625)\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-31\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\r\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"matplotlib.axis_2\">\r\n    <g id=\"ytick_1\">\r\n     <g id=\"line2d_7\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL -3.5 0 \r\n\" id=\"ma163f9c66a\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#ma163f9c66a\" y=\"195.691927\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_7\">\r\n      <!-- 2 -->\r\n      <g transform=\"translate(13.5625 199.491146)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-32\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_2\">\r\n     <g id=\"line2d_8\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#ma163f9c66a\" y=\"165.645673\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_8\">\r\n      <!-- 4 -->\r\n      <g transform=\"translate(13.5625 169.444891)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-34\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_3\">\r\n     <g id=\"line2d_9\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#ma163f9c66a\" y=\"135.599418\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_9\">\r\n      <!-- 6 -->\r\n      <g transform=\"translate(13.5625 139.398637)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-36\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_4\">\r\n     <g id=\"line2d_10\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#ma163f9c66a\" y=\"105.553164\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_10\">\r\n      <!-- 8 -->\r\n      <g transform=\"translate(13.5625 109.352382)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-38\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_5\">\r\n     <g id=\"line2d_11\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#ma163f9c66a\" y=\"75.506909\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_11\">\r\n      <!-- 10 -->\r\n      <g transform=\"translate(7.2 79.306128)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-31\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_6\">\r\n     <g id=\"line2d_12\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#ma163f9c66a\" y=\"45.460655\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_12\">\r\n      <!-- 12 -->\r\n      <g transform=\"translate(7.2 49.259873)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-31\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-32\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_7\">\r\n     <g id=\"line2d_13\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#ma163f9c66a\" y=\"15.4144\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_13\">\r\n      <!-- 14 -->\r\n      <g transform=\"translate(7.2 19.213619)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-31\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-34\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"line2d_14\">\r\n    <path clip-path=\"url(#p19ca4dcc6c)\" d=\"M 42.143182 214.756364 \r\nL 49.759889 214.756364 \r\nL 50.369226 209.265455 \r\nL 50.673894 213.658182 \r\nL 50.978563 214.756364 \r\nL 51.283231 200.48 \r\nL 51.587899 214.756364 \r\nL 51.892568 205.970909 \r\nL 52.197236 212.56 \r\nL 52.501904 199.381818 \r\nL 52.806572 205.970909 \r\nL 53.111241 198.283636 \r\nL 54.329914 198.283636 \r\nL 54.634582 196.087273 \r\nL 54.939251 198.283636 \r\nL 55.243919 207.069091 \r\nL 55.548587 193.890909 \r\nL 55.853256 191.694545 \r\nL 56.462592 193.890909 \r\nL 56.76726 198.283636 \r\nL 57.071929 191.694545 \r\nL 57.376597 193.890909 \r\nL 57.681265 194.989091 \r\nL 57.985934 200.48 \r\nL 58.290602 199.381818 \r\nL 58.59527 197.185455 \r\nL 58.899939 193.890909 \r\nL 59.204607 193.890909 \r\nL 59.509275 196.087273 \r\nL 59.813943 194.989091 \r\nL 60.118612 192.792727 \r\nL 60.42328 196.087273 \r\nL 60.727948 194.989091 \r\nL 61.032617 194.989091 \r\nL 61.337285 190.596364 \r\nL 61.641953 194.989091 \r\nL 61.946622 196.087273 \r\nL 62.25129 190.596364 \r\nL 62.555958 189.498182 \r\nL 62.860627 191.694545 \r\nL 63.165295 191.694545 \r\nL 63.774631 200.48 \r\nL 64.0793 189.498182 \r\nL 64.383968 193.890909 \r\nL 64.688636 194.989091 \r\nL 64.993305 192.792727 \r\nL 65.297973 194.989091 \r\nL 65.90731 194.989091 \r\nL 66.211978 193.890909 \r\nL 66.516646 191.694545 \r\nL 66.821314 195.285897 \r\nL 67.125983 190.774447 \r\nL 67.430651 203.180934 \r\nL 67.735319 189.646585 \r\nL 68.039988 198.669484 \r\nL 68.344656 189.646585 \r\nL 68.649324 193.030172 \r\nL 68.953993 193.030172 \r\nL 69.258661 194.44 \r\nL 69.563329 193.280808 \r\nL 69.867998 189.803232 \r\nL 70.172666 190.144171 \r\nL 70.477334 188.776519 \r\nL 70.782002 189.968831 \r\nL 71.086671 187.689412 \r\nL 71.391339 184.007273 \r\nL 71.696007 186.462032 \r\nL 72.000676 187.689412 \r\nL 72.610012 182.7427 \r\nL 72.914681 184.007273 \r\nL 73.219349 187.800992 \r\nL 73.524017 180.095 \r\nL 73.828686 180.095 \r\nL 74.133354 184.007273 \r\nL 74.438022 182.703182 \r\nL 74.74269 185.311364 \r\nL 75.047359 180.095 \r\nL 75.352027 190.527727 \r\nL 75.656695 189.223636 \r\nL 75.961364 179.968798 \r\nL 76.266032 188.045748 \r\nL 76.5707 181.225212 \r\nL 76.875369 181.026494 \r\nL 77.180037 194.44 \r\nL 77.484705 191.735219 \r\nL 77.789373 185.552862 \r\nL 78.094042 175.064935 \r\nL 78.39871 184.007273 \r\nL 78.703378 185.497662 \r\nL 79.008047 182.516883 \r\nL 79.312715 178.251285 \r\nL 79.617383 186.885266 \r\nL 79.922052 184.007273 \r\nL 80.22672 176.279327 \r\nL 80.531388 173.188148 \r\nL 80.836057 179.370505 \r\nL 81.140725 175.982098 \r\nL 81.445393 175.982098 \r\nL 81.750061 168.984145 \r\nL 82.05473 170.653382 \r\nL 82.359398 173.991855 \r\nL 82.664066 168.984145 \r\nL 82.968735 177.330327 \r\nL 83.273403 173.991855 \r\nL 83.578071 173.991855 \r\nL 83.88274 171.835758 \r\nL 84.187408 165.863399 \r\nL 84.492076 166.935537 \r\nL 84.796744 166.935537 \r\nL 85.101413 163.141818 \r\nL 85.406081 163.141818 \r\nL 85.710749 161.244959 \r\nL 86.015418 161.244959 \r\nL 86.320086 162.148225 \r\nL 86.624754 156.882182 \r\nL 86.929423 158.968727 \r\nL 87.234091 158.968727 \r\nL 87.538759 157.650909 \r\nL 87.843428 151.061818 \r\nL 88.148096 151.061818 \r\nL 88.452764 148.865455 \r\nL 88.757432 151.061818 \r\nL 89.366769 151.061818 \r\nL 89.671437 159.847273 \r\nL 90.280774 151.061818 \r\nL 90.585442 155.454545 \r\nL 90.890111 163.141818 \r\nL 91.194779 152.709091 \r\nL 91.499447 154.795636 \r\nL 91.804115 158.968727 \r\nL 92.413452 154.795636 \r\nL 92.71812 158.968727 \r\nL 93.022789 165.228364 \r\nL 93.327457 163.141818 \r\nL 93.632125 173.574545 \r\nL 94.241462 154.795636 \r\nL 94.54613 154.795636 \r\nL 94.850799 157.650909 \r\nL 95.155467 157.650909 \r\nL 95.460135 156.882182 \r\nL 95.764803 154.795636 \r\nL 96.069472 161.055273 \r\nL 96.37414 156.882182 \r\nL 96.678808 157.650909 \r\nL 96.983477 153.258182 \r\nL 97.288145 151.061818 \r\nL 97.592813 151.061818 \r\nL 97.897482 157.650909 \r\nL 98.20215 151.061818 \r\nL 98.506818 151.061818 \r\nL 98.811486 148.865455 \r\nL 99.116155 153.258182 \r\nL 99.420823 162.043636 \r\nL 99.725491 151.061818 \r\nL 100.03016 153.258182 \r\nL 100.334828 151.061818 \r\nL 100.639496 151.061818 \r\nL 100.944165 153.258182 \r\nL 101.248833 149.231515 \r\nL 101.553501 142.276364 \r\nL 101.85817 139.821604 \r\nL 102.162838 142.276364 \r\nL 102.467506 149.640642 \r\nL 102.772174 142.276364 \r\nL 103.076843 142.276364 \r\nL 103.381511 149.640642 \r\nL 103.686179 144.884545 \r\nL 103.990848 142.276364 \r\nL 104.295516 142.276364 \r\nL 104.600184 137.06 \r\nL 105.818857 137.06 \r\nL 106.123526 139.668182 \r\nL 106.428194 137.06 \r\nL 107.037531 137.06 \r\nL 107.342199 134.451818 \r\nL 107.951536 134.451818 \r\nL 108.256204 139.668182 \r\nL 108.560872 139.668182 \r\nL 108.865541 137.06 \r\nL 109.170209 139.668182 \r\nL 109.474877 131.148121 \r\nL 110.084214 131.148121 \r\nL 110.388882 124.391688 \r\nL 110.69355 121.410909 \r\nL 110.998219 124.391688 \r\nL 112.826229 124.391688 \r\nL 113.435565 118.43013 \r\nL 113.740233 127.372468 \r\nL 114.044902 124.391688 \r\nL 114.654238 124.391688 \r\nL 114.958907 127.372468 \r\nL 115.263575 124.391688 \r\nL 115.872912 124.391688 \r\nL 116.17758 113.385734 \r\nL 116.482248 113.385734 \r\nL 116.786916 100.545455 \r\nL 117.091585 107.500606 \r\nL 117.396253 110.978182 \r\nL 117.700921 107.500606 \r\nL 118.00559 96.751736 \r\nL 118.310258 96.751736 \r\nL 118.919595 89.164298 \r\nL 119.528931 96.751736 \r\nL 120.138268 96.751736 \r\nL 120.747604 104.339174 \r\nL 121.052273 100.545455 \r\nL 121.356941 83.853091 \r\nL 121.966278 83.853091 \r\nL 122.270946 75.506909 \r\nL 122.575614 79.68 \r\nL 122.880283 79.68 \r\nL 123.184951 83.853091 \r\nL 125.622297 83.853091 \r\nL 125.926966 79.68 \r\nL 126.231634 83.853091 \r\nL 128.059644 83.853091 \r\nL 128.364312 79.68 \r\nL 128.66898 83.853091 \r\nL 128.973649 83.853091 \r\nL 129.278317 68.088081 \r\nL 130.49699 68.088081 \r\nL 130.801658 63.451313 \r\nL 131.106327 63.451313 \r\nL 131.410995 68.088081 \r\nL 131.715663 63.451313 \r\nL 132.020332 63.451313 \r\nL 132.325 68.088081 \r\nL 136.895025 68.088081 \r\nL 137.199693 63.451313 \r\nL 137.504361 68.088081 \r\nL 138.723034 68.088081 \r\nL 139.027703 63.451313 \r\nL 139.332371 68.088081 \r\nL 139.637039 63.451313 \r\nL 139.941708 63.451313 \r\nL 140.246376 68.088081 \r\nL 141.465049 68.088081 \r\nL 141.769717 63.451313 \r\nL 142.074386 68.088081 \r\nL 142.379054 68.088081 \r\nL 142.683722 58.814545 \r\nL 142.988391 68.088081 \r\nL 143.293059 68.088081 \r\nL 143.597727 63.451313 \r\nL 143.902396 68.088081 \r\nL 145.730405 68.088081 \r\nL 146.035074 63.451313 \r\nL 146.339742 63.451313 \r\nL 146.64441 68.088081 \r\nL 148.167752 68.088081 \r\nL 148.47242 63.451313 \r\nL 148.777088 68.088081 \r\nL 149.386425 68.088081 \r\nL 149.691093 48.381818 \r\nL 150.909767 48.381818 \r\nL 151.214435 43.165455 \r\nL 151.519103 43.165455 \r\nL 151.823771 48.381818 \r\nL 157.917138 48.381818 \r\nL 158.221806 43.165455 \r\nL 158.526474 48.381818 \r\nL 160.049816 48.381818 \r\nL 160.354484 43.165455 \r\nL 160.659152 48.381818 \r\nL 167.057187 48.381818 \r\nL 167.361855 43.165455 \r\nL 167.666523 48.381818 \r\nL 211.234091 48.381818 \r\nL 211.538759 53.598182 \r\nL 211.843428 48.381818 \r\nL 261.504361 48.381818 \r\nL 261.809029 53.598182 \r\nL 262.113698 48.381818 \r\nL 310.555958 48.381818 \r\nL 310.860627 53.598182 \r\nL 311.165295 53.598182 \r\nL 311.469963 48.381818 \r\nL 312.688636 48.381818 \r\nL 312.993305 53.598182 \r\nL 313.297973 48.381818 \r\nL 315.125983 48.381818 \r\nL 315.430651 53.598182 \r\nL 315.735319 48.381818 \r\nL 316.649324 48.381818 \r\nL 316.953993 53.598182 \r\nL 317.258661 53.598182 \r\nL 317.563329 48.381818 \r\nL 333.710749 48.381818 \r\nL 334.015418 23.045195 \r\nL 337.366769 23.045195 \r\nL 337.671437 70.737662 \r\nL 337.976106 23.045195 \r\nL 341.632125 23.045195 \r\nL 341.936794 29.006753 \r\nL 342.241462 29.006753 \r\nL 342.54613 23.045195 \r\nL 342.850799 29.006753 \r\nL 343.460135 17.083636 \r\nL 343.764803 23.045195 \r\nL 344.983477 23.045195 \r\nL 345.288145 17.083636 \r\nL 345.592813 17.083636 \r\nL 345.897482 23.045195 \r\nL 346.506818 23.045195 \r\nL 346.506818 23.045195 \r\n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\r\n   </g>\r\n   <g id=\"line2d_15\">\r\n    <path clip-path=\"url(#p19ca4dcc6c)\" d=\"M 42.143182 212.56 \r\nL 44.27586 212.56 \r\nL 45.494533 208.167273 \r\nL 45.799201 208.167273 \r\nL 46.10387 207.069091 \r\nL 46.408538 204.872727 \r\nL 46.713206 204.872727 \r\nL 47.017875 202.676364 \r\nL 47.322543 201.578182 \r\nL 50.064558 201.578182 \r\nL 50.369226 199.381818 \r\nL 50.673894 192.792727 \r\nL 51.283231 188.4 \r\nL 346.506818 188.4 \r\nL 346.506818 188.4 \r\n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\r\n   </g>\r\n   <g id=\"patch_3\">\r\n    <path d=\"M 26.925 224.64 \r\nL 26.925 7.2 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_4\">\r\n    <path d=\"M 361.725 224.64 \r\nL 361.725 7.2 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_5\">\r\n    <path d=\"M 26.925 224.64 \r\nL 361.725 224.64 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_6\">\r\n    <path d=\"M 26.925 7.2 \r\nL 361.725 7.2 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n  </g>\r\n </g>\r\n <defs>\r\n  <clipPath id=\"p19ca4dcc6c\">\r\n   <rect height=\"217.44\" width=\"334.8\" x=\"26.925\" y=\"7.2\"/>\r\n  </clipPath>\r\n </defs>\r\n</svg>\r\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(stg_history['epoch'], stg_history['valid_acc']/stg_history['# feature'], \n",
    "    label=\"STG\")\n",
    "plt.plot(history['epoch'], history['valid_acc']/history['# feature'], \n",
    "    label=\"No Feature Selection\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000 Loss: 1.0924816131591797 num_feat: 19/19 Reg Loss: 0.08414222300052643 Val Accuracy: 27.77777777777778\n",
      "Epoch 2/1000 Loss: 1.091377854347229 num_feat: 19/19 Reg Loss: 0.0841294601559639 Val Accuracy: 27.77777777777778\n",
      "Epoch 3/1000 Loss: 1.0902191400527954 num_feat: 19/19 Reg Loss: 0.08410605043172836 Val Accuracy: 27.77777777777778\n",
      "Epoch 4/1000 Loss: 1.0934693813323975 num_feat: 19/19 Reg Loss: 0.08407244086265564 Val Accuracy: 27.77777777777778\n",
      "Epoch 5/1000 Loss: 1.088834524154663 num_feat: 19/19 Reg Loss: 0.0840277373790741 Val Accuracy: 27.77777777777778\n",
      "Epoch 6/1000 Loss: 1.0932363271713257 num_feat: 19/19 Reg Loss: 0.08397557586431503 Val Accuracy: 27.77777777777778\n",
      "Epoch 7/1000 Loss: 1.0914306640625 num_feat: 19/19 Reg Loss: 0.08391638845205307 Val Accuracy: 27.77777777777778\n",
      "Epoch 8/1000 Loss: 1.0911245346069336 num_feat: 19/19 Reg Loss: 0.08385036885738373 Val Accuracy: 27.77777777777778\n",
      "Epoch 9/1000 Loss: 1.0891772508621216 num_feat: 19/19 Reg Loss: 0.08377768099308014 Val Accuracy: 27.77777777777778\n",
      "Epoch 10/1000 Loss: 1.08559250831604 num_feat: 19/19 Reg Loss: 0.08370058983564377 Val Accuracy: 27.77777777777778\n",
      "Epoch 11/1000 Loss: 1.0864442586898804 num_feat: 19/19 Reg Loss: 0.08361747860908508 Val Accuracy: 27.77777777777778\n",
      "Epoch 12/1000 Loss: 1.0861250162124634 num_feat: 19/19 Reg Loss: 0.08353012055158615 Val Accuracy: 27.77777777777778\n",
      "Epoch 13/1000 Loss: 1.0848586559295654 num_feat: 19/19 Reg Loss: 0.08343895524740219 Val Accuracy: 27.77777777777778\n",
      "Epoch 14/1000 Loss: 1.0826536417007446 num_feat: 19/19 Reg Loss: 0.08334480971097946 Val Accuracy: 27.77777777777778\n",
      "Epoch 15/1000 Loss: 1.084354281425476 num_feat: 19/19 Reg Loss: 0.08325038105249405 Val Accuracy: 27.77777777777778\n",
      "Epoch 16/1000 Loss: 1.0812160968780518 num_feat: 19/19 Reg Loss: 0.08315529674291611 Val Accuracy: 27.77777777777778\n",
      "Epoch 17/1000 Loss: 1.0863093137741089 num_feat: 19/19 Reg Loss: 0.08305522054433823 Val Accuracy: 27.77777777777778\n",
      "Epoch 18/1000 Loss: 1.0759748220443726 num_feat: 19/19 Reg Loss: 0.08295294642448425 Val Accuracy: 27.77777777777778\n",
      "Epoch 19/1000 Loss: 1.0789438486099243 num_feat: 19/19 Reg Loss: 0.08284945040941238 Val Accuracy: 27.77777777777778\n",
      "Epoch 20/1000 Loss: 1.0818333625793457 num_feat: 19/19 Reg Loss: 0.08274541050195694 Val Accuracy: 27.77777777777778\n",
      "Epoch 21/1000 Loss: 1.0711041688919067 num_feat: 19/19 Reg Loss: 0.08264513313770294 Val Accuracy: 27.77777777777778\n",
      "Epoch 22/1000 Loss: 1.0810796022415161 num_feat: 19/19 Reg Loss: 0.08254476636648178 Val Accuracy: 27.77777777777778\n",
      "Epoch 23/1000 Loss: 1.071234941482544 num_feat: 19/19 Reg Loss: 0.08244738727807999 Val Accuracy: 27.77777777777778\n",
      "Epoch 24/1000 Loss: 1.073039174079895 num_feat: 19/19 Reg Loss: 0.0823545902967453 Val Accuracy: 27.77777777777778\n",
      "Epoch 25/1000 Loss: 1.0575395822525024 num_feat: 19/19 Reg Loss: 0.08226093649864197 Val Accuracy: 27.77777777777778\n",
      "Epoch 26/1000 Loss: 1.073272943496704 num_feat: 19/19 Reg Loss: 0.08216847479343414 Val Accuracy: 27.77777777777778\n",
      "Epoch 27/1000 Loss: 1.0558537244796753 num_feat: 19/19 Reg Loss: 0.08207852393388748 Val Accuracy: 27.77777777777778\n",
      "Epoch 28/1000 Loss: 1.0632654428482056 num_feat: 19/19 Reg Loss: 0.08198452740907669 Val Accuracy: 27.77777777777778\n",
      "Epoch 29/1000 Loss: 1.0517480373382568 num_feat: 19/19 Reg Loss: 0.08189405500888824 Val Accuracy: 27.77777777777778\n",
      "Epoch 30/1000 Loss: 1.044523000717163 num_feat: 19/19 Reg Loss: 0.08181342482566833 Val Accuracy: 27.77777777777778\n",
      "Epoch 31/1000 Loss: 1.047298550605774 num_feat: 19/19 Reg Loss: 0.08174428343772888 Val Accuracy: 27.77777777777778\n",
      "Epoch 32/1000 Loss: 1.03121817111969 num_feat: 19/19 Reg Loss: 0.0816742405295372 Val Accuracy: 63.888888888888886\n",
      "Epoch 33/1000 Loss: 1.036481499671936 num_feat: 19/19 Reg Loss: 0.08161593228578568 Val Accuracy: 27.77777777777778\n",
      "Epoch 34/1000 Loss: 0.993862509727478 num_feat: 19/19 Reg Loss: 0.08157241344451904 Val Accuracy: 50.0\n",
      "Epoch 35/1000 Loss: 0.9778991937637329 num_feat: 19/19 Reg Loss: 0.08152101188898087 Val Accuracy: 50.0\n",
      "Epoch 36/1000 Loss: 1.0260096788406372 num_feat: 19/19 Reg Loss: 0.08146817237138748 Val Accuracy: 61.111111111111114\n",
      "Epoch 37/1000 Loss: 0.9420875310897827 num_feat: 19/19 Reg Loss: 0.08145800232887268 Val Accuracy: 66.66666666666667\n",
      "Epoch 38/1000 Loss: 0.9760332703590393 num_feat: 19/19 Reg Loss: 0.08145928382873535 Val Accuracy: 66.66666666666667\n",
      "Epoch 39/1000 Loss: 1.0936156511306763 num_feat: 19/19 Reg Loss: 0.0815245509147644 Val Accuracy: 66.66666666666667\n",
      "Epoch 40/1000 Loss: 0.8751071691513062 num_feat: 19/19 Reg Loss: 0.08162041008472443 Val Accuracy: 66.66666666666667\n",
      "Epoch 41/1000 Loss: 0.841202974319458 num_feat: 19/19 Reg Loss: 0.08169764280319214 Val Accuracy: 66.66666666666667\n",
      "Epoch 42/1000 Loss: 0.893378734588623 num_feat: 19/19 Reg Loss: 0.08177433162927628 Val Accuracy: 66.66666666666667\n",
      "Epoch 43/1000 Loss: 0.82795250415802 num_feat: 19/19 Reg Loss: 0.0819011852145195 Val Accuracy: 66.66666666666667\n",
      "Epoch 44/1000 Loss: 0.7735072374343872 num_feat: 19/19 Reg Loss: 0.08198755979537964 Val Accuracy: 66.66666666666667\n",
      "Epoch 45/1000 Loss: 0.752197802066803 num_feat: 19/19 Reg Loss: 0.0819903016090393 Val Accuracy: 66.66666666666667\n",
      "Epoch 46/1000 Loss: 0.6309354901313782 num_feat: 19/19 Reg Loss: 0.08203194290399551 Val Accuracy: 66.66666666666667\n",
      "Epoch 47/1000 Loss: 0.656034529209137 num_feat: 19/19 Reg Loss: 0.08203724026679993 Val Accuracy: 75.0\n",
      "Epoch 48/1000 Loss: 0.5300383567810059 num_feat: 19/19 Reg Loss: 0.08214838057756424 Val Accuracy: 72.22222222222223\n",
      "Epoch 49/1000 Loss: 0.842190682888031 num_feat: 19/19 Reg Loss: 0.0822635367512703 Val Accuracy: 66.66666666666667\n",
      "Epoch 50/1000 Loss: 0.7209988832473755 num_feat: 19/19 Reg Loss: 0.08257324248552322 Val Accuracy: 38.888888888888886\n",
      "Epoch 51/1000 Loss: 1.0609766244888306 num_feat: 19/19 Reg Loss: 0.08293996006250381 Val Accuracy: 27.77777777777778\n",
      "Epoch 52/1000 Loss: 1.4957501888275146 num_feat: 19/19 Reg Loss: 0.08307211101055145 Val Accuracy: 66.66666666666667\n",
      "Epoch 53/1000 Loss: 0.7713572978973389 num_feat: 19/19 Reg Loss: 0.08293057978153229 Val Accuracy: 69.44444444444444\n",
      "Epoch 54/1000 Loss: 0.5306361317634583 num_feat: 19/19 Reg Loss: 0.08272771537303925 Val Accuracy: 72.22222222222223\n",
      "Epoch 55/1000 Loss: 0.5797441005706787 num_feat: 19/19 Reg Loss: 0.08251452445983887 Val Accuracy: 69.44444444444444\n",
      "Epoch 56/1000 Loss: 0.6144341230392456 num_feat: 19/19 Reg Loss: 0.08229677379131317 Val Accuracy: 72.22222222222223\n",
      "Epoch 57/1000 Loss: 0.7506228089332581 num_feat: 19/19 Reg Loss: 0.08207619935274124 Val Accuracy: 69.44444444444444\n",
      "Epoch 58/1000 Loss: 0.7323939800262451 num_feat: 19/19 Reg Loss: 0.08176989108324051 Val Accuracy: 80.55555555555556\n",
      "Epoch 59/1000 Loss: 0.4514250159263611 num_feat: 19/19 Reg Loss: 0.0813884437084198 Val Accuracy: 86.11111111111111\n",
      "Epoch 60/1000 Loss: 0.4387473165988922 num_feat: 19/19 Reg Loss: 0.0810142531991005 Val Accuracy: 69.44444444444444\n",
      "Epoch 61/1000 Loss: 0.41326403617858887 num_feat: 19/19 Reg Loss: 0.08064382523298264 Val Accuracy: 69.44444444444444\n",
      "Epoch 62/1000 Loss: 0.5225725769996643 num_feat: 19/19 Reg Loss: 0.08028744161128998 Val Accuracy: 66.66666666666667\n",
      "Epoch 63/1000 Loss: 0.602473795413971 num_feat: 19/19 Reg Loss: 0.0799063891172409 Val Accuracy: 66.66666666666667\n",
      "Epoch 64/1000 Loss: 0.5380232930183411 num_feat: 19/19 Reg Loss: 0.07948243618011475 Val Accuracy: 66.66666666666667\n",
      "Epoch 65/1000 Loss: 0.4405617415904999 num_feat: 19/19 Reg Loss: 0.0790090560913086 Val Accuracy: 69.44444444444444\n",
      "Epoch 66/1000 Loss: 0.4088343679904938 num_feat: 19/19 Reg Loss: 0.07856371253728867 Val Accuracy: 83.33333333333333\n",
      "Epoch 67/1000 Loss: 0.42127853631973267 num_feat: 19/19 Reg Loss: 0.0781419575214386 Val Accuracy: 86.11111111111111\n",
      "Epoch 68/1000 Loss: 0.44907060265541077 num_feat: 19/19 Reg Loss: 0.07774736732244492 Val Accuracy: 69.44444444444444\n",
      "Epoch 69/1000 Loss: 0.4626550078392029 num_feat: 19/19 Reg Loss: 0.07738282531499863 Val Accuracy: 86.11111111111111\n",
      "Epoch 70/1000 Loss: 0.4664575159549713 num_feat: 19/19 Reg Loss: 0.07704705744981766 Val Accuracy: 69.44444444444444\n",
      "Epoch 71/1000 Loss: 0.5746992826461792 num_feat: 19/19 Reg Loss: 0.07673221081495285 Val Accuracy: 75.0\n",
      "Epoch 72/1000 Loss: 0.41953128576278687 num_feat: 18/19 Reg Loss: 0.0763653963804245 Val Accuracy: 72.22222222222223\n",
      "Epoch 73/1000 Loss: 0.3806932270526886 num_feat: 18/19 Reg Loss: 0.07600900530815125 Val Accuracy: 66.66666666666667\n",
      "Epoch 74/1000 Loss: 0.3866165578365326 num_feat: 18/19 Reg Loss: 0.07566992193460464 Val Accuracy: 66.66666666666667\n",
      "Epoch 75/1000 Loss: 0.4075261056423187 num_feat: 18/19 Reg Loss: 0.07534384727478027 Val Accuracy: 69.44444444444444\n",
      "Epoch 76/1000 Loss: 0.3978097438812256 num_feat: 18/19 Reg Loss: 0.07497500628232956 Val Accuracy: 69.44444444444444\n",
      "Epoch 77/1000 Loss: 0.3803589344024658 num_feat: 18/19 Reg Loss: 0.07464640587568283 Val Accuracy: 69.44444444444444\n",
      "Epoch 78/1000 Loss: 0.3431163728237152 num_feat: 18/19 Reg Loss: 0.07433118671178818 Val Accuracy: 100.0\n",
      "Epoch 79/1000 Loss: 0.37031179666519165 num_feat: 18/19 Reg Loss: 0.07400812953710556 Val Accuracy: 86.11111111111111\n",
      "Epoch 80/1000 Loss: 0.34321150183677673 num_feat: 18/19 Reg Loss: 0.07372241467237473 Val Accuracy: 94.44444444444444\n",
      "Epoch 81/1000 Loss: 0.2988528907299042 num_feat: 18/19 Reg Loss: 0.07347898930311203 Val Accuracy: 100.0\n",
      "Epoch 82/1000 Loss: 0.28533920645713806 num_feat: 18/19 Reg Loss: 0.07324934750795364 Val Accuracy: 97.22222222222223\n",
      "Epoch 83/1000 Loss: 0.2647472620010376 num_feat: 18/19 Reg Loss: 0.07302733510732651 Val Accuracy: 91.66666666666667\n",
      "Epoch 84/1000 Loss: 0.2499191015958786 num_feat: 18/19 Reg Loss: 0.0728008896112442 Val Accuracy: 94.44444444444444\n",
      "Epoch 85/1000 Loss: 0.27940669655799866 num_feat: 18/19 Reg Loss: 0.07257488369941711 Val Accuracy: 83.33333333333333\n",
      "Epoch 86/1000 Loss: 0.33627045154571533 num_feat: 18/19 Reg Loss: 0.07234983891248703 Val Accuracy: 80.55555555555556\n",
      "Epoch 87/1000 Loss: 0.256404310464859 num_feat: 18/19 Reg Loss: 0.0720524936914444 Val Accuracy: 94.44444444444444\n",
      "Epoch 88/1000 Loss: 0.21291802823543549 num_feat: 18/19 Reg Loss: 0.07172888517379761 Val Accuracy: 97.22222222222223\n",
      "Epoch 89/1000 Loss: 0.21664837002754211 num_feat: 18/19 Reg Loss: 0.07142708450555801 Val Accuracy: 91.66666666666667\n",
      "Epoch 90/1000 Loss: 0.23146478831768036 num_feat: 18/19 Reg Loss: 0.07113613188266754 Val Accuracy: 88.88888888888889\n",
      "Epoch 91/1000 Loss: 0.23400545120239258 num_feat: 18/19 Reg Loss: 0.07085592299699783 Val Accuracy: 94.44444444444444\n",
      "Epoch 92/1000 Loss: 0.18676893413066864 num_feat: 18/19 Reg Loss: 0.07055278867483139 Val Accuracy: 83.33333333333333\n",
      "Epoch 93/1000 Loss: 0.2424072027206421 num_feat: 18/19 Reg Loss: 0.07026272267103195 Val Accuracy: 83.33333333333333\n",
      "Epoch 94/1000 Loss: 0.17874445021152496 num_feat: 18/19 Reg Loss: 0.069952093064785 Val Accuracy: 86.11111111111111\n",
      "Epoch 95/1000 Loss: 0.17670035362243652 num_feat: 18/19 Reg Loss: 0.06966350227594376 Val Accuracy: 88.88888888888889\n",
      "Epoch 96/1000 Loss: 0.16279560327529907 num_feat: 18/19 Reg Loss: 0.0693879947066307 Val Accuracy: 88.88888888888889\n",
      "Epoch 97/1000 Loss: 0.346106618642807 num_feat: 18/19 Reg Loss: 0.06911813467741013 Val Accuracy: 72.22222222222223\n",
      "Epoch 98/1000 Loss: 0.3578678071498871 num_feat: 17/19 Reg Loss: 0.06881528347730637 Val Accuracy: 75.0\n",
      "Epoch 99/1000 Loss: 0.1718156337738037 num_feat: 17/19 Reg Loss: 0.0684567242860794 Val Accuracy: 69.44444444444444\n",
      "Epoch 100/1000 Loss: 0.4544224441051483 num_feat: 17/19 Reg Loss: 0.06813942641019821 Val Accuracy: 72.22222222222223\n",
      "Epoch 101/1000 Loss: 0.6898068189620972 num_feat: 17/19 Reg Loss: 0.06781616061925888 Val Accuracy: 83.33333333333333\n",
      "Epoch 102/1000 Loss: 0.16249550879001617 num_feat: 17/19 Reg Loss: 0.06748869270086288 Val Accuracy: 91.66666666666667\n",
      "Epoch 103/1000 Loss: 0.2006039172410965 num_feat: 17/19 Reg Loss: 0.06719069927930832 Val Accuracy: 75.0\n",
      "Epoch 104/1000 Loss: 0.34549838304519653 num_feat: 17/19 Reg Loss: 0.06692298501729965 Val Accuracy: 91.66666666666667\n",
      "Epoch 105/1000 Loss: 0.2853935658931732 num_feat: 15/19 Reg Loss: 0.0666395053267479 Val Accuracy: 94.44444444444444\n",
      "Epoch 106/1000 Loss: 0.12283409386873245 num_feat: 15/19 Reg Loss: 0.06636689603328705 Val Accuracy: 72.22222222222223\n",
      "Epoch 107/1000 Loss: 0.5011097192764282 num_feat: 13/19 Reg Loss: 0.06608916074037552 Val Accuracy: 75.0\n",
      "Epoch 108/1000 Loss: 0.2785909175872803 num_feat: 13/19 Reg Loss: 0.06580395996570587 Val Accuracy: 83.33333333333333\n",
      "Epoch 109/1000 Loss: 0.13435900211334229 num_feat: 12/19 Reg Loss: 0.0654943659901619 Val Accuracy: 94.44444444444444\n",
      "Epoch 110/1000 Loss: 0.18785659968852997 num_feat: 12/19 Reg Loss: 0.0652088150382042 Val Accuracy: 88.88888888888889\n",
      "Epoch 111/1000 Loss: 0.12854696810245514 num_feat: 12/19 Reg Loss: 0.06487613171339035 Val Accuracy: 97.22222222222223\n",
      "Epoch 112/1000 Loss: 0.13859041035175323 num_feat: 11/19 Reg Loss: 0.06456074118614197 Val Accuracy: 97.22222222222223\n",
      "Epoch 113/1000 Loss: 0.11571972817182541 num_feat: 11/19 Reg Loss: 0.06425052136182785 Val Accuracy: 88.88888888888889\n",
      "Epoch 114/1000 Loss: 0.11538044363260269 num_feat: 10/19 Reg Loss: 0.06395500153303146 Val Accuracy: 94.44444444444444\n",
      "Epoch 115/1000 Loss: 0.11746202409267426 num_feat: 10/19 Reg Loss: 0.06366722285747528 Val Accuracy: 91.66666666666667\n",
      "Epoch 116/1000 Loss: 0.3228372037410736 num_feat: 10/19 Reg Loss: 0.06338213384151459 Val Accuracy: 97.22222222222223\n",
      "Epoch 117/1000 Loss: 0.17666728794574738 num_feat: 10/19 Reg Loss: 0.06301548331975937 Val Accuracy: 88.88888888888889\n",
      "Epoch 118/1000 Loss: 0.18315084278583527 num_feat: 10/19 Reg Loss: 0.06266984343528748 Val Accuracy: 88.88888888888889\n",
      "Epoch 119/1000 Loss: 0.2960543930530548 num_feat: 10/19 Reg Loss: 0.06233521178364754 Val Accuracy: 97.22222222222223\n",
      "Epoch 120/1000 Loss: 0.09955582022666931 num_feat: 9/19 Reg Loss: 0.061940114945173264 Val Accuracy: 91.66666666666667\n",
      "Epoch 121/1000 Loss: 0.19141528010368347 num_feat: 9/19 Reg Loss: 0.061569202691316605 Val Accuracy: 91.66666666666667\n",
      "Epoch 122/1000 Loss: 0.15989480912685394 num_feat: 9/19 Reg Loss: 0.061267223209142685 Val Accuracy: 91.66666666666667\n",
      "Epoch 123/1000 Loss: 0.15789632499217987 num_feat: 9/19 Reg Loss: 0.06095769628882408 Val Accuracy: 97.22222222222223\n",
      "Epoch 124/1000 Loss: 0.10344523191452026 num_feat: 9/19 Reg Loss: 0.060596805065870285 Val Accuracy: 100.0\n",
      "Epoch 125/1000 Loss: 0.15464742481708527 num_feat: 9/19 Reg Loss: 0.060259100049734116 Val Accuracy: 86.11111111111111\n",
      "Epoch 126/1000 Loss: 0.7886640429496765 num_feat: 9/19 Reg Loss: 0.059874165803194046 Val Accuracy: 69.44444444444444\n",
      "Epoch 127/1000 Loss: 0.8550735712051392 num_feat: 9/19 Reg Loss: 0.05970260500907898 Val Accuracy: 69.44444444444444\n",
      "Epoch 128/1000 Loss: 1.0664948225021362 num_feat: 8/19 Reg Loss: 0.059377577155828476 Val Accuracy: 69.44444444444444\n",
      "Epoch 129/1000 Loss: 0.6519131660461426 num_feat: 8/19 Reg Loss: 0.05884021148085594 Val Accuracy: 97.22222222222223\n",
      "Epoch 130/1000 Loss: 0.23393939435482025 num_feat: 7/19 Reg Loss: 0.058159589767456055 Val Accuracy: 80.55555555555556\n",
      "Epoch 131/1000 Loss: 0.34801220893859863 num_feat: 7/19 Reg Loss: 0.05745715647935867 Val Accuracy: 72.22222222222223\n",
      "Epoch 132/1000 Loss: 0.6964054107666016 num_feat: 6/19 Reg Loss: 0.05684460327029228 Val Accuracy: 72.22222222222223\n",
      "Epoch 133/1000 Loss: 0.6929759383201599 num_feat: 6/19 Reg Loss: 0.0563029907643795 Val Accuracy: 75.0\n",
      "Epoch 134/1000 Loss: 0.5761986970901489 num_feat: 6/19 Reg Loss: 0.05588477849960327 Val Accuracy: 91.66666666666667\n",
      "Epoch 135/1000 Loss: 0.2843482494354248 num_feat: 6/19 Reg Loss: 0.05551476404070854 Val Accuracy: 77.77777777777777\n",
      "Epoch 136/1000 Loss: 0.2199656367301941 num_feat: 6/19 Reg Loss: 0.055178917944431305 Val Accuracy: 72.22222222222223\n",
      "Epoch 137/1000 Loss: 0.38358503580093384 num_feat: 6/19 Reg Loss: 0.05481081083416939 Val Accuracy: 72.22222222222223\n",
      "Epoch 138/1000 Loss: 0.21152764558792114 num_feat: 6/19 Reg Loss: 0.054391175508499146 Val Accuracy: 72.22222222222223\n",
      "Epoch 139/1000 Loss: 0.5971571207046509 num_feat: 6/19 Reg Loss: 0.05402993783354759 Val Accuracy: 86.11111111111111\n",
      "Epoch 140/1000 Loss: 0.23325686156749725 num_feat: 6/19 Reg Loss: 0.05352635309100151 Val Accuracy: 94.44444444444444\n",
      "Epoch 141/1000 Loss: 0.2721422612667084 num_feat: 6/19 Reg Loss: 0.05303209647536278 Val Accuracy: 91.66666666666667\n",
      "Epoch 142/1000 Loss: 0.14952991902828217 num_feat: 7/19 Reg Loss: 0.05261809378862381 Val Accuracy: 83.33333333333333\n",
      "Epoch 143/1000 Loss: 0.22035031020641327 num_feat: 7/19 Reg Loss: 0.05229786038398743 Val Accuracy: 91.66666666666667\n",
      "Epoch 144/1000 Loss: 0.1903441697359085 num_feat: 7/19 Reg Loss: 0.05207906290888786 Val Accuracy: 94.44444444444444\n",
      "Epoch 145/1000 Loss: 0.18227079510688782 num_feat: 7/19 Reg Loss: 0.051882725208997726 Val Accuracy: 97.22222222222223\n",
      "Epoch 146/1000 Loss: 0.11755197495222092 num_feat: 7/19 Reg Loss: 0.05167662724852562 Val Accuracy: 86.11111111111111\n",
      "Epoch 147/1000 Loss: 0.15159645676612854 num_feat: 7/19 Reg Loss: 0.051480066031217575 Val Accuracy: 97.22222222222223\n",
      "Epoch 148/1000 Loss: 0.19983898103237152 num_feat: 7/19 Reg Loss: 0.05128515511751175 Val Accuracy: 91.66666666666667\n",
      "Epoch 149/1000 Loss: 0.23995347321033478 num_feat: 7/19 Reg Loss: 0.051080502569675446 Val Accuracy: 97.22222222222223\n",
      "Epoch 150/1000 Loss: 0.17390629649162292 num_feat: 7/19 Reg Loss: 0.05080994591116905 Val Accuracy: 91.66666666666667\n",
      "Epoch 151/1000 Loss: 0.11607278138399124 num_feat: 7/19 Reg Loss: 0.05051335692405701 Val Accuracy: 91.66666666666667\n",
      "Epoch 152/1000 Loss: 0.21502560377120972 num_feat: 7/19 Reg Loss: 0.05025627091526985 Val Accuracy: 83.33333333333333\n",
      "Epoch 153/1000 Loss: 0.2131139636039734 num_feat: 7/19 Reg Loss: 0.05002240464091301 Val Accuracy: 83.33333333333333\n",
      "Epoch 154/1000 Loss: 0.20479197800159454 num_feat: 7/19 Reg Loss: 0.04983692988753319 Val Accuracy: 97.22222222222223\n",
      "Epoch 155/1000 Loss: 0.0983772873878479 num_feat: 7/19 Reg Loss: 0.04962893947958946 Val Accuracy: 97.22222222222223\n",
      "Epoch 156/1000 Loss: 0.09558933228254318 num_feat: 6/19 Reg Loss: 0.04942625015974045 Val Accuracy: 97.22222222222223\n",
      "Epoch 157/1000 Loss: 0.22459125518798828 num_feat: 6/19 Reg Loss: 0.04924064502120018 Val Accuracy: 100.0\n",
      "Epoch 158/1000 Loss: 0.3741658627986908 num_feat: 6/19 Reg Loss: 0.04905394837260246 Val Accuracy: 97.22222222222223\n",
      "Epoch 159/1000 Loss: 0.22388029098510742 num_feat: 6/19 Reg Loss: 0.048916496336460114 Val Accuracy: 97.22222222222223\n",
      "Epoch 160/1000 Loss: 0.10555589944124222 num_feat: 6/19 Reg Loss: 0.04872537776827812 Val Accuracy: 80.55555555555556\n",
      "Epoch 161/1000 Loss: 0.2630915939807892 num_feat: 6/19 Reg Loss: 0.048536088317632675 Val Accuracy: 80.55555555555556\n",
      "Epoch 162/1000 Loss: 0.17730028927326202 num_feat: 6/19 Reg Loss: 0.04828948155045509 Val Accuracy: 97.22222222222223\n",
      "Epoch 163/1000 Loss: 0.32911810278892517 num_feat: 6/19 Reg Loss: 0.04805339500308037 Val Accuracy: 88.88888888888889\n",
      "Epoch 164/1000 Loss: 0.11232981085777283 num_feat: 6/19 Reg Loss: 0.04769135266542435 Val Accuracy: 88.88888888888889\n",
      "Epoch 165/1000 Loss: 0.3122284710407257 num_feat: 6/19 Reg Loss: 0.047361109405756 Val Accuracy: 83.33333333333333\n",
      "Epoch 166/1000 Loss: 0.21765899658203125 num_feat: 6/19 Reg Loss: 0.04710320010781288 Val Accuracy: 88.88888888888889\n",
      "Epoch 167/1000 Loss: 0.1474149227142334 num_feat: 6/19 Reg Loss: 0.04684258997440338 Val Accuracy: 97.22222222222223\n",
      "Epoch 168/1000 Loss: 0.13713248074054718 num_feat: 6/19 Reg Loss: 0.04661676660180092 Val Accuracy: 94.44444444444444\n",
      "Epoch 169/1000 Loss: 0.1151883453130722 num_feat: 6/19 Reg Loss: 0.04641256481409073 Val Accuracy: 88.88888888888889\n",
      "Epoch 170/1000 Loss: 0.16886958479881287 num_feat: 6/19 Reg Loss: 0.046210046857595444 Val Accuracy: 91.66666666666667\n",
      "Epoch 171/1000 Loss: 0.11832781136035919 num_feat: 6/19 Reg Loss: 0.04605720564723015 Val Accuracy: 88.88888888888889\n",
      "Epoch 172/1000 Loss: 0.1541343331336975 num_feat: 6/19 Reg Loss: 0.0459168516099453 Val Accuracy: 91.66666666666667\n",
      "Epoch 173/1000 Loss: 0.11440607160329819 num_feat: 6/19 Reg Loss: 0.04577077925205231 Val Accuracy: 97.22222222222223\n",
      "Epoch 174/1000 Loss: 0.08812913298606873 num_feat: 6/19 Reg Loss: 0.0456295982003212 Val Accuracy: 97.22222222222223\n",
      "Epoch 175/1000 Loss: 0.08218053728342056 num_feat: 6/19 Reg Loss: 0.04548434540629387 Val Accuracy: 97.22222222222223\n",
      "Epoch 176/1000 Loss: 0.17715446650981903 num_feat: 6/19 Reg Loss: 0.04534950852394104 Val Accuracy: 80.55555555555556\n",
      "Epoch 177/1000 Loss: 0.17586453258991241 num_feat: 6/19 Reg Loss: 0.045203376561403275 Val Accuracy: 97.22222222222223\n",
      "Epoch 178/1000 Loss: 0.08140742778778076 num_feat: 6/19 Reg Loss: 0.04510927200317383 Val Accuracy: 97.22222222222223\n",
      "Epoch 179/1000 Loss: 0.07457705587148666 num_feat: 6/19 Reg Loss: 0.04503067955374718 Val Accuracy: 97.22222222222223\n",
      "Epoch 180/1000 Loss: 0.08185920864343643 num_feat: 6/19 Reg Loss: 0.04495147988200188 Val Accuracy: 88.88888888888889\n",
      "Epoch 181/1000 Loss: 0.10762730985879898 num_feat: 6/19 Reg Loss: 0.044867731630802155 Val Accuracy: 97.22222222222223\n",
      "Epoch 182/1000 Loss: 0.11338868737220764 num_feat: 6/19 Reg Loss: 0.04479117691516876 Val Accuracy: 97.22222222222223\n",
      "Epoch 183/1000 Loss: 0.09585597366094589 num_feat: 6/19 Reg Loss: 0.044675059616565704 Val Accuracy: 97.22222222222223\n",
      "Epoch 184/1000 Loss: 0.0726870447397232 num_feat: 6/19 Reg Loss: 0.04454437643289566 Val Accuracy: 97.22222222222223\n",
      "Epoch 185/1000 Loss: 0.1240963339805603 num_feat: 6/19 Reg Loss: 0.044408075511455536 Val Accuracy: 97.22222222222223\n",
      "Epoch 186/1000 Loss: 0.07681551575660706 num_feat: 6/19 Reg Loss: 0.04426144063472748 Val Accuracy: 97.22222222222223\n",
      "Epoch 187/1000 Loss: 0.07430274784564972 num_feat: 6/19 Reg Loss: 0.04411957040429115 Val Accuracy: 97.22222222222223\n",
      "Epoch 188/1000 Loss: 0.07152716815471649 num_feat: 6/19 Reg Loss: 0.043975815176963806 Val Accuracy: 100.0\n",
      "Epoch 189/1000 Loss: 0.06209626793861389 num_feat: 6/19 Reg Loss: 0.04382907971739769 Val Accuracy: 100.0\n",
      "Epoch 190/1000 Loss: 0.06617359071969986 num_feat: 6/19 Reg Loss: 0.0436943955719471 Val Accuracy: 97.22222222222223\n",
      "Epoch 191/1000 Loss: 0.08007301390171051 num_feat: 6/19 Reg Loss: 0.04357343912124634 Val Accuracy: 100.0\n",
      "Epoch 192/1000 Loss: 0.06542037427425385 num_feat: 6/19 Reg Loss: 0.04346122965216637 Val Accuracy: 97.22222222222223\n",
      "Epoch 193/1000 Loss: 0.060649413615465164 num_feat: 6/19 Reg Loss: 0.04335148259997368 Val Accuracy: 97.22222222222223\n",
      "Epoch 194/1000 Loss: 0.06299207359552383 num_feat: 6/19 Reg Loss: 0.043239936232566833 Val Accuracy: 97.22222222222223\n",
      "Epoch 195/1000 Loss: 0.06431171298027039 num_feat: 6/19 Reg Loss: 0.04313216730952263 Val Accuracy: 97.22222222222223\n",
      "Epoch 196/1000 Loss: 0.06584910303354263 num_feat: 6/19 Reg Loss: 0.04302622005343437 Val Accuracy: 100.0\n",
      "Epoch 197/1000 Loss: 0.10158265382051468 num_feat: 6/19 Reg Loss: 0.04293745011091232 Val Accuracy: 97.22222222222223\n",
      "Epoch 198/1000 Loss: 0.06057135388255119 num_feat: 6/19 Reg Loss: 0.04282683879137039 Val Accuracy: 97.22222222222223\n",
      "Epoch 199/1000 Loss: 0.06280554085969925 num_feat: 6/19 Reg Loss: 0.04272235557436943 Val Accuracy: 97.22222222222223\n",
      "Epoch 200/1000 Loss: 0.09850776940584183 num_feat: 6/19 Reg Loss: 0.04261975735425949 Val Accuracy: 100.0\n",
      "Epoch 201/1000 Loss: 0.05845377966761589 num_feat: 6/19 Reg Loss: 0.04250204190611839 Val Accuracy: 97.22222222222223\n",
      "Epoch 202/1000 Loss: 0.0628281757235527 num_feat: 6/19 Reg Loss: 0.0423881821334362 Val Accuracy: 97.22222222222223\n",
      "Epoch 203/1000 Loss: 0.07402144372463226 num_feat: 6/19 Reg Loss: 0.042287833988666534 Val Accuracy: 97.22222222222223\n",
      "Epoch 204/1000 Loss: 0.09946049004793167 num_feat: 6/19 Reg Loss: 0.04216444492340088 Val Accuracy: 97.22222222222223\n",
      "Epoch 205/1000 Loss: 0.05061758682131767 num_feat: 6/19 Reg Loss: 0.042076658457517624 Val Accuracy: 91.66666666666667\n",
      "Epoch 206/1000 Loss: 0.06946200132369995 num_feat: 6/19 Reg Loss: 0.04199459031224251 Val Accuracy: 97.22222222222223\n",
      "Epoch 207/1000 Loss: 0.058705467730760574 num_feat: 6/19 Reg Loss: 0.04189789667725563 Val Accuracy: 94.44444444444444\n",
      "Epoch 208/1000 Loss: 0.07435841858386993 num_feat: 6/19 Reg Loss: 0.04180830717086792 Val Accuracy: 83.33333333333333\n",
      "Epoch 209/1000 Loss: 0.06919102370738983 num_feat: 6/19 Reg Loss: 0.0417034737765789 Val Accuracy: 91.66666666666667\n",
      "Epoch 210/1000 Loss: 0.054813168942928314 num_feat: 6/19 Reg Loss: 0.041610222309827805 Val Accuracy: 91.66666666666667\n",
      "Epoch 211/1000 Loss: 0.14804306626319885 num_feat: 6/19 Reg Loss: 0.04152515158057213 Val Accuracy: 91.66666666666667\n",
      "Epoch 212/1000 Loss: 0.07294438034296036 num_feat: 6/19 Reg Loss: 0.04142165184020996 Val Accuracy: 100.0\n",
      "Epoch 213/1000 Loss: 0.07257679849863052 num_feat: 6/19 Reg Loss: 0.04131451994180679 Val Accuracy: 97.22222222222223\n",
      "Epoch 214/1000 Loss: 0.11131846159696579 num_feat: 6/19 Reg Loss: 0.041211336851119995 Val Accuracy: 97.22222222222223\n",
      "Epoch 215/1000 Loss: 0.09140501916408539 num_feat: 6/19 Reg Loss: 0.04112667590379715 Val Accuracy: 100.0\n",
      "Epoch 216/1000 Loss: 0.059803541749715805 num_feat: 6/19 Reg Loss: 0.041085343807935715 Val Accuracy: 100.0\n",
      "Epoch 217/1000 Loss: 0.05310880392789841 num_feat: 6/19 Reg Loss: 0.04102574288845062 Val Accuracy: 97.22222222222223\n",
      "Epoch 218/1000 Loss: 0.052361637353897095 num_feat: 6/19 Reg Loss: 0.04096977785229683 Val Accuracy: 91.66666666666667\n",
      "Epoch 219/1000 Loss: 0.06700799614191055 num_feat: 6/19 Reg Loss: 0.04090096801519394 Val Accuracy: 97.22222222222223\n",
      "Epoch 220/1000 Loss: 0.052288446575403214 num_feat: 6/19 Reg Loss: 0.04082566127181053 Val Accuracy: 91.66666666666667\n",
      "Epoch 221/1000 Loss: 0.06601588428020477 num_feat: 6/19 Reg Loss: 0.04074295610189438 Val Accuracy: 97.22222222222223\n",
      "Epoch 222/1000 Loss: 0.05036997050046921 num_feat: 6/19 Reg Loss: 0.04064188897609711 Val Accuracy: 97.22222222222223\n",
      "Epoch 223/1000 Loss: 0.18208086490631104 num_feat: 6/19 Reg Loss: 0.04054073616862297 Val Accuracy: 97.22222222222223\n",
      "Epoch 224/1000 Loss: 0.046141788363456726 num_feat: 6/19 Reg Loss: 0.040419887751340866 Val Accuracy: 94.44444444444444\n",
      "Epoch 225/1000 Loss: 0.0485992506146431 num_feat: 6/19 Reg Loss: 0.04030682146549225 Val Accuracy: 91.66666666666667\n",
      "Epoch 226/1000 Loss: 0.05135445296764374 num_feat: 6/19 Reg Loss: 0.04020218551158905 Val Accuracy: 94.44444444444444\n",
      "Epoch 227/1000 Loss: 0.05654609203338623 num_feat: 6/19 Reg Loss: 0.040109582245349884 Val Accuracy: 91.66666666666667\n",
      "Epoch 228/1000 Loss: 0.0760730654001236 num_feat: 6/19 Reg Loss: 0.04003413766622543 Val Accuracy: 97.22222222222223\n",
      "Epoch 229/1000 Loss: 0.048995789140462875 num_feat: 6/19 Reg Loss: 0.03996136412024498 Val Accuracy: 97.22222222222223\n",
      "Epoch 230/1000 Loss: 0.04879776015877724 num_feat: 6/19 Reg Loss: 0.03988495469093323 Val Accuracy: 97.22222222222223\n",
      "Epoch 231/1000 Loss: 0.07398517429828644 num_feat: 6/19 Reg Loss: 0.0398273728787899 Val Accuracy: 97.22222222222223\n",
      "Epoch 232/1000 Loss: 0.050657741725444794 num_feat: 6/19 Reg Loss: 0.03977454453706741 Val Accuracy: 97.22222222222223\n",
      "Epoch 233/1000 Loss: 0.06112707778811455 num_feat: 6/19 Reg Loss: 0.0397249311208725 Val Accuracy: 97.22222222222223\n",
      "Epoch 234/1000 Loss: 0.042556170374155045 num_feat: 6/19 Reg Loss: 0.03966405615210533 Val Accuracy: 94.44444444444444\n",
      "Epoch 235/1000 Loss: 0.07476145029067993 num_feat: 6/19 Reg Loss: 0.039603110402822495 Val Accuracy: 94.44444444444444\n",
      "Epoch 236/1000 Loss: 0.09853192418813705 num_feat: 6/19 Reg Loss: 0.039565008133649826 Val Accuracy: 91.66666666666667\n",
      "Epoch 237/1000 Loss: 0.057023804634809494 num_feat: 6/19 Reg Loss: 0.03950550779700279 Val Accuracy: 94.44444444444444\n",
      "Epoch 238/1000 Loss: 0.06282909959554672 num_feat: 6/19 Reg Loss: 0.03944548964500427 Val Accuracy: 97.22222222222223\n",
      "Epoch 239/1000 Loss: 0.04853692650794983 num_feat: 6/19 Reg Loss: 0.03939443826675415 Val Accuracy: 97.22222222222223\n",
      "Epoch 240/1000 Loss: 0.05572586506605148 num_feat: 6/19 Reg Loss: 0.03934838995337486 Val Accuracy: 97.22222222222223\n",
      "Epoch 241/1000 Loss: 0.0555291622877121 num_feat: 6/19 Reg Loss: 0.039294082671403885 Val Accuracy: 97.22222222222223\n",
      "Epoch 242/1000 Loss: 0.046281181275844574 num_feat: 6/19 Reg Loss: 0.03924206644296646 Val Accuracy: 97.22222222222223\n",
      "Epoch 243/1000 Loss: 0.05756122246384621 num_feat: 6/19 Reg Loss: 0.039195310324430466 Val Accuracy: 97.22222222222223\n",
      "Epoch 244/1000 Loss: 0.04315555840730667 num_feat: 6/19 Reg Loss: 0.03914997726678848 Val Accuracy: 97.22222222222223\n",
      "Epoch 245/1000 Loss: 0.07967399805784225 num_feat: 6/19 Reg Loss: 0.03910228610038757 Val Accuracy: 97.22222222222223\n",
      "Epoch 246/1000 Loss: 0.03808634728193283 num_feat: 6/19 Reg Loss: 0.03902066871523857 Val Accuracy: 97.22222222222223\n",
      "Epoch 247/1000 Loss: 0.043251268565654755 num_feat: 6/19 Reg Loss: 0.03893859311938286 Val Accuracy: 97.22222222222223\n",
      "Epoch 248/1000 Loss: 0.11651327461004257 num_feat: 6/19 Reg Loss: 0.03885864466428757 Val Accuracy: 94.44444444444444\n",
      "Epoch 249/1000 Loss: 0.05613172799348831 num_feat: 6/19 Reg Loss: 0.038793157786130905 Val Accuracy: 97.22222222222223\n",
      "Epoch 250/1000 Loss: 0.03897139057517052 num_feat: 6/19 Reg Loss: 0.03874993324279785 Val Accuracy: 94.44444444444444\n",
      "Epoch 251/1000 Loss: 0.0464903749525547 num_feat: 6/19 Reg Loss: 0.03870658203959465 Val Accuracy: 97.22222222222223\n",
      "Epoch 252/1000 Loss: 0.07362271845340729 num_feat: 6/19 Reg Loss: 0.03866242244839668 Val Accuracy: 97.22222222222223\n",
      "Epoch 253/1000 Loss: 0.045990340411663055 num_feat: 6/19 Reg Loss: 0.03860095515847206 Val Accuracy: 97.22222222222223\n",
      "Epoch 254/1000 Loss: 0.05456423759460449 num_feat: 6/19 Reg Loss: 0.038541361689567566 Val Accuracy: 97.22222222222223\n",
      "Epoch 255/1000 Loss: 0.038920190185308456 num_feat: 6/19 Reg Loss: 0.038467325270175934 Val Accuracy: 97.22222222222223\n",
      "Epoch 256/1000 Loss: 0.05844167247414589 num_feat: 6/19 Reg Loss: 0.038402799516916275 Val Accuracy: 97.22222222222223\n",
      "Epoch 257/1000 Loss: 0.03814013674855232 num_feat: 6/19 Reg Loss: 0.03834031894803047 Val Accuracy: 97.22222222222223\n",
      "Epoch 258/1000 Loss: 0.046631865203380585 num_feat: 6/19 Reg Loss: 0.0382869653403759 Val Accuracy: 97.22222222222223\n",
      "Epoch 259/1000 Loss: 0.0544707253575325 num_feat: 6/19 Reg Loss: 0.03824855387210846 Val Accuracy: 97.22222222222223\n",
      "Epoch 260/1000 Loss: 0.07321640104055405 num_feat: 6/19 Reg Loss: 0.038209475576877594 Val Accuracy: 97.22222222222223\n",
      "Epoch 261/1000 Loss: 0.0566224604845047 num_feat: 6/19 Reg Loss: 0.038170285522937775 Val Accuracy: 97.22222222222223\n",
      "Epoch 262/1000 Loss: 0.10699402540922165 num_feat: 6/19 Reg Loss: 0.03812655434012413 Val Accuracy: 88.88888888888889\n",
      "Epoch 263/1000 Loss: 0.05498863384127617 num_feat: 6/19 Reg Loss: 0.03805192932486534 Val Accuracy: 97.22222222222223\n",
      "Epoch 264/1000 Loss: 0.04342738911509514 num_feat: 6/19 Reg Loss: 0.0379767045378685 Val Accuracy: 97.22222222222223\n",
      "Epoch 265/1000 Loss: 0.06522532552480698 num_feat: 6/19 Reg Loss: 0.037905674427747726 Val Accuracy: 97.22222222222223\n",
      "Epoch 266/1000 Loss: 0.060128360986709595 num_feat: 6/19 Reg Loss: 0.037835054099559784 Val Accuracy: 97.22222222222223\n",
      "Epoch 267/1000 Loss: 0.054741233587265015 num_feat: 6/19 Reg Loss: 0.03777296468615532 Val Accuracy: 97.22222222222223\n",
      "Epoch 268/1000 Loss: 0.048684608191251755 num_feat: 6/19 Reg Loss: 0.03771356865763664 Val Accuracy: 97.22222222222223\n",
      "Epoch 269/1000 Loss: 0.05026262626051903 num_feat: 6/19 Reg Loss: 0.03766701743006706 Val Accuracy: 91.66666666666667\n",
      "Epoch 270/1000 Loss: 0.09424420446157455 num_feat: 6/19 Reg Loss: 0.03762136027216911 Val Accuracy: 91.66666666666667\n",
      "Epoch 271/1000 Loss: 0.09915544092655182 num_feat: 6/19 Reg Loss: 0.03757646679878235 Val Accuracy: 97.22222222222223\n",
      "Epoch 272/1000 Loss: 0.05071427300572395 num_feat: 6/19 Reg Loss: 0.037531670182943344 Val Accuracy: 97.22222222222223\n",
      "Epoch 273/1000 Loss: 0.03981002792716026 num_feat: 6/19 Reg Loss: 0.03748520836234093 Val Accuracy: 97.22222222222223\n",
      "Epoch 274/1000 Loss: 0.07068463414907455 num_feat: 6/19 Reg Loss: 0.037440311163663864 Val Accuracy: 97.22222222222223\n",
      "Epoch 275/1000 Loss: 0.10067349672317505 num_feat: 6/19 Reg Loss: 0.037395622581243515 Val Accuracy: 97.22222222222223\n",
      "Epoch 276/1000 Loss: 0.057486873120069504 num_feat: 6/19 Reg Loss: 0.03735046088695526 Val Accuracy: 97.22222222222223\n",
      "Epoch 277/1000 Loss: 0.04673626273870468 num_feat: 6/19 Reg Loss: 0.03732162341475487 Val Accuracy: 97.22222222222223\n",
      "Epoch 278/1000 Loss: 0.042976267635822296 num_feat: 6/19 Reg Loss: 0.037292610853910446 Val Accuracy: 91.66666666666667\n",
      "Epoch 279/1000 Loss: 0.108896903693676 num_feat: 6/19 Reg Loss: 0.03726255148649216 Val Accuracy: 97.22222222222223\n",
      "Epoch 280/1000 Loss: 0.04838145896792412 num_feat: 6/19 Reg Loss: 0.03721624240279198 Val Accuracy: 97.22222222222223\n",
      "Epoch 281/1000 Loss: 0.040854938328266144 num_feat: 6/19 Reg Loss: 0.03716876357793808 Val Accuracy: 97.22222222222223\n",
      "Epoch 282/1000 Loss: 0.04094903916120529 num_feat: 6/19 Reg Loss: 0.037129610776901245 Val Accuracy: 97.22222222222223\n",
      "Epoch 283/1000 Loss: 0.030762584879994392 num_feat: 6/19 Reg Loss: 0.037098269909620285 Val Accuracy: 97.22222222222223\n",
      "Epoch 284/1000 Loss: 0.07870987057685852 num_feat: 6/19 Reg Loss: 0.03706841543316841 Val Accuracy: 97.22222222222223\n",
      "Epoch 285/1000 Loss: 0.04408776015043259 num_feat: 6/19 Reg Loss: 0.037052176892757416 Val Accuracy: 97.22222222222223\n",
      "Epoch 286/1000 Loss: 0.035303883254528046 num_feat: 6/19 Reg Loss: 0.037034984678030014 Val Accuracy: 97.22222222222223\n",
      "Epoch 287/1000 Loss: 0.043162573128938675 num_feat: 6/19 Reg Loss: 0.037016611546278 Val Accuracy: 97.22222222222223\n",
      "Epoch 288/1000 Loss: 0.047765664756298065 num_feat: 6/19 Reg Loss: 0.03699319437146187 Val Accuracy: 91.66666666666667\n",
      "Epoch 289/1000 Loss: 0.04390987753868103 num_feat: 6/19 Reg Loss: 0.03696148097515106 Val Accuracy: 97.22222222222223\n",
      "Epoch 290/1000 Loss: 0.040646929293870926 num_feat: 6/19 Reg Loss: 0.0369379036128521 Val Accuracy: 97.22222222222223\n",
      "Epoch 291/1000 Loss: 0.03841526061296463 num_feat: 6/19 Reg Loss: 0.03690830618143082 Val Accuracy: 97.22222222222223\n",
      "Epoch 292/1000 Loss: 0.03395054489374161 num_feat: 6/19 Reg Loss: 0.03687969595193863 Val Accuracy: 97.22222222222223\n",
      "Epoch 293/1000 Loss: 0.06412429362535477 num_feat: 6/19 Reg Loss: 0.03685068339109421 Val Accuracy: 100.0\n",
      "Epoch 294/1000 Loss: 0.03396117314696312 num_feat: 6/19 Reg Loss: 0.03682224452495575 Val Accuracy: 94.44444444444444\n",
      "Epoch 295/1000 Loss: 0.03596220165491104 num_feat: 6/19 Reg Loss: 0.036793436855077744 Val Accuracy: 97.22222222222223\n",
      "Epoch 296/1000 Loss: 0.035837240517139435 num_feat: 6/19 Reg Loss: 0.03676464781165123 Val Accuracy: 97.22222222222223\n",
      "Epoch 297/1000 Loss: 0.03706112131476402 num_feat: 6/19 Reg Loss: 0.03673556074500084 Val Accuracy: 97.22222222222223\n",
      "Epoch 298/1000 Loss: 0.032544445246458054 num_feat: 6/19 Reg Loss: 0.03669985383749008 Val Accuracy: 97.22222222222223\n",
      "Epoch 299/1000 Loss: 0.03976437449455261 num_feat: 6/19 Reg Loss: 0.036664996296167374 Val Accuracy: 97.22222222222223\n",
      "Epoch 300/1000 Loss: 0.041017234325408936 num_feat: 6/19 Reg Loss: 0.03663095459342003 Val Accuracy: 97.22222222222223\n",
      "Epoch 301/1000 Loss: 0.03618447855114937 num_feat: 6/19 Reg Loss: 0.036596689373254776 Val Accuracy: 97.22222222222223\n",
      "Epoch 302/1000 Loss: 0.07828722149133682 num_feat: 6/19 Reg Loss: 0.03656240552663803 Val Accuracy: 91.66666666666667\n",
      "Epoch 303/1000 Loss: 0.052268486469984055 num_feat: 6/19 Reg Loss: 0.03653364256024361 Val Accuracy: 91.66666666666667\n",
      "Epoch 304/1000 Loss: 0.04751837998628616 num_feat: 6/19 Reg Loss: 0.03649578616023064 Val Accuracy: 91.66666666666667\n",
      "Epoch 305/1000 Loss: 0.07282904535531998 num_feat: 6/19 Reg Loss: 0.03645942360162735 Val Accuracy: 91.66666666666667\n",
      "Epoch 306/1000 Loss: 0.0927496999502182 num_feat: 6/19 Reg Loss: 0.036418940871953964 Val Accuracy: 97.22222222222223\n",
      "Epoch 307/1000 Loss: 0.03524114936590195 num_feat: 6/19 Reg Loss: 0.03637984022498131 Val Accuracy: 97.22222222222223\n",
      "Epoch 308/1000 Loss: 0.04429037868976593 num_feat: 6/19 Reg Loss: 0.03634089231491089 Val Accuracy: 97.22222222222223\n",
      "Epoch 309/1000 Loss: 0.05673318728804588 num_feat: 6/19 Reg Loss: 0.03630474954843521 Val Accuracy: 97.22222222222223\n",
      "Epoch 310/1000 Loss: 0.10647197812795639 num_feat: 6/19 Reg Loss: 0.036280449479818344 Val Accuracy: 97.22222222222223\n",
      "Epoch 311/1000 Loss: 0.0713052749633789 num_feat: 6/19 Reg Loss: 0.0362558476626873 Val Accuracy: 97.22222222222223\n",
      "Epoch 312/1000 Loss: 0.032022424042224884 num_feat: 6/19 Reg Loss: 0.036231014877557755 Val Accuracy: 97.22222222222223\n",
      "Epoch 313/1000 Loss: 0.043118078261613846 num_feat: 6/19 Reg Loss: 0.0362059660255909 Val Accuracy: 91.66666666666667\n",
      "Epoch 314/1000 Loss: 0.07483645528554916 num_feat: 6/19 Reg Loss: 0.03618074581027031 Val Accuracy: 91.66666666666667\n",
      "Epoch 315/1000 Loss: 0.06259173154830933 num_feat: 6/19 Reg Loss: 0.03615804761648178 Val Accuracy: 97.22222222222223\n",
      "Epoch 316/1000 Loss: 0.03179773688316345 num_feat: 6/19 Reg Loss: 0.03613511100411415 Val Accuracy: 97.22222222222223\n",
      "Epoch 317/1000 Loss: 0.038052503019571304 num_feat: 6/19 Reg Loss: 0.03611113131046295 Val Accuracy: 97.22222222222223\n",
      "Epoch 318/1000 Loss: 0.10929686576128006 num_feat: 6/19 Reg Loss: 0.03608725965023041 Val Accuracy: 97.22222222222223\n",
      "Epoch 319/1000 Loss: 0.0313982218503952 num_feat: 6/19 Reg Loss: 0.036064743995666504 Val Accuracy: 97.22222222222223\n",
      "Epoch 320/1000 Loss: 0.05759113281965256 num_feat: 6/19 Reg Loss: 0.03605034947395325 Val Accuracy: 97.22222222222223\n",
      "Epoch 321/1000 Loss: 0.029509669169783592 num_feat: 6/19 Reg Loss: 0.03603556379675865 Val Accuracy: 94.44444444444444\n",
      "Epoch 322/1000 Loss: 0.054937902837991714 num_feat: 6/19 Reg Loss: 0.036019567400217056 Val Accuracy: 94.44444444444444\n",
      "Epoch 323/1000 Loss: 0.05036387965083122 num_feat: 6/19 Reg Loss: 0.03600082919001579 Val Accuracy: 97.22222222222223\n",
      "Epoch 324/1000 Loss: 0.031546466052532196 num_feat: 6/19 Reg Loss: 0.03598138689994812 Val Accuracy: 97.22222222222223\n",
      "Epoch 325/1000 Loss: 0.024951305240392685 num_feat: 6/19 Reg Loss: 0.03596073389053345 Val Accuracy: 97.22222222222223\n",
      "Epoch 326/1000 Loss: 0.04460650309920311 num_feat: 6/19 Reg Loss: 0.03594277799129486 Val Accuracy: 97.22222222222223\n",
      "Epoch 327/1000 Loss: 0.05099952220916748 num_feat: 6/19 Reg Loss: 0.03592412546277046 Val Accuracy: 97.22222222222223\n",
      "Epoch 328/1000 Loss: 0.0753077045083046 num_feat: 6/19 Reg Loss: 0.03590487316250801 Val Accuracy: 97.22222222222223\n",
      "Epoch 329/1000 Loss: 0.028294850140810013 num_feat: 6/19 Reg Loss: 0.03588234260678291 Val Accuracy: 94.44444444444444\n",
      "Epoch 330/1000 Loss: 0.04046313092112541 num_feat: 6/19 Reg Loss: 0.035855650901794434 Val Accuracy: 88.88888888888889\n",
      "Epoch 331/1000 Loss: 0.047878339886665344 num_feat: 6/19 Reg Loss: 0.03582802787423134 Val Accuracy: 97.22222222222223\n",
      "Epoch 332/1000 Loss: 0.03055652230978012 num_feat: 6/19 Reg Loss: 0.03578943759202957 Val Accuracy: 97.22222222222223\n",
      "Epoch 333/1000 Loss: 0.026105588302016258 num_feat: 6/19 Reg Loss: 0.035753119736909866 Val Accuracy: 97.22222222222223\n",
      "Epoch 334/1000 Loss: 0.03697735816240311 num_feat: 6/19 Reg Loss: 0.03571869060397148 Val Accuracy: 97.22222222222223\n",
      "Epoch 335/1000 Loss: 0.023221800103783607 num_feat: 6/19 Reg Loss: 0.03569585829973221 Val Accuracy: 97.22222222222223\n",
      "Epoch 336/1000 Loss: 0.038694195449352264 num_feat: 6/19 Reg Loss: 0.03567669540643692 Val Accuracy: 97.22222222222223\n",
      "Epoch 337/1000 Loss: 0.030123069882392883 num_feat: 6/19 Reg Loss: 0.03565710783004761 Val Accuracy: 94.44444444444444\n",
      "Epoch 338/1000 Loss: 0.02496151439845562 num_feat: 6/19 Reg Loss: 0.0356372706592083 Val Accuracy: 97.22222222222223\n",
      "Epoch 339/1000 Loss: 0.028703993186354637 num_feat: 6/19 Reg Loss: 0.03561563417315483 Val Accuracy: 97.22222222222223\n",
      "Epoch 340/1000 Loss: 0.0465257465839386 num_feat: 6/19 Reg Loss: 0.03559397906064987 Val Accuracy: 97.22222222222223\n",
      "Epoch 341/1000 Loss: 0.024990394711494446 num_feat: 6/19 Reg Loss: 0.0355585478246212 Val Accuracy: 97.22222222222223\n",
      "Epoch 342/1000 Loss: 0.0330197811126709 num_feat: 6/19 Reg Loss: 0.03552153706550598 Val Accuracy: 94.44444444444444\n",
      "Epoch 343/1000 Loss: 0.034684162586927414 num_feat: 6/19 Reg Loss: 0.03548640385270119 Val Accuracy: 97.22222222222223\n",
      "Epoch 344/1000 Loss: 0.03529740124940872 num_feat: 6/19 Reg Loss: 0.03545389696955681 Val Accuracy: 97.22222222222223\n",
      "Epoch 345/1000 Loss: 0.023651115596294403 num_feat: 6/19 Reg Loss: 0.03542478010058403 Val Accuracy: 97.22222222222223\n",
      "Epoch 346/1000 Loss: 0.023823579773306847 num_feat: 6/19 Reg Loss: 0.03540103882551193 Val Accuracy: 97.22222222222223\n",
      "Epoch 347/1000 Loss: 0.03330203518271446 num_feat: 6/19 Reg Loss: 0.035378288477659225 Val Accuracy: 88.88888888888889\n",
      "Epoch 348/1000 Loss: 0.03193042427301407 num_feat: 6/19 Reg Loss: 0.03535586968064308 Val Accuracy: 97.22222222222223\n",
      "Epoch 349/1000 Loss: 0.027650507166981697 num_feat: 6/19 Reg Loss: 0.0353337787091732 Val Accuracy: 97.22222222222223\n",
      "Epoch 350/1000 Loss: 0.024948600679636 num_feat: 6/19 Reg Loss: 0.03531091287732124 Val Accuracy: 97.22222222222223\n",
      "Epoch 351/1000 Loss: 0.03332260996103287 num_feat: 6/19 Reg Loss: 0.03528844937682152 Val Accuracy: 97.22222222222223\n",
      "Epoch 352/1000 Loss: 0.03147507458925247 num_feat: 6/19 Reg Loss: 0.035266365855932236 Val Accuracy: 97.22222222222223\n",
      "Epoch 353/1000 Loss: 0.021136069670319557 num_feat: 6/19 Reg Loss: 0.035244639962911606 Val Accuracy: 97.22222222222223\n",
      "Epoch 354/1000 Loss: 0.0214911587536335 num_feat: 6/19 Reg Loss: 0.03522346168756485 Val Accuracy: 97.22222222222223\n",
      "Epoch 355/1000 Loss: 0.02499707043170929 num_feat: 6/19 Reg Loss: 0.03520256280899048 Val Accuracy: 97.22222222222223\n",
      "Epoch 356/1000 Loss: 0.04067574068903923 num_feat: 6/19 Reg Loss: 0.03518195077776909 Val Accuracy: 97.22222222222223\n",
      "Epoch 357/1000 Loss: 0.05412791296839714 num_feat: 6/19 Reg Loss: 0.035150766372680664 Val Accuracy: 97.22222222222223\n",
      "Epoch 358/1000 Loss: 0.021321898326277733 num_feat: 6/19 Reg Loss: 0.035120125859975815 Val Accuracy: 97.22222222222223\n",
      "Epoch 359/1000 Loss: 0.019990243017673492 num_feat: 6/19 Reg Loss: 0.03509069234132767 Val Accuracy: 97.22222222222223\n",
      "Epoch 360/1000 Loss: 0.02001033164560795 num_feat: 6/19 Reg Loss: 0.03506280854344368 Val Accuracy: 97.22222222222223\n",
      "Epoch 361/1000 Loss: 0.019922573119401932 num_feat: 6/19 Reg Loss: 0.0350363664329052 Val Accuracy: 97.22222222222223\n",
      "Epoch 362/1000 Loss: 0.019695047289133072 num_feat: 6/19 Reg Loss: 0.035011012107133865 Val Accuracy: 97.22222222222223\n",
      "Epoch 363/1000 Loss: 0.019534822553396225 num_feat: 6/19 Reg Loss: 0.03498663753271103 Val Accuracy: 97.22222222222223\n",
      "Epoch 364/1000 Loss: 0.019421739503741264 num_feat: 6/19 Reg Loss: 0.03496314212679863 Val Accuracy: 97.22222222222223\n",
      "Epoch 365/1000 Loss: 0.019622020423412323 num_feat: 6/19 Reg Loss: 0.034940455108881 Val Accuracy: 97.22222222222223\n",
      "Epoch 366/1000 Loss: 0.019226184114813805 num_feat: 6/19 Reg Loss: 0.03491703048348427 Val Accuracy: 97.22222222222223\n",
      "Epoch 367/1000 Loss: 0.018591389060020447 num_feat: 6/19 Reg Loss: 0.03489445149898529 Val Accuracy: 97.22222222222223\n",
      "Epoch 368/1000 Loss: 0.019985990598797798 num_feat: 6/19 Reg Loss: 0.034872643649578094 Val Accuracy: 97.22222222222223\n",
      "Epoch 369/1000 Loss: 0.019114993512630463 num_feat: 6/19 Reg Loss: 0.03485124185681343 Val Accuracy: 97.22222222222223\n",
      "Epoch 370/1000 Loss: 0.018366774544119835 num_feat: 6/19 Reg Loss: 0.03483045473694801 Val Accuracy: 97.22222222222223\n",
      "Epoch 371/1000 Loss: 0.01874569244682789 num_feat: 6/19 Reg Loss: 0.034810274839401245 Val Accuracy: 97.22222222222223\n",
      "Epoch 372/1000 Loss: 0.0189029723405838 num_feat: 6/19 Reg Loss: 0.03479064628481865 Val Accuracy: 97.22222222222223\n",
      "Epoch 373/1000 Loss: 0.01819916069507599 num_feat: 6/19 Reg Loss: 0.03477152809500694 Val Accuracy: 97.22222222222223\n",
      "Epoch 374/1000 Loss: 0.017714396119117737 num_feat: 6/19 Reg Loss: 0.034752875566482544 Val Accuracy: 97.22222222222223\n",
      "Epoch 375/1000 Loss: 0.01793627440929413 num_feat: 6/19 Reg Loss: 0.03473465517163277 Val Accuracy: 97.22222222222223\n",
      "Epoch 376/1000 Loss: 0.017948664724826813 num_feat: 6/19 Reg Loss: 0.03471682593226433 Val Accuracy: 97.22222222222223\n",
      "Epoch 377/1000 Loss: 0.016507621854543686 num_feat: 6/19 Reg Loss: 0.034699369221925735 Val Accuracy: 97.22222222222223\n",
      "Epoch 378/1000 Loss: 0.01703552156686783 num_feat: 6/19 Reg Loss: 0.0346829928457737 Val Accuracy: 97.22222222222223\n",
      "Epoch 379/1000 Loss: 0.016952034085989 num_feat: 6/19 Reg Loss: 0.03466695174574852 Val Accuracy: 97.22222222222223\n",
      "Epoch 380/1000 Loss: 0.016887743026018143 num_feat: 6/19 Reg Loss: 0.03465111926198006 Val Accuracy: 97.22222222222223\n",
      "Epoch 381/1000 Loss: 0.016697177663445473 num_feat: 6/19 Reg Loss: 0.03463549539446831 Val Accuracy: 97.22222222222223\n",
      "Epoch 382/1000 Loss: 0.016461076214909554 num_feat: 6/19 Reg Loss: 0.034620072692632675 Val Accuracy: 97.22222222222223\n",
      "Epoch 383/1000 Loss: 0.016333965584635735 num_feat: 6/19 Reg Loss: 0.03460483253002167 Val Accuracy: 97.22222222222223\n",
      "Epoch 384/1000 Loss: 0.016240544617176056 num_feat: 6/19 Reg Loss: 0.03458977863192558 Val Accuracy: 97.22222222222223\n",
      "Epoch 385/1000 Loss: 0.016027819365262985 num_feat: 6/19 Reg Loss: 0.03457489237189293 Val Accuracy: 97.22222222222223\n",
      "Epoch 386/1000 Loss: 0.01450307760387659 num_feat: 6/19 Reg Loss: 0.03456017002463341 Val Accuracy: 97.22222222222223\n",
      "Epoch 387/1000 Loss: 0.026497233659029007 num_feat: 6/19 Reg Loss: 0.03454614803195 Val Accuracy: 97.22222222222223\n",
      "Epoch 388/1000 Loss: 0.021419573575258255 num_feat: 6/19 Reg Loss: 0.03452463075518608 Val Accuracy: 97.22222222222223\n",
      "Epoch 389/1000 Loss: 0.022563669830560684 num_feat: 6/19 Reg Loss: 0.034504104405641556 Val Accuracy: 97.22222222222223\n",
      "Epoch 390/1000 Loss: 0.019627690315246582 num_feat: 6/19 Reg Loss: 0.03448518365621567 Val Accuracy: 97.22222222222223\n",
      "Epoch 391/1000 Loss: 0.01902168057858944 num_feat: 6/19 Reg Loss: 0.034466978162527084 Val Accuracy: 97.22222222222223\n",
      "Epoch 392/1000 Loss: 0.026182183995842934 num_feat: 6/19 Reg Loss: 0.03445333614945412 Val Accuracy: 97.22222222222223\n",
      "Epoch 393/1000 Loss: 0.030221186578273773 num_feat: 6/19 Reg Loss: 0.03443911299109459 Val Accuracy: 97.22222222222223\n",
      "Epoch 394/1000 Loss: 0.018866118043661118 num_feat: 6/19 Reg Loss: 0.03442509099841118 Val Accuracy: 97.22222222222223\n",
      "Epoch 395/1000 Loss: 0.02565276063978672 num_feat: 6/19 Reg Loss: 0.03441125527024269 Val Accuracy: 97.22222222222223\n",
      "Epoch 396/1000 Loss: 0.017347099259495735 num_feat: 6/19 Reg Loss: 0.03439764305949211 Val Accuracy: 97.22222222222223\n",
      "Epoch 397/1000 Loss: 0.015805169939994812 num_feat: 6/19 Reg Loss: 0.034383926540613174 Val Accuracy: 97.22222222222223\n",
      "Epoch 398/1000 Loss: 0.013958628289401531 num_feat: 6/19 Reg Loss: 0.03437039256095886 Val Accuracy: 97.22222222222223\n",
      "Epoch 399/1000 Loss: 0.014490198343992233 num_feat: 6/19 Reg Loss: 0.03435708209872246 Val Accuracy: 97.22222222222223\n",
      "Epoch 400/1000 Loss: 0.017472168430685997 num_feat: 6/19 Reg Loss: 0.03434402123093605 Val Accuracy: 97.22222222222223\n",
      "Epoch 401/1000 Loss: 0.01404277328401804 num_feat: 6/19 Reg Loss: 0.03433056175708771 Val Accuracy: 97.22222222222223\n",
      "Epoch 402/1000 Loss: 0.013602365739643574 num_feat: 6/19 Reg Loss: 0.03431729972362518 Val Accuracy: 97.22222222222223\n",
      "Epoch 403/1000 Loss: 0.015410564839839935 num_feat: 6/19 Reg Loss: 0.03430447354912758 Val Accuracy: 97.22222222222223\n",
      "Epoch 404/1000 Loss: 0.014630182646214962 num_feat: 6/19 Reg Loss: 0.03429180011153221 Val Accuracy: 97.22222222222223\n",
      "Epoch 405/1000 Loss: 0.013597048819065094 num_feat: 6/19 Reg Loss: 0.03427925333380699 Val Accuracy: 97.22222222222223\n",
      "Epoch 406/1000 Loss: 0.013999909162521362 num_feat: 6/19 Reg Loss: 0.03426684811711311 Val Accuracy: 97.22222222222223\n",
      "Epoch 407/1000 Loss: 0.014484632760286331 num_feat: 6/19 Reg Loss: 0.03425457328557968 Val Accuracy: 97.22222222222223\n",
      "Epoch 408/1000 Loss: 0.013384302146732807 num_feat: 6/19 Reg Loss: 0.034242235124111176 Val Accuracy: 97.22222222222223\n",
      "Epoch 409/1000 Loss: 0.013171837665140629 num_feat: 6/19 Reg Loss: 0.034229692071676254 Val Accuracy: 97.22222222222223\n",
      "Epoch 410/1000 Loss: 0.013307509943842888 num_feat: 6/19 Reg Loss: 0.03421732410788536 Val Accuracy: 97.22222222222223\n",
      "Epoch 411/1000 Loss: 0.01370223332196474 num_feat: 6/19 Reg Loss: 0.0342053659260273 Val Accuracy: 97.22222222222223\n",
      "Epoch 412/1000 Loss: 0.013241835869848728 num_feat: 6/19 Reg Loss: 0.03419381380081177 Val Accuracy: 97.22222222222223\n",
      "Epoch 413/1000 Loss: 0.012716250494122505 num_feat: 6/19 Reg Loss: 0.0341823510825634 Val Accuracy: 97.22222222222223\n",
      "Epoch 414/1000 Loss: 0.013098456896841526 num_feat: 6/19 Reg Loss: 0.034170981496572495 Val Accuracy: 97.22222222222223\n",
      "Epoch 415/1000 Loss: 0.013166738674044609 num_feat: 6/19 Reg Loss: 0.03415968641638756 Val Accuracy: 97.22222222222223\n",
      "Epoch 416/1000 Loss: 0.012566929683089256 num_feat: 6/19 Reg Loss: 0.034148477017879486 Val Accuracy: 97.22222222222223\n",
      "Epoch 417/1000 Loss: 0.012326185591518879 num_feat: 6/19 Reg Loss: 0.034137360751628876 Val Accuracy: 97.22222222222223\n",
      "Epoch 418/1000 Loss: 0.014389390125870705 num_feat: 6/19 Reg Loss: 0.03412632271647453 Val Accuracy: 97.22222222222223\n",
      "Epoch 419/1000 Loss: 0.015894092619419098 num_feat: 6/19 Reg Loss: 0.03411537781357765 Val Accuracy: 97.22222222222223\n",
      "Epoch 420/1000 Loss: 0.015620545484125614 num_feat: 6/19 Reg Loss: 0.03410451486706734 Val Accuracy: 97.22222222222223\n",
      "Epoch 421/1000 Loss: 0.012187126092612743 num_feat: 6/19 Reg Loss: 0.03409373760223389 Val Accuracy: 97.22222222222223\n",
      "Epoch 422/1000 Loss: 0.011848272755742073 num_feat: 6/19 Reg Loss: 0.03408275544643402 Val Accuracy: 97.22222222222223\n",
      "Epoch 423/1000 Loss: 0.012517576105892658 num_feat: 6/19 Reg Loss: 0.03407188877463341 Val Accuracy: 97.22222222222223\n",
      "Epoch 424/1000 Loss: 0.012484476901590824 num_feat: 6/19 Reg Loss: 0.03406112641096115 Val Accuracy: 97.22222222222223\n",
      "Epoch 425/1000 Loss: 0.012107512913644314 num_feat: 6/19 Reg Loss: 0.03405047208070755 Val Accuracy: 97.22222222222223\n",
      "Epoch 426/1000 Loss: 0.011101349256932735 num_feat: 6/19 Reg Loss: 0.03403922915458679 Val Accuracy: 97.22222222222223\n",
      "Epoch 427/1000 Loss: 0.010513491928577423 num_feat: 6/19 Reg Loss: 0.03402836620807648 Val Accuracy: 97.22222222222223\n",
      "Epoch 428/1000 Loss: 0.01452942006289959 num_feat: 6/19 Reg Loss: 0.03401777520775795 Val Accuracy: 97.22222222222223\n",
      "Epoch 429/1000 Loss: 0.012314368970692158 num_feat: 6/19 Reg Loss: 0.034007295966148376 Val Accuracy: 97.22222222222223\n",
      "Epoch 430/1000 Loss: 0.011107433587312698 num_feat: 6/19 Reg Loss: 0.033996932208538055 Val Accuracy: 97.22222222222223\n",
      "Epoch 431/1000 Loss: 0.012632261030375957 num_feat: 6/19 Reg Loss: 0.03398667275905609 Val Accuracy: 97.22222222222223\n",
      "Epoch 432/1000 Loss: 0.012842514552175999 num_feat: 6/19 Reg Loss: 0.03397664055228233 Val Accuracy: 97.22222222222223\n",
      "Epoch 433/1000 Loss: 0.011135655455291271 num_feat: 6/19 Reg Loss: 0.03396669775247574 Val Accuracy: 97.22222222222223\n",
      "Epoch 434/1000 Loss: 0.010950671508908272 num_feat: 6/19 Reg Loss: 0.03395683690905571 Val Accuracy: 97.22222222222223\n",
      "Epoch 435/1000 Loss: 0.01262371614575386 num_feat: 6/19 Reg Loss: 0.03394705802202225 Val Accuracy: 97.22222222222223\n",
      "Epoch 436/1000 Loss: 0.011239622719585896 num_feat: 6/19 Reg Loss: 0.03393691033124924 Val Accuracy: 97.22222222222223\n",
      "Epoch 437/1000 Loss: 0.010449033230543137 num_feat: 6/19 Reg Loss: 0.03392689302563667 Val Accuracy: 97.22222222222223\n",
      "Epoch 438/1000 Loss: 0.015512615442276001 num_feat: 6/19 Reg Loss: 0.03391699492931366 Val Accuracy: 97.22222222222223\n",
      "Epoch 439/1000 Loss: 0.01620406284928322 num_feat: 6/19 Reg Loss: 0.0339072160422802 Val Accuracy: 97.22222222222223\n",
      "Epoch 440/1000 Loss: 0.011668182909488678 num_feat: 6/19 Reg Loss: 0.03389851748943329 Val Accuracy: 94.44444444444444\n",
      "Epoch 441/1000 Loss: 0.024917617440223694 num_feat: 6/19 Reg Loss: 0.0338895209133625 Val Accuracy: 97.22222222222223\n",
      "Epoch 442/1000 Loss: 0.012625440023839474 num_feat: 6/19 Reg Loss: 0.0338805690407753 Val Accuracy: 97.22222222222223\n",
      "Epoch 443/1000 Loss: 0.011769570410251617 num_feat: 6/19 Reg Loss: 0.03387164697051048 Val Accuracy: 97.22222222222223\n",
      "Epoch 444/1000 Loss: 0.01995604857802391 num_feat: 6/19 Reg Loss: 0.03386276587843895 Val Accuracy: 97.22222222222223\n",
      "Epoch 445/1000 Loss: 0.01687336340546608 num_feat: 6/19 Reg Loss: 0.03385394066572189 Val Accuracy: 97.22222222222223\n",
      "Epoch 446/1000 Loss: 0.025586972013115883 num_feat: 6/19 Reg Loss: 0.03384514898061752 Val Accuracy: 94.44444444444444\n",
      "Epoch 447/1000 Loss: 0.03516436368227005 num_feat: 6/19 Reg Loss: 0.03383387625217438 Val Accuracy: 94.44444444444444\n",
      "Epoch 448/1000 Loss: 0.03326631337404251 num_feat: 6/19 Reg Loss: 0.03381751477718353 Val Accuracy: 97.22222222222223\n",
      "Epoch 449/1000 Loss: 0.011287556029856205 num_feat: 6/19 Reg Loss: 0.03380214422941208 Val Accuracy: 97.22222222222223\n",
      "Epoch 450/1000 Loss: 0.01667393371462822 num_feat: 6/19 Reg Loss: 0.03378749638795853 Val Accuracy: 97.22222222222223\n",
      "Epoch 451/1000 Loss: 0.025952262803912163 num_feat: 6/19 Reg Loss: 0.03377362713217735 Val Accuracy: 97.22222222222223\n",
      "Epoch 452/1000 Loss: 0.022483577951788902 num_feat: 6/19 Reg Loss: 0.033760443329811096 Val Accuracy: 97.22222222222223\n",
      "Epoch 453/1000 Loss: 0.009510594420135021 num_feat: 6/19 Reg Loss: 0.03374733030796051 Val Accuracy: 97.22222222222223\n",
      "Epoch 454/1000 Loss: 0.020617883652448654 num_feat: 6/19 Reg Loss: 0.03373481705784798 Val Accuracy: 94.44444444444444\n",
      "Epoch 455/1000 Loss: 0.0265725739300251 num_feat: 6/19 Reg Loss: 0.03372284397482872 Val Accuracy: 97.22222222222223\n",
      "Epoch 456/1000 Loss: 0.011878492310643196 num_feat: 6/19 Reg Loss: 0.03371044620871544 Val Accuracy: 97.22222222222223\n",
      "Epoch 457/1000 Loss: 0.014978872612118721 num_feat: 6/19 Reg Loss: 0.0336979478597641 Val Accuracy: 97.22222222222223\n",
      "Epoch 458/1000 Loss: 0.025129936635494232 num_feat: 6/19 Reg Loss: 0.033685993403196335 Val Accuracy: 97.22222222222223\n",
      "Epoch 459/1000 Loss: 0.015605885535478592 num_feat: 6/19 Reg Loss: 0.033674538135528564 Val Accuracy: 97.22222222222223\n",
      "Epoch 460/1000 Loss: 0.00919589027762413 num_feat: 6/19 Reg Loss: 0.03366344794631004 Val Accuracy: 97.22222222222223\n",
      "Epoch 461/1000 Loss: 0.016597528010606766 num_feat: 6/19 Reg Loss: 0.03365277126431465 Val Accuracy: 94.44444444444444\n",
      "Epoch 462/1000 Loss: 0.018626941367983818 num_feat: 6/19 Reg Loss: 0.03364245966076851 Val Accuracy: 97.22222222222223\n",
      "Epoch 463/1000 Loss: 0.010625193826854229 num_feat: 6/19 Reg Loss: 0.033632487058639526 Val Accuracy: 97.22222222222223\n",
      "Epoch 464/1000 Loss: 0.009942843578755856 num_feat: 6/19 Reg Loss: 0.033622827380895615 Val Accuracy: 97.22222222222223\n",
      "Epoch 465/1000 Loss: 0.015453680418431759 num_feat: 6/19 Reg Loss: 0.03361343964934349 Val Accuracy: 97.22222222222223\n",
      "Epoch 466/1000 Loss: 0.01370338723063469 num_feat: 6/19 Reg Loss: 0.03360430523753166 Val Accuracy: 97.22222222222223\n",
      "Epoch 467/1000 Loss: 0.008897049352526665 num_feat: 6/19 Reg Loss: 0.033595409244298935 Val Accuracy: 97.22222222222223\n",
      "Epoch 468/1000 Loss: 0.010446210391819477 num_feat: 6/19 Reg Loss: 0.03358671814203262 Val Accuracy: 86.11111111111111\n",
      "Epoch 469/1000 Loss: 0.014308469370007515 num_feat: 6/19 Reg Loss: 0.03357822075486183 Val Accuracy: 97.22222222222223\n",
      "Epoch 470/1000 Loss: 0.010458584874868393 num_feat: 6/19 Reg Loss: 0.033569157123565674 Val Accuracy: 97.22222222222223\n",
      "Epoch 471/1000 Loss: 0.008451645262539387 num_feat: 6/19 Reg Loss: 0.03356034308671951 Val Accuracy: 97.22222222222223\n",
      "Epoch 472/1000 Loss: 0.010506500490009785 num_feat: 6/19 Reg Loss: 0.03355174884200096 Val Accuracy: 88.88888888888889\n",
      "Epoch 473/1000 Loss: 0.011347675696015358 num_feat: 6/19 Reg Loss: 0.033543359488248825 Val Accuracy: 97.22222222222223\n",
      "Epoch 474/1000 Loss: 0.009029087610542774 num_feat: 6/19 Reg Loss: 0.03353516012430191 Val Accuracy: 97.22222222222223\n",
      "Epoch 475/1000 Loss: 0.008661762811243534 num_feat: 6/19 Reg Loss: 0.03352713584899902 Val Accuracy: 97.22222222222223\n",
      "Epoch 476/1000 Loss: 0.00933702290058136 num_feat: 6/19 Reg Loss: 0.03351914882659912 Val Accuracy: 97.22222222222223\n",
      "Epoch 477/1000 Loss: 0.009198790416121483 num_feat: 6/19 Reg Loss: 0.033511318266391754 Val Accuracy: 97.22222222222223\n",
      "Epoch 478/1000 Loss: 0.008194820955395699 num_feat: 6/19 Reg Loss: 0.033503636717796326 Val Accuracy: 97.22222222222223\n",
      "Epoch 479/1000 Loss: 0.008190742693841457 num_feat: 6/19 Reg Loss: 0.03349604085087776 Val Accuracy: 97.22222222222223\n",
      "Epoch 480/1000 Loss: 0.008900240063667297 num_feat: 6/19 Reg Loss: 0.03348856791853905 Val Accuracy: 97.22222222222223\n",
      "Epoch 481/1000 Loss: 0.008552919141948223 num_feat: 6/19 Reg Loss: 0.03348122909665108 Val Accuracy: 97.22222222222223\n",
      "Epoch 482/1000 Loss: 0.007805872708559036 num_feat: 6/19 Reg Loss: 0.03347399830818176 Val Accuracy: 97.22222222222223\n",
      "Epoch 483/1000 Loss: 0.007960524410009384 num_feat: 6/19 Reg Loss: 0.033466871827840805 Val Accuracy: 97.22222222222223\n",
      "Epoch 484/1000 Loss: 0.007755504455417395 num_feat: 6/19 Reg Loss: 0.033459845930337906 Val Accuracy: 97.22222222222223\n",
      "Epoch 485/1000 Loss: 0.009178699925541878 num_feat: 6/19 Reg Loss: 0.03345291689038277 Val Accuracy: 97.22222222222223\n",
      "Epoch 486/1000 Loss: 0.0270092636346817 num_feat: 6/19 Reg Loss: 0.03344608470797539 Val Accuracy: 94.44444444444444\n",
      "Epoch 487/1000 Loss: 0.019627651199698448 num_feat: 6/19 Reg Loss: 0.03344070911407471 Val Accuracy: 94.44444444444444\n",
      "Epoch 488/1000 Loss: 0.019763341173529625 num_feat: 6/19 Reg Loss: 0.03343527764081955 Val Accuracy: 97.22222222222223\n",
      "Epoch 489/1000 Loss: 0.007426935248076916 num_feat: 6/19 Reg Loss: 0.03342907875776291 Val Accuracy: 97.22222222222223\n",
      "Epoch 490/1000 Loss: 0.010573575273156166 num_feat: 6/19 Reg Loss: 0.033422913402318954 Val Accuracy: 97.22222222222223\n",
      "Epoch 491/1000 Loss: 0.015549871139228344 num_feat: 6/19 Reg Loss: 0.03341677412390709 Val Accuracy: 97.22222222222223\n",
      "Epoch 492/1000 Loss: 0.009296500124037266 num_feat: 6/19 Reg Loss: 0.03341066464781761 Val Accuracy: 97.22222222222223\n",
      "Epoch 493/1000 Loss: 0.007242526859045029 num_feat: 6/19 Reg Loss: 0.03340493515133858 Val Accuracy: 97.22222222222223\n",
      "Epoch 494/1000 Loss: 0.009582466445863247 num_feat: 6/19 Reg Loss: 0.033399201929569244 Val Accuracy: 97.22222222222223\n",
      "Epoch 495/1000 Loss: 0.011413770727813244 num_feat: 6/19 Reg Loss: 0.033393457531929016 Val Accuracy: 97.22222222222223\n",
      "Epoch 496/1000 Loss: 0.008571082726120949 num_feat: 6/19 Reg Loss: 0.03338770940899849 Val Accuracy: 97.22222222222223\n",
      "Epoch 497/1000 Loss: 0.007133424282073975 num_feat: 6/19 Reg Loss: 0.03338191285729408 Val Accuracy: 97.22222222222223\n",
      "Epoch 498/1000 Loss: 0.009100912138819695 num_feat: 6/19 Reg Loss: 0.033376120030879974 Val Accuracy: 97.22222222222223\n",
      "Epoch 499/1000 Loss: 0.009595670737326145 num_feat: 6/19 Reg Loss: 0.03337034210562706 Val Accuracy: 97.22222222222223\n",
      "Epoch 500/1000 Loss: 0.00750772887840867 num_feat: 6/19 Reg Loss: 0.03336457535624504 Val Accuracy: 97.22222222222223\n",
      "Epoch 501/1000 Loss: 0.0070810033939778805 num_feat: 6/19 Reg Loss: 0.033358823508024216 Val Accuracy: 97.22222222222223\n",
      "Epoch 502/1000 Loss: 0.008604198694229126 num_feat: 6/19 Reg Loss: 0.033353082835674286 Val Accuracy: 97.22222222222223\n",
      "Epoch 503/1000 Loss: 0.00819595716893673 num_feat: 6/19 Reg Loss: 0.033347245305776596 Val Accuracy: 97.22222222222223\n",
      "Epoch 504/1000 Loss: 0.008793707937002182 num_feat: 6/19 Reg Loss: 0.033341433852910995 Val Accuracy: 97.22222222222223\n",
      "Epoch 505/1000 Loss: 0.008364254608750343 num_feat: 6/19 Reg Loss: 0.03333527222275734 Val Accuracy: 97.22222222222223\n",
      "Epoch 506/1000 Loss: 0.010952096432447433 num_feat: 6/19 Reg Loss: 0.033329181373119354 Val Accuracy: 97.22222222222223\n",
      "Epoch 507/1000 Loss: 0.008148319087922573 num_feat: 6/19 Reg Loss: 0.03332315385341644 Val Accuracy: 97.22222222222223\n",
      "Epoch 508/1000 Loss: 0.006456929259002209 num_feat: 6/19 Reg Loss: 0.033317457884550095 Val Accuracy: 97.22222222222223\n",
      "Epoch 509/1000 Loss: 0.007160995155572891 num_feat: 6/19 Reg Loss: 0.0333116352558136 Val Accuracy: 97.22222222222223\n",
      "Epoch 510/1000 Loss: 0.00868869200348854 num_feat: 6/19 Reg Loss: 0.03330586478114128 Val Accuracy: 97.22222222222223\n",
      "Epoch 511/1000 Loss: 0.006722808815538883 num_feat: 6/19 Reg Loss: 0.03329974412918091 Val Accuracy: 97.22222222222223\n",
      "Epoch 512/1000 Loss: 0.006510015111416578 num_feat: 6/19 Reg Loss: 0.0332937128841877 Val Accuracy: 97.22222222222223\n",
      "Epoch 513/1000 Loss: 0.006516512483358383 num_feat: 6/19 Reg Loss: 0.033287759870290756 Val Accuracy: 97.22222222222223\n",
      "Epoch 514/1000 Loss: 0.007559833116829395 num_feat: 6/19 Reg Loss: 0.03328196331858635 Val Accuracy: 97.22222222222223\n",
      "Epoch 515/1000 Loss: 0.007103337906301022 num_feat: 6/19 Reg Loss: 0.03327622637152672 Val Accuracy: 97.22222222222223\n",
      "Epoch 516/1000 Loss: 0.006341628264635801 num_feat: 6/19 Reg Loss: 0.033270470798015594 Val Accuracy: 97.22222222222223\n",
      "Epoch 517/1000 Loss: 0.007163863629102707 num_feat: 6/19 Reg Loss: 0.03326477110385895 Val Accuracy: 97.22222222222223\n",
      "Epoch 518/1000 Loss: 0.007205613423138857 num_feat: 6/19 Reg Loss: 0.03325914219021797 Val Accuracy: 94.44444444444444\n",
      "Epoch 519/1000 Loss: 0.006342626642435789 num_feat: 6/19 Reg Loss: 0.03325347229838371 Val Accuracy: 97.22222222222223\n",
      "Epoch 520/1000 Loss: 0.00617265934124589 num_feat: 6/19 Reg Loss: 0.033247873187065125 Val Accuracy: 97.22222222222223\n",
      "Epoch 521/1000 Loss: 0.006707038264721632 num_feat: 6/19 Reg Loss: 0.03324232995510101 Val Accuracy: 97.22222222222223\n",
      "Epoch 522/1000 Loss: 0.005857742857187986 num_feat: 6/19 Reg Loss: 0.033236850053071976 Val Accuracy: 97.22222222222223\n",
      "Epoch 523/1000 Loss: 0.00631027203053236 num_feat: 6/19 Reg Loss: 0.0332319438457489 Val Accuracy: 97.22222222222223\n",
      "Epoch 524/1000 Loss: 0.005942140705883503 num_feat: 6/19 Reg Loss: 0.03322703391313553 Val Accuracy: 97.22222222222223\n",
      "Epoch 525/1000 Loss: 0.00611055688932538 num_feat: 6/19 Reg Loss: 0.033222127705812454 Val Accuracy: 97.22222222222223\n",
      "Epoch 526/1000 Loss: 0.006724979728460312 num_feat: 6/19 Reg Loss: 0.0332171805202961 Val Accuracy: 97.22222222222223\n",
      "Epoch 527/1000 Loss: 0.006510758772492409 num_feat: 6/19 Reg Loss: 0.03321224823594093 Val Accuracy: 97.22222222222223\n",
      "Epoch 528/1000 Loss: 0.005868148524314165 num_feat: 6/19 Reg Loss: 0.03320732340216637 Val Accuracy: 97.22222222222223\n",
      "Epoch 529/1000 Loss: 0.005903090350329876 num_feat: 6/19 Reg Loss: 0.03320242092013359 Val Accuracy: 97.22222222222223\n",
      "Epoch 530/1000 Loss: 0.006195073947310448 num_feat: 6/19 Reg Loss: 0.03319752216339111 Val Accuracy: 97.22222222222223\n",
      "Epoch 531/1000 Loss: 0.0061545828357338905 num_feat: 6/19 Reg Loss: 0.03319283202290535 Val Accuracy: 97.22222222222223\n",
      "Epoch 532/1000 Loss: 0.005705236457288265 num_feat: 6/19 Reg Loss: 0.033188145607709885 Val Accuracy: 97.22222222222223\n",
      "Epoch 533/1000 Loss: 0.005718602798879147 num_feat: 6/19 Reg Loss: 0.03318345174193382 Val Accuracy: 97.22222222222223\n",
      "Epoch 534/1000 Loss: 0.005964844021946192 num_feat: 6/19 Reg Loss: 0.03317875787615776 Val Accuracy: 97.22222222222223\n",
      "Epoch 535/1000 Loss: 0.005857683718204498 num_feat: 6/19 Reg Loss: 0.033174067735672 Val Accuracy: 97.22222222222223\n",
      "Epoch 536/1000 Loss: 0.005562115926295519 num_feat: 6/19 Reg Loss: 0.03316938877105713 Val Accuracy: 97.22222222222223\n",
      "Epoch 537/1000 Loss: 0.005548755172640085 num_feat: 6/19 Reg Loss: 0.03316471725702286 Val Accuracy: 97.22222222222223\n",
      "Epoch 538/1000 Loss: 0.005706485826522112 num_feat: 6/19 Reg Loss: 0.033160045742988586 Val Accuracy: 97.22222222222223\n",
      "Epoch 539/1000 Loss: 0.005626255180686712 num_feat: 6/19 Reg Loss: 0.03315539285540581 Val Accuracy: 97.22222222222223\n",
      "Epoch 540/1000 Loss: 0.0054673743434250355 num_feat: 6/19 Reg Loss: 0.033150747418403625 Val Accuracy: 97.22222222222223\n",
      "Epoch 541/1000 Loss: 0.0054268804378807545 num_feat: 6/19 Reg Loss: 0.033146072179079056 Val Accuracy: 97.22222222222223\n",
      "Epoch 542/1000 Loss: 0.005574779585003853 num_feat: 6/19 Reg Loss: 0.03314141556620598 Val Accuracy: 97.22222222222223\n",
      "Epoch 543/1000 Loss: 0.00539506645873189 num_feat: 6/19 Reg Loss: 0.033136751502752304 Val Accuracy: 97.22222222222223\n",
      "Epoch 544/1000 Loss: 0.005278503056615591 num_feat: 6/19 Reg Loss: 0.03313210606575012 Val Accuracy: 97.22222222222223\n",
      "Epoch 545/1000 Loss: 0.005231174174696207 num_feat: 6/19 Reg Loss: 0.03312748298048973 Val Accuracy: 97.22222222222223\n",
      "Epoch 546/1000 Loss: 0.005409464705735445 num_feat: 6/19 Reg Loss: 0.03312293812632561 Val Accuracy: 97.22222222222223\n",
      "Epoch 547/1000 Loss: 0.005298017989844084 num_feat: 6/19 Reg Loss: 0.03311840817332268 Val Accuracy: 94.44444444444444\n",
      "Epoch 548/1000 Loss: 0.005164457019418478 num_feat: 6/19 Reg Loss: 0.03311390057206154 Val Accuracy: 97.22222222222223\n",
      "Epoch 549/1000 Loss: 0.005182576831430197 num_feat: 6/19 Reg Loss: 0.033109404146671295 Val Accuracy: 97.22222222222223\n",
      "Epoch 550/1000 Loss: 0.0052278852090239525 num_feat: 6/19 Reg Loss: 0.033104926347732544 Val Accuracy: 97.22222222222223\n",
      "Epoch 551/1000 Loss: 0.005154540296643972 num_feat: 6/19 Reg Loss: 0.03310047462582588 Val Accuracy: 97.22222222222223\n",
      "Epoch 552/1000 Loss: 0.005053949076682329 num_feat: 6/19 Reg Loss: 0.033096037805080414 Val Accuracy: 97.22222222222223\n",
      "Epoch 553/1000 Loss: 0.004955006763339043 num_feat: 6/19 Reg Loss: 0.03309161961078644 Val Accuracy: 97.22222222222223\n",
      "Epoch 554/1000 Loss: 0.005165637005120516 num_feat: 6/19 Reg Loss: 0.03308723866939545 Val Accuracy: 97.22222222222223\n",
      "Epoch 555/1000 Loss: 0.005100870970636606 num_feat: 6/19 Reg Loss: 0.033082809299230576 Val Accuracy: 97.22222222222223\n",
      "Epoch 556/1000 Loss: 0.004962270613759756 num_feat: 6/19 Reg Loss: 0.033078402280807495 Val Accuracy: 97.22222222222223\n",
      "Epoch 557/1000 Loss: 0.004931923933327198 num_feat: 6/19 Reg Loss: 0.033074021339416504 Val Accuracy: 94.44444444444444\n",
      "Epoch 558/1000 Loss: 0.004971647635102272 num_feat: 6/19 Reg Loss: 0.0330696664750576 Val Accuracy: 97.22222222222223\n",
      "Epoch 559/1000 Loss: 0.004938156343996525 num_feat: 6/19 Reg Loss: 0.033065345138311386 Val Accuracy: 97.22222222222223\n",
      "Epoch 560/1000 Loss: 0.004848527256399393 num_feat: 6/19 Reg Loss: 0.03306104242801666 Val Accuracy: 97.22222222222223\n",
      "Epoch 561/1000 Loss: 0.00475310767069459 num_feat: 6/19 Reg Loss: 0.03305676206946373 Val Accuracy: 97.22222222222223\n",
      "Epoch 562/1000 Loss: 0.004794313572347164 num_feat: 6/19 Reg Loss: 0.03305259719491005 Val Accuracy: 97.22222222222223\n",
      "Epoch 563/1000 Loss: 0.004762930329889059 num_feat: 6/19 Reg Loss: 0.03304844722151756 Val Accuracy: 97.22222222222223\n",
      "Epoch 564/1000 Loss: 0.004740043543279171 num_feat: 6/19 Reg Loss: 0.03304430842399597 Val Accuracy: 97.22222222222223\n",
      "Epoch 565/1000 Loss: 0.00472002848982811 num_feat: 6/19 Reg Loss: 0.03304018825292587 Val Accuracy: 97.22222222222223\n",
      "Epoch 566/1000 Loss: 0.004693858325481415 num_feat: 6/19 Reg Loss: 0.03303607553243637 Val Accuracy: 97.22222222222223\n",
      "Epoch 567/1000 Loss: 0.0046651316806674 num_feat: 6/19 Reg Loss: 0.03303198143839836 Val Accuracy: 97.22222222222223\n",
      "Epoch 568/1000 Loss: 0.004641431849449873 num_feat: 6/19 Reg Loss: 0.03302789852023125 Val Accuracy: 97.22222222222223\n",
      "Epoch 569/1000 Loss: 0.004682271741330624 num_feat: 6/19 Reg Loss: 0.03302384167909622 Val Accuracy: 97.22222222222223\n",
      "Epoch 570/1000 Loss: 0.00480584567412734 num_feat: 6/19 Reg Loss: 0.0330195426940918 Val Accuracy: 94.44444444444444\n",
      "Epoch 571/1000 Loss: 0.004889699164777994 num_feat: 6/19 Reg Loss: 0.03301529213786125 Val Accuracy: 97.22222222222223\n",
      "Epoch 572/1000 Loss: 0.004582896362990141 num_feat: 6/19 Reg Loss: 0.03301108255982399 Val Accuracy: 97.22222222222223\n",
      "Epoch 573/1000 Loss: 0.004500237759202719 num_feat: 6/19 Reg Loss: 0.03300727903842926 Val Accuracy: 97.22222222222223\n",
      "Epoch 574/1000 Loss: 0.0045714243315160275 num_feat: 6/19 Reg Loss: 0.03300353139638901 Val Accuracy: 97.22222222222223\n",
      "Epoch 575/1000 Loss: 0.004660329781472683 num_feat: 6/19 Reg Loss: 0.03299979493021965 Val Accuracy: 97.22222222222223\n",
      "Epoch 576/1000 Loss: 0.004559488035738468 num_feat: 6/19 Reg Loss: 0.0329960435628891 Val Accuracy: 97.22222222222223\n",
      "Epoch 577/1000 Loss: 0.0044370973482728004 num_feat: 6/19 Reg Loss: 0.032992299646139145 Val Accuracy: 97.22222222222223\n",
      "Epoch 578/1000 Loss: 0.0044549014419317245 num_feat: 6/19 Reg Loss: 0.03298855200409889 Val Accuracy: 97.22222222222223\n",
      "Epoch 579/1000 Loss: 0.004507215693593025 num_feat: 6/19 Reg Loss: 0.03298480436205864 Val Accuracy: 97.22222222222223\n",
      "Epoch 580/1000 Loss: 0.004445536993443966 num_feat: 6/19 Reg Loss: 0.032981060445308685 Val Accuracy: 97.22222222222223\n",
      "Epoch 581/1000 Loss: 0.03308561444282532 num_feat: 6/19 Reg Loss: 0.03297732025384903 Val Accuracy: 97.22222222222223\n",
      "Epoch 582/1000 Loss: 0.032681286334991455 num_feat: 6/19 Reg Loss: 0.03297363221645355 Val Accuracy: 97.22222222222223\n",
      "Epoch 583/1000 Loss: 0.03712846338748932 num_feat: 6/19 Reg Loss: 0.032969940453767776 Val Accuracy: 97.22222222222223\n",
      "Epoch 584/1000 Loss: 0.004240110050886869 num_feat: 6/19 Reg Loss: 0.0329662561416626 Val Accuracy: 94.44444444444444\n",
      "Epoch 585/1000 Loss: 0.10966111719608307 num_feat: 6/19 Reg Loss: 0.032962556928396225 Val Accuracy: 97.22222222222223\n",
      "Epoch 586/1000 Loss: 0.014936810359358788 num_feat: 6/19 Reg Loss: 0.032957132905721664 Val Accuracy: 97.22222222222223\n",
      "Epoch 587/1000 Loss: 0.07485686987638474 num_feat: 6/19 Reg Loss: 0.03295191377401352 Val Accuracy: 97.22222222222223\n",
      "Epoch 588/1000 Loss: 0.020770367234945297 num_feat: 6/19 Reg Loss: 0.0329468660056591 Val Accuracy: 94.44444444444444\n",
      "Epoch 589/1000 Loss: 0.008724342100322247 num_feat: 6/19 Reg Loss: 0.03294200077652931 Val Accuracy: 91.66666666666667\n",
      "Epoch 590/1000 Loss: 0.058484286069869995 num_feat: 6/19 Reg Loss: 0.03293727710843086 Val Accuracy: 94.44444444444444\n",
      "Epoch 591/1000 Loss: 0.013223680667579174 num_feat: 6/19 Reg Loss: 0.03293268382549286 Val Accuracy: 97.22222222222223\n",
      "Epoch 592/1000 Loss: 0.008099592290818691 num_feat: 6/19 Reg Loss: 0.032928217202425 Val Accuracy: 97.22222222222223\n",
      "Epoch 593/1000 Loss: 0.037962764501571655 num_feat: 6/19 Reg Loss: 0.0329238586127758 Val Accuracy: 97.22222222222223\n",
      "Epoch 594/1000 Loss: 0.01853783428668976 num_feat: 6/19 Reg Loss: 0.03291959688067436 Val Accuracy: 97.22222222222223\n",
      "Epoch 595/1000 Loss: 0.0050791120156645775 num_feat: 6/19 Reg Loss: 0.032915424555540085 Val Accuracy: 94.44444444444444\n",
      "Epoch 596/1000 Loss: 0.021698277443647385 num_feat: 6/19 Reg Loss: 0.03291132301092148 Val Accuracy: 94.44444444444444\n",
      "Epoch 597/1000 Loss: 0.02347131073474884 num_feat: 6/19 Reg Loss: 0.032907288521528244 Val Accuracy: 97.22222222222223\n",
      "Epoch 598/1000 Loss: 0.006383704952895641 num_feat: 6/19 Reg Loss: 0.03290332853794098 Val Accuracy: 97.22222222222223\n",
      "Epoch 599/1000 Loss: 0.00854730699211359 num_feat: 6/19 Reg Loss: 0.032899435609579086 Val Accuracy: 97.22222222222223\n",
      "Epoch 600/1000 Loss: 0.020538702607154846 num_feat: 6/19 Reg Loss: 0.03289559483528137 Val Accuracy: 97.22222222222223\n",
      "Epoch 601/1000 Loss: 0.01105050090700388 num_feat: 6/19 Reg Loss: 0.032891806215047836 Val Accuracy: 97.22222222222223\n",
      "Epoch 602/1000 Loss: 0.005030749831348658 num_feat: 6/19 Reg Loss: 0.03288720175623894 Val Accuracy: 94.44444444444444\n",
      "Epoch 603/1000 Loss: 0.012694940902292728 num_feat: 6/19 Reg Loss: 0.03288274258375168 Val Accuracy: 94.44444444444444\n",
      "Epoch 604/1000 Loss: 0.014635568484663963 num_feat: 6/19 Reg Loss: 0.03287840634584427 Val Accuracy: 97.22222222222223\n",
      "Epoch 605/1000 Loss: 0.006901134271174669 num_feat: 6/19 Reg Loss: 0.03287419304251671 Val Accuracy: 97.22222222222223\n",
      "Epoch 606/1000 Loss: 0.005605690181255341 num_feat: 6/19 Reg Loss: 0.03286995738744736 Val Accuracy: 97.22222222222223\n",
      "Epoch 607/1000 Loss: 0.01190641988068819 num_feat: 6/19 Reg Loss: 0.03286583349108696 Val Accuracy: 97.22222222222223\n",
      "Epoch 608/1000 Loss: 0.009930982254445553 num_feat: 6/19 Reg Loss: 0.032861802726984024 Val Accuracy: 97.22222222222223\n",
      "Epoch 609/1000 Loss: 0.0048207808285951614 num_feat: 6/19 Reg Loss: 0.03285786136984825 Val Accuracy: 97.22222222222223\n",
      "Epoch 610/1000 Loss: 0.02119029127061367 num_feat: 6/19 Reg Loss: 0.03285400569438934 Val Accuracy: 97.22222222222223\n",
      "Epoch 611/1000 Loss: 0.004596642218530178 num_feat: 6/19 Reg Loss: 0.03284837305545807 Val Accuracy: 97.22222222222223\n",
      "Epoch 612/1000 Loss: 0.005370598752051592 num_feat: 6/19 Reg Loss: 0.03284303471446037 Val Accuracy: 97.22222222222223\n",
      "Epoch 613/1000 Loss: 0.006593527738004923 num_feat: 6/19 Reg Loss: 0.03283796086907387 Val Accuracy: 97.22222222222223\n",
      "Epoch 614/1000 Loss: 0.0055929976515471935 num_feat: 6/19 Reg Loss: 0.03283311054110527 Val Accuracy: 97.22222222222223\n",
      "Epoch 615/1000 Loss: 0.007983422838151455 num_feat: 6/19 Reg Loss: 0.032828450202941895 Val Accuracy: 97.22222222222223\n",
      "Epoch 616/1000 Loss: 0.004694219212979078 num_feat: 6/19 Reg Loss: 0.032824013382196426 Val Accuracy: 97.22222222222223\n",
      "Epoch 617/1000 Loss: 0.004745951853692532 num_feat: 6/19 Reg Loss: 0.03281976282596588 Val Accuracy: 97.22222222222223\n",
      "Epoch 618/1000 Loss: 0.004472332075238228 num_feat: 6/19 Reg Loss: 0.032815687358379364 Val Accuracy: 97.22222222222223\n",
      "Epoch 619/1000 Loss: 0.004318039398640394 num_feat: 6/19 Reg Loss: 0.03281175345182419 Val Accuracy: 97.22222222222223\n",
      "Epoch 620/1000 Loss: 0.004436786286532879 num_feat: 6/19 Reg Loss: 0.03280793875455856 Val Accuracy: 97.22222222222223\n",
      "Epoch 621/1000 Loss: 0.004498942289501429 num_feat: 6/19 Reg Loss: 0.032804232090711594 Val Accuracy: 97.22222222222223\n",
      "Epoch 622/1000 Loss: 0.004338993225246668 num_feat: 6/19 Reg Loss: 0.032800622284412384 Val Accuracy: 97.22222222222223\n",
      "Epoch 623/1000 Loss: 0.004191153217107058 num_feat: 6/19 Reg Loss: 0.03279709443449974 Val Accuracy: 97.22222222222223\n",
      "Epoch 624/1000 Loss: 0.004228669684380293 num_feat: 6/19 Reg Loss: 0.032793641090393066 Val Accuracy: 97.22222222222223\n",
      "Epoch 625/1000 Loss: 0.004284679889678955 num_feat: 6/19 Reg Loss: 0.032790251076221466 Val Accuracy: 97.22222222222223\n",
      "Epoch 626/1000 Loss: 0.00576521223410964 num_feat: 6/19 Reg Loss: 0.03278691694140434 Val Accuracy: 97.22222222222223\n",
      "Epoch 627/1000 Loss: 0.0042700194753706455 num_feat: 6/19 Reg Loss: 0.03278377279639244 Val Accuracy: 97.22222222222223\n",
      "Epoch 628/1000 Loss: 0.004870919045060873 num_feat: 6/19 Reg Loss: 0.03278065845370293 Val Accuracy: 97.22222222222223\n",
      "Epoch 629/1000 Loss: 0.004775607027113438 num_feat: 6/19 Reg Loss: 0.0327775739133358 Val Accuracy: 97.22222222222223\n",
      "Epoch 630/1000 Loss: 0.004149227403104305 num_feat: 6/19 Reg Loss: 0.03277451917529106 Val Accuracy: 97.22222222222223\n",
      "Epoch 631/1000 Loss: 0.0038607940077781677 num_feat: 6/19 Reg Loss: 0.03277149423956871 Val Accuracy: 94.44444444444444\n",
      "Epoch 632/1000 Loss: 0.004580437205731869 num_feat: 6/19 Reg Loss: 0.03276848420500755 Val Accuracy: 97.22222222222223\n",
      "Epoch 633/1000 Loss: 0.0047900923527777195 num_feat: 6/19 Reg Loss: 0.032765503972768784 Val Accuracy: 97.22222222222223\n",
      "Epoch 634/1000 Loss: 0.004197676200419664 num_feat: 6/19 Reg Loss: 0.03276253864169121 Val Accuracy: 97.22222222222223\n",
      "Epoch 635/1000 Loss: 0.003841792233288288 num_feat: 6/19 Reg Loss: 0.03275960311293602 Val Accuracy: 97.22222222222223\n",
      "Epoch 636/1000 Loss: 0.004124435130506754 num_feat: 6/19 Reg Loss: 0.03275667503476143 Val Accuracy: 97.22222222222223\n",
      "Epoch 637/1000 Loss: 0.0043687960132956505 num_feat: 6/19 Reg Loss: 0.03275376185774803 Val Accuracy: 97.22222222222223\n",
      "Epoch 638/1000 Loss: 0.004099756479263306 num_feat: 6/19 Reg Loss: 0.03275087848305702 Val Accuracy: 97.22222222222223\n",
      "Epoch 639/1000 Loss: 0.003760599298402667 num_feat: 6/19 Reg Loss: 0.032748010009527206 Val Accuracy: 97.22222222222223\n",
      "Epoch 640/1000 Loss: 0.0037862027529627085 num_feat: 6/19 Reg Loss: 0.03274514898657799 Val Accuracy: 97.22222222222223\n",
      "Epoch 641/1000 Loss: 0.004075535573065281 num_feat: 6/19 Reg Loss: 0.03274231404066086 Val Accuracy: 97.22222222222223\n",
      "Epoch 642/1000 Loss: 0.004012720659375191 num_feat: 6/19 Reg Loss: 0.032739486545324326 Val Accuracy: 97.22222222222223\n",
      "Epoch 643/1000 Loss: 0.0037182806991040707 num_feat: 6/19 Reg Loss: 0.03273667022585869 Val Accuracy: 97.22222222222223\n",
      "Epoch 644/1000 Loss: 0.0036429769825190306 num_feat: 6/19 Reg Loss: 0.03273387253284454 Val Accuracy: 97.22222222222223\n",
      "Epoch 645/1000 Loss: 0.003794032847508788 num_feat: 6/19 Reg Loss: 0.032731086015701294 Val Accuracy: 97.22222222222223\n",
      "Epoch 646/1000 Loss: 0.0038378522731363773 num_feat: 6/19 Reg Loss: 0.03272831439971924 Val Accuracy: 97.22222222222223\n",
      "Epoch 647/1000 Loss: 0.003672786755487323 num_feat: 6/19 Reg Loss: 0.03272555395960808 Val Accuracy: 97.22222222222223\n",
      "Epoch 648/1000 Loss: 0.003540222765877843 num_feat: 6/19 Reg Loss: 0.03272281214594841 Val Accuracy: 97.22222222222223\n",
      "Epoch 649/1000 Loss: 0.003587082028388977 num_feat: 6/19 Reg Loss: 0.03272007778286934 Val Accuracy: 97.22222222222223\n",
      "Epoch 650/1000 Loss: 0.0036615985445678234 num_feat: 6/19 Reg Loss: 0.03271735832095146 Val Accuracy: 97.22222222222223\n",
      "Epoch 651/1000 Loss: 0.0035940820816904306 num_feat: 6/19 Reg Loss: 0.03271465003490448 Val Accuracy: 97.22222222222223\n",
      "Epoch 652/1000 Loss: 0.0034754369407892227 num_feat: 6/19 Reg Loss: 0.03271195665001869 Val Accuracy: 97.22222222222223\n",
      "Epoch 653/1000 Loss: 0.0034571525175124407 num_feat: 6/19 Reg Loss: 0.0327092707157135 Val Accuracy: 97.22222222222223\n",
      "Epoch 654/1000 Loss: 0.0035056499764323235 num_feat: 6/19 Reg Loss: 0.0327066034078598 Val Accuracy: 97.22222222222223\n",
      "Epoch 655/1000 Loss: 0.0034995791502296925 num_feat: 6/19 Reg Loss: 0.0327039435505867 Val Accuracy: 97.22222222222223\n",
      "Epoch 656/1000 Loss: 0.003423573449254036 num_feat: 6/19 Reg Loss: 0.0327012874186039 Val Accuracy: 97.22222222222223\n",
      "Epoch 657/1000 Loss: 0.003370625665411353 num_feat: 6/19 Reg Loss: 0.032698653638362885 Val Accuracy: 97.22222222222223\n",
      "Epoch 658/1000 Loss: 0.003382367780432105 num_feat: 6/19 Reg Loss: 0.03269602730870247 Val Accuracy: 97.22222222222223\n",
      "Epoch 659/1000 Loss: 0.003397687105461955 num_feat: 6/19 Reg Loss: 0.03269341215491295 Val Accuracy: 97.22222222222223\n",
      "Epoch 660/1000 Loss: 0.0033600390888750553 num_feat: 6/19 Reg Loss: 0.032690808176994324 Val Accuracy: 97.22222222222223\n",
      "Epoch 661/1000 Loss: 0.003307900857180357 num_feat: 6/19 Reg Loss: 0.032688215374946594 Val Accuracy: 97.22222222222223\n",
      "Epoch 662/1000 Loss: 0.0032937764190137386 num_feat: 6/19 Reg Loss: 0.03268563747406006 Val Accuracy: 97.22222222222223\n",
      "Epoch 663/1000 Loss: 0.003303161356598139 num_feat: 6/19 Reg Loss: 0.03268307074904442 Val Accuracy: 97.22222222222223\n",
      "Epoch 664/1000 Loss: 0.003291292814537883 num_feat: 6/19 Reg Loss: 0.03268050774931908 Val Accuracy: 97.22222222222223\n",
      "Epoch 665/1000 Loss: 0.0032537728548049927 num_feat: 6/19 Reg Loss: 0.03267795965075493 Val Accuracy: 97.22222222222223\n",
      "Epoch 666/1000 Loss: 0.003225146559998393 num_feat: 6/19 Reg Loss: 0.032675422728061676 Val Accuracy: 97.22222222222223\n",
      "Epoch 667/1000 Loss: 0.0032208000775426626 num_feat: 6/19 Reg Loss: 0.03267289325594902 Val Accuracy: 97.22222222222223\n",
      "Epoch 668/1000 Loss: 0.0032184177543967962 num_feat: 6/19 Reg Loss: 0.03267037868499756 Val Accuracy: 97.22222222222223\n",
      "Epoch 669/1000 Loss: 0.0031977053731679916 num_feat: 6/19 Reg Loss: 0.032667871564626694 Val Accuracy: 97.22222222222223\n",
      "Epoch 670/1000 Loss: 0.003169297706335783 num_feat: 6/19 Reg Loss: 0.03266538307070732 Val Accuracy: 97.22222222222223\n",
      "Epoch 671/1000 Loss: 0.0031533364672213793 num_feat: 6/19 Reg Loss: 0.03266289457678795 Val Accuracy: 97.22222222222223\n",
      "Epoch 672/1000 Loss: 0.0031485469080507755 num_feat: 6/19 Reg Loss: 0.03266042098402977 Val Accuracy: 97.22222222222223\n",
      "Epoch 673/1000 Loss: 0.003138135187327862 num_feat: 6/19 Reg Loss: 0.03265795484185219 Val Accuracy: 97.22222222222223\n",
      "Epoch 674/1000 Loss: 0.003117077751085162 num_feat: 6/19 Reg Loss: 0.0326555036008358 Val Accuracy: 97.22222222222223\n",
      "Epoch 675/1000 Loss: 0.0030969041399657726 num_feat: 6/19 Reg Loss: 0.03265305608510971 Val Accuracy: 97.22222222222223\n",
      "Epoch 676/1000 Loss: 0.003086099633947015 num_feat: 6/19 Reg Loss: 0.032650623470544815 Val Accuracy: 97.22222222222223\n",
      "Epoch 677/1000 Loss: 0.0030782620888203382 num_feat: 6/19 Reg Loss: 0.032648198306560516 Val Accuracy: 97.22222222222223\n",
      "Epoch 678/1000 Loss: 0.003064383752644062 num_feat: 6/19 Reg Loss: 0.03264578431844711 Val Accuracy: 97.22222222222223\n",
      "Epoch 679/1000 Loss: 0.0030460567213594913 num_feat: 6/19 Reg Loss: 0.03264337778091431 Val Accuracy: 97.22222222222223\n",
      "Epoch 680/1000 Loss: 0.003031183499842882 num_feat: 6/19 Reg Loss: 0.032640986144542694 Val Accuracy: 97.22222222222223\n",
      "Epoch 681/1000 Loss: 0.0030214881990104914 num_feat: 6/19 Reg Loss: 0.03263860195875168 Val Accuracy: 97.22222222222223\n",
      "Epoch 682/1000 Loss: 0.0030112199019640684 num_feat: 6/19 Reg Loss: 0.03263622149825096 Val Accuracy: 97.22222222222223\n",
      "Epoch 683/1000 Loss: 0.0029968093149363995 num_feat: 6/19 Reg Loss: 0.032633859664201736 Val Accuracy: 97.22222222222223\n",
      "Epoch 684/1000 Loss: 0.0029814611189067364 num_feat: 6/19 Reg Loss: 0.03263149783015251 Val Accuracy: 97.22222222222223\n",
      "Epoch 685/1000 Loss: 0.0029692973475903273 num_feat: 6/19 Reg Loss: 0.03262915089726448 Val Accuracy: 97.22222222222223\n",
      "Epoch 686/1000 Loss: 0.004320385865867138 num_feat: 6/19 Reg Loss: 0.032626811414957047 Val Accuracy: 97.22222222222223\n",
      "Epoch 687/1000 Loss: 0.003211705479770899 num_feat: 6/19 Reg Loss: 0.03262414038181305 Val Accuracy: 94.44444444444444\n",
      "Epoch 688/1000 Loss: 0.0035958364605903625 num_feat: 6/19 Reg Loss: 0.032621514052152634 Val Accuracy: 97.22222222222223\n",
      "Epoch 689/1000 Loss: 0.0036439625546336174 num_feat: 6/19 Reg Loss: 0.0326189249753952 Val Accuracy: 97.22222222222223\n",
      "Epoch 690/1000 Loss: 0.0029494878835976124 num_feat: 6/19 Reg Loss: 0.032616328448057175 Val Accuracy: 97.22222222222223\n",
      "Epoch 691/1000 Loss: 0.0030098308343440294 num_feat: 6/19 Reg Loss: 0.03261376544833183 Val Accuracy: 97.22222222222223\n",
      "Epoch 692/1000 Loss: 0.0033539303112775087 num_feat: 6/19 Reg Loss: 0.032611239701509476 Val Accuracy: 97.22222222222223\n",
      "Epoch 693/1000 Loss: 0.003348918631672859 num_feat: 6/19 Reg Loss: 0.0326087586581707 Val Accuracy: 97.22222222222223\n",
      "Epoch 694/1000 Loss: 0.0030085972975939512 num_feat: 6/19 Reg Loss: 0.03260629624128342 Val Accuracy: 97.22222222222223\n",
      "Epoch 695/1000 Loss: 0.002853680867701769 num_feat: 6/19 Reg Loss: 0.03260387107729912 Val Accuracy: 97.22222222222223\n",
      "Epoch 696/1000 Loss: 0.0030140234157443047 num_feat: 6/19 Reg Loss: 0.03260147199034691 Val Accuracy: 94.44444444444444\n",
      "Epoch 697/1000 Loss: 0.004000821150839329 num_feat: 6/19 Reg Loss: 0.03259910270571709 Val Accuracy: 97.22222222222223\n",
      "Epoch 698/1000 Loss: 0.002847669878974557 num_feat: 6/19 Reg Loss: 0.03259638696908951 Val Accuracy: 97.22222222222223\n",
      "Epoch 699/1000 Loss: 0.0028701708652079105 num_feat: 6/19 Reg Loss: 0.032593727111816406 Val Accuracy: 97.22222222222223\n",
      "Epoch 700/1000 Loss: 0.003084603464230895 num_feat: 6/19 Reg Loss: 0.03259113058447838 Val Accuracy: 97.22222222222223\n",
      "Epoch 701/1000 Loss: 0.003096175380051136 num_feat: 6/19 Reg Loss: 0.03258857876062393 Val Accuracy: 97.22222222222223\n",
      "Epoch 702/1000 Loss: 0.0028415131382644176 num_feat: 6/19 Reg Loss: 0.032586075365543365 Val Accuracy: 97.22222222222223\n",
      "Epoch 703/1000 Loss: 0.0027629833202809095 num_feat: 6/19 Reg Loss: 0.03258365020155907 Val Accuracy: 97.22222222222223\n",
      "Epoch 704/1000 Loss: 0.002821403555572033 num_feat: 6/19 Reg Loss: 0.03258126229047775 Val Accuracy: 97.22222222222223\n",
      "Epoch 705/1000 Loss: 0.00283494358882308 num_feat: 6/19 Reg Loss: 0.032578907907009125 Val Accuracy: 97.22222222222223\n",
      "Epoch 706/1000 Loss: 0.002907924819737673 num_feat: 6/19 Reg Loss: 0.03257664665579796 Val Accuracy: 97.22222222222223\n",
      "Epoch 707/1000 Loss: 0.002782875671982765 num_feat: 6/19 Reg Loss: 0.03257439658045769 Val Accuracy: 97.22222222222223\n",
      "Epoch 708/1000 Loss: 0.002710192697122693 num_feat: 6/19 Reg Loss: 0.032572176307439804 Val Accuracy: 97.22222222222223\n",
      "Epoch 709/1000 Loss: 0.0027529874350875616 num_feat: 6/19 Reg Loss: 0.03256997466087341 Val Accuracy: 97.22222222222223\n",
      "Epoch 710/1000 Loss: 0.0028069179970771074 num_feat: 6/19 Reg Loss: 0.03256779536604881 Val Accuracy: 97.22222222222223\n",
      "Epoch 711/1000 Loss: 0.0027696487959474325 num_feat: 6/19 Reg Loss: 0.03256562352180481 Val Accuracy: 97.22222222222223\n",
      "Epoch 712/1000 Loss: 0.002688803244382143 num_feat: 6/19 Reg Loss: 0.0325634740293026 Val Accuracy: 94.44444444444444\n",
      "Epoch 713/1000 Loss: 0.002663575578480959 num_feat: 6/19 Reg Loss: 0.03256132826209068 Val Accuracy: 97.22222222222223\n",
      "Epoch 714/1000 Loss: 0.0026964526623487473 num_feat: 6/19 Reg Loss: 0.03255920484662056 Val Accuracy: 97.22222222222223\n",
      "Epoch 715/1000 Loss: 0.0027122357860207558 num_feat: 6/19 Reg Loss: 0.032557081431150436 Val Accuracy: 97.22222222222223\n",
      "Epoch 716/1000 Loss: 0.0026736713480204344 num_feat: 6/19 Reg Loss: 0.0325549878180027 Val Accuracy: 97.22222222222223\n",
      "Epoch 717/1000 Loss: 0.0026259657461196184 num_feat: 6/19 Reg Loss: 0.032552894204854965 Val Accuracy: 97.22222222222223\n",
      "Epoch 718/1000 Loss: 0.002618101891130209 num_feat: 6/19 Reg Loss: 0.03255081549286842 Val Accuracy: 97.22222222222223\n",
      "Epoch 719/1000 Loss: 0.00263606128282845 num_feat: 6/19 Reg Loss: 0.032548751682043076 Val Accuracy: 97.22222222222223\n",
      "Epoch 720/1000 Loss: 0.0026345516089349985 num_feat: 6/19 Reg Loss: 0.03254668787121773 Val Accuracy: 97.22222222222223\n",
      "Epoch 721/1000 Loss: 0.002603544620797038 num_feat: 6/19 Reg Loss: 0.03254464268684387 Val Accuracy: 97.22222222222223\n",
      "Epoch 722/1000 Loss: 0.0025750573258847 num_feat: 6/19 Reg Loss: 0.03254260867834091 Val Accuracy: 97.22222222222223\n",
      "Epoch 723/1000 Loss: 0.0025228194426745176 num_feat: 6/19 Reg Loss: 0.03254057466983795 Val Accuracy: 97.22222222222223\n",
      "Epoch 724/1000 Loss: 0.0026000840589404106 num_feat: 6/19 Reg Loss: 0.03253857046365738 Val Accuracy: 97.22222222222223\n",
      "Epoch 725/1000 Loss: 0.002605367684736848 num_feat: 6/19 Reg Loss: 0.032536569982767105 Val Accuracy: 97.22222222222223\n",
      "Epoch 726/1000 Loss: 0.0025683168787509203 num_feat: 6/19 Reg Loss: 0.03253457695245743 Val Accuracy: 97.22222222222223\n",
      "Epoch 727/1000 Loss: 0.0025308672338724136 num_feat: 6/19 Reg Loss: 0.03253259137272835 Val Accuracy: 97.22222222222223\n",
      "Epoch 728/1000 Loss: 0.0025284336879849434 num_feat: 6/19 Reg Loss: 0.03253061696887016 Val Accuracy: 97.22222222222223\n",
      "Epoch 729/1000 Loss: 0.002543293172493577 num_feat: 6/19 Reg Loss: 0.03252864629030228 Val Accuracy: 97.22222222222223\n",
      "Epoch 730/1000 Loss: 0.002537861932069063 num_feat: 6/19 Reg Loss: 0.03252668306231499 Val Accuracy: 97.22222222222223\n",
      "Epoch 731/1000 Loss: 0.0025089848786592484 num_feat: 6/19 Reg Loss: 0.03252473101019859 Val Accuracy: 97.22222222222223\n",
      "Epoch 732/1000 Loss: 0.0024854904040694237 num_feat: 6/19 Reg Loss: 0.032522790133953094 Val Accuracy: 97.22222222222223\n",
      "Epoch 733/1000 Loss: 0.0024838177487254143 num_feat: 6/19 Reg Loss: 0.0325208455324173 Val Accuracy: 97.22222222222223\n",
      "Epoch 734/1000 Loss: 0.002488737925887108 num_feat: 6/19 Reg Loss: 0.032518912106752396 Val Accuracy: 97.22222222222223\n",
      "Epoch 735/1000 Loss: 0.002479465678334236 num_feat: 6/19 Reg Loss: 0.03251699358224869 Val Accuracy: 97.22222222222223\n",
      "Epoch 736/1000 Loss: 0.002458244562149048 num_feat: 6/19 Reg Loss: 0.03251507133245468 Val Accuracy: 97.22222222222223\n",
      "Epoch 737/1000 Loss: 0.002442566677927971 num_feat: 6/19 Reg Loss: 0.03251316398382187 Val Accuracy: 97.22222222222223\n",
      "Epoch 738/1000 Loss: 0.0024413582868874073 num_feat: 6/19 Reg Loss: 0.032511260360479355 Val Accuracy: 97.22222222222223\n",
      "Epoch 739/1000 Loss: 0.002442004857584834 num_feat: 6/19 Reg Loss: 0.03250936046242714 Val Accuracy: 97.22222222222223\n",
      "Epoch 740/1000 Loss: 0.0024315528571605682 num_feat: 6/19 Reg Loss: 0.03250746801495552 Val Accuracy: 97.22222222222223\n",
      "Epoch 741/1000 Loss: 0.002413972979411483 num_feat: 6/19 Reg Loss: 0.0325055867433548 Val Accuracy: 97.22222222222223\n",
      "Epoch 742/1000 Loss: 0.002401968464255333 num_feat: 6/19 Reg Loss: 0.03250370919704437 Val Accuracy: 97.22222222222223\n",
      "Epoch 743/1000 Loss: 0.002398656215518713 num_feat: 6/19 Reg Loss: 0.03250184282660484 Val Accuracy: 97.22222222222223\n",
      "Epoch 744/1000 Loss: 0.0023954110220074654 num_feat: 6/19 Reg Loss: 0.03249998018145561 Val Accuracy: 97.22222222222223\n",
      "Epoch 745/1000 Loss: 0.0023853282909840345 num_feat: 6/19 Reg Loss: 0.03249812871217728 Val Accuracy: 97.22222222222223\n",
      "Epoch 746/1000 Loss: 0.002373805735260248 num_feat: 6/19 Reg Loss: 0.03249628096818924 Val Accuracy: 97.22222222222223\n",
      "Epoch 747/1000 Loss: 0.002756241476163268 num_feat: 6/19 Reg Loss: 0.032494429498910904 Val Accuracy: 97.22222222222223\n",
      "Epoch 748/1000 Loss: 0.0023983377031981945 num_feat: 6/19 Reg Loss: 0.0324925072491169 Val Accuracy: 97.22222222222223\n",
      "Epoch 749/1000 Loss: 0.0024620622862130404 num_feat: 6/19 Reg Loss: 0.032490603625774384 Val Accuracy: 97.22222222222223\n",
      "Epoch 750/1000 Loss: 0.0024446756578981876 num_feat: 6/19 Reg Loss: 0.03248871490359306 Val Accuracy: 97.22222222222223\n",
      "Epoch 751/1000 Loss: 0.0023662191815674305 num_feat: 6/19 Reg Loss: 0.03248683363199234 Val Accuracy: 97.22222222222223\n",
      "Epoch 752/1000 Loss: 0.0023237839341163635 num_feat: 6/19 Reg Loss: 0.03248497098684311 Val Accuracy: 97.22222222222223\n",
      "Epoch 753/1000 Loss: 0.0023485529236495495 num_feat: 6/19 Reg Loss: 0.032483115792274475 Val Accuracy: 97.22222222222223\n",
      "Epoch 754/1000 Loss: 0.002379696350544691 num_feat: 6/19 Reg Loss: 0.03248127922415733 Val Accuracy: 97.22222222222223\n",
      "Epoch 755/1000 Loss: 0.0023591923527419567 num_feat: 6/19 Reg Loss: 0.03247944638133049 Val Accuracy: 97.22222222222223\n",
      "Epoch 756/1000 Loss: 0.0023090343456715345 num_feat: 6/19 Reg Loss: 0.03247763216495514 Val Accuracy: 97.22222222222223\n",
      "Epoch 757/1000 Loss: 0.0027511727530509233 num_feat: 6/19 Reg Loss: 0.03247582167387009 Val Accuracy: 97.22222222222223\n",
      "Epoch 758/1000 Loss: 0.002311257179826498 num_feat: 6/19 Reg Loss: 0.03247378394007683 Val Accuracy: 97.22222222222223\n",
      "Epoch 759/1000 Loss: 0.002359547885134816 num_feat: 6/19 Reg Loss: 0.032471783459186554 Val Accuracy: 97.22222222222223\n",
      "Epoch 760/1000 Loss: 0.002346588298678398 num_feat: 6/19 Reg Loss: 0.032469816505908966 Val Accuracy: 97.22222222222223\n",
      "Epoch 761/1000 Loss: 0.0022848614025861025 num_feat: 6/19 Reg Loss: 0.03246787562966347 Val Accuracy: 97.22222222222223\n",
      "Epoch 762/1000 Loss: 0.002277613617479801 num_feat: 6/19 Reg Loss: 0.03246596083045006 Val Accuracy: 97.22222222222223\n",
      "Epoch 763/1000 Loss: 0.0022825771011412144 num_feat: 6/19 Reg Loss: 0.03246406838297844 Val Accuracy: 97.22222222222223\n",
      "Epoch 764/1000 Loss: 0.002329427981749177 num_feat: 6/19 Reg Loss: 0.03246219456195831 Val Accuracy: 97.22222222222223\n",
      "Epoch 765/1000 Loss: 0.002309863455593586 num_feat: 6/19 Reg Loss: 0.03246034309267998 Val Accuracy: 97.22222222222223\n",
      "Epoch 766/1000 Loss: 0.002246106043457985 num_feat: 6/19 Reg Loss: 0.032458510249853134 Val Accuracy: 97.22222222222223\n",
      "Epoch 767/1000 Loss: 0.0022132007870823145 num_feat: 6/19 Reg Loss: 0.03245669603347778 Val Accuracy: 97.22222222222223\n",
      "Epoch 768/1000 Loss: 0.002232260536402464 num_feat: 6/19 Reg Loss: 0.032454896718263626 Val Accuracy: 97.22222222222223\n",
      "Epoch 769/1000 Loss: 0.0022556264884769917 num_feat: 6/19 Reg Loss: 0.03245312348008156 Val Accuracy: 97.22222222222223\n",
      "Epoch 770/1000 Loss: 0.002232344588264823 num_feat: 6/19 Reg Loss: 0.03245135769248009 Val Accuracy: 97.22222222222223\n",
      "Epoch 771/1000 Loss: 0.0021990989334881306 num_feat: 6/19 Reg Loss: 0.03244961053133011 Val Accuracy: 97.22222222222223\n",
      "Epoch 772/1000 Loss: 0.00217836769297719 num_feat: 6/19 Reg Loss: 0.03244787082076073 Val Accuracy: 97.22222222222223\n",
      "Epoch 773/1000 Loss: 0.0021895631216466427 num_feat: 6/19 Reg Loss: 0.03244614973664284 Val Accuracy: 97.22222222222223\n",
      "Epoch 774/1000 Loss: 0.002189673949033022 num_feat: 6/19 Reg Loss: 0.03244442865252495 Val Accuracy: 97.22222222222223\n",
      "Epoch 775/1000 Loss: 0.0021753727924078703 num_feat: 6/19 Reg Loss: 0.03244272992014885 Val Accuracy: 97.22222222222223\n",
      "Epoch 776/1000 Loss: 0.0023617299739271402 num_feat: 6/19 Reg Loss: 0.03244103491306305 Val Accuracy: 97.22222222222223\n",
      "Epoch 777/1000 Loss: 0.0021828734315931797 num_feat: 6/19 Reg Loss: 0.0324392132461071 Val Accuracy: 97.22222222222223\n",
      "Epoch 778/1000 Loss: 0.0023402797523885965 num_feat: 6/19 Reg Loss: 0.03243745118379593 Val Accuracy: 97.22222222222223\n",
      "Epoch 779/1000 Loss: 0.0023402089718729258 num_feat: 6/19 Reg Loss: 0.03243570774793625 Val Accuracy: 97.22222222222223\n",
      "Epoch 780/1000 Loss: 0.002207854762673378 num_feat: 6/19 Reg Loss: 0.032433975487947464 Val Accuracy: 97.22222222222223\n",
      "Epoch 781/1000 Loss: 0.0021190177649259567 num_feat: 6/19 Reg Loss: 0.03243226930499077 Val Accuracy: 97.22222222222223\n",
      "Epoch 782/1000 Loss: 0.0021539388690143824 num_feat: 6/19 Reg Loss: 0.03243057057261467 Val Accuracy: 94.44444444444444\n",
      "Epoch 783/1000 Loss: 0.002226221840828657 num_feat: 6/19 Reg Loss: 0.032428890466690063 Val Accuracy: 94.44444444444444\n",
      "Epoch 784/1000 Loss: 0.002219361485913396 num_feat: 6/19 Reg Loss: 0.032427217811346054 Val Accuracy: 97.22222222222223\n",
      "Epoch 785/1000 Loss: 0.0021400784607976675 num_feat: 6/19 Reg Loss: 0.03242556005716324 Val Accuracy: 97.22222222222223\n",
      "Epoch 786/1000 Loss: 0.0020862985402345657 num_feat: 6/19 Reg Loss: 0.03242390975356102 Val Accuracy: 97.22222222222223\n",
      "Epoch 787/1000 Loss: 0.0021036681719124317 num_feat: 6/19 Reg Loss: 0.032422274351119995 Val Accuracy: 97.22222222222223\n",
      "Epoch 788/1000 Loss: 0.002221029018983245 num_feat: 6/19 Reg Loss: 0.032420653849840164 Val Accuracy: 97.22222222222223\n",
      "Epoch 789/1000 Loss: 0.002284945920109749 num_feat: 6/19 Reg Loss: 0.03241898491978645 Val Accuracy: 97.22222222222223\n",
      "Epoch 790/1000 Loss: 0.0022837729193270206 num_feat: 6/19 Reg Loss: 0.03241733834147453 Val Accuracy: 97.22222222222223\n",
      "Epoch 791/1000 Loss: 0.0021435092203319073 num_feat: 6/19 Reg Loss: 0.03241569548845291 Val Accuracy: 97.22222222222223\n",
      "Epoch 792/1000 Loss: 0.0020491075702011585 num_feat: 6/19 Reg Loss: 0.03241407126188278 Val Accuracy: 97.22222222222223\n",
      "Epoch 793/1000 Loss: 0.0020836431067436934 num_feat: 6/19 Reg Loss: 0.03241245821118355 Val Accuracy: 94.44444444444444\n",
      "Epoch 794/1000 Loss: 0.0021318094804883003 num_feat: 6/19 Reg Loss: 0.03241085261106491 Val Accuracy: 94.44444444444444\n",
      "Epoch 795/1000 Loss: 0.002113112946972251 num_feat: 6/19 Reg Loss: 0.03240925818681717 Val Accuracy: 97.22222222222223\n",
      "Epoch 796/1000 Loss: 0.002051008865237236 num_feat: 6/19 Reg Loss: 0.032407671213150024 Val Accuracy: 97.22222222222223\n",
      "Epoch 797/1000 Loss: 0.0020170605275779963 num_feat: 6/19 Reg Loss: 0.03240608796477318 Val Accuracy: 97.22222222222223\n",
      "Epoch 798/1000 Loss: 0.002034015953540802 num_feat: 6/19 Reg Loss: 0.032404523342847824 Val Accuracy: 97.22222222222223\n",
      "Epoch 799/1000 Loss: 0.002149406587705016 num_feat: 6/19 Reg Loss: 0.03240296617150307 Val Accuracy: 97.22222222222223\n",
      "Epoch 800/1000 Loss: 0.0020105810835957527 num_feat: 6/19 Reg Loss: 0.032401394098997116 Val Accuracy: 97.22222222222223\n",
      "Epoch 801/1000 Loss: 0.002001902787014842 num_feat: 6/19 Reg Loss: 0.03239983320236206 Val Accuracy: 97.22222222222223\n",
      "Epoch 802/1000 Loss: 0.0019875536672770977 num_feat: 6/19 Reg Loss: 0.0323982834815979 Val Accuracy: 97.22222222222223\n",
      "Epoch 803/1000 Loss: 0.001993735320866108 num_feat: 6/19 Reg Loss: 0.032396744936704636 Val Accuracy: 97.22222222222223\n",
      "Epoch 804/1000 Loss: 0.002000203123316169 num_feat: 6/19 Reg Loss: 0.032395217567682266 Val Accuracy: 97.22222222222223\n",
      "Epoch 805/1000 Loss: 0.001990333665162325 num_feat: 6/19 Reg Loss: 0.0323936901986599 Val Accuracy: 97.22222222222223\n",
      "Epoch 806/1000 Loss: 0.0019706517923623323 num_feat: 6/19 Reg Loss: 0.03239217773079872 Val Accuracy: 97.22222222222223\n",
      "Epoch 807/1000 Loss: 0.001958320615813136 num_feat: 6/19 Reg Loss: 0.03239067271351814 Val Accuracy: 94.44444444444444\n",
      "Epoch 808/1000 Loss: 0.001958500826731324 num_feat: 6/19 Reg Loss: 0.03238917514681816 Val Accuracy: 97.22222222222223\n",
      "Epoch 809/1000 Loss: 0.0019532775040715933 num_feat: 6/19 Reg Loss: 0.03238767758011818 Val Accuracy: 97.22222222222223\n",
      "Epoch 810/1000 Loss: 0.0019553638994693756 num_feat: 6/19 Reg Loss: 0.03238620236515999 Val Accuracy: 97.22222222222223\n",
      "Epoch 811/1000 Loss: 0.0019419961608946323 num_feat: 6/19 Reg Loss: 0.032384730875492096 Val Accuracy: 97.22222222222223\n",
      "Epoch 812/1000 Loss: 0.0019806574564427137 num_feat: 6/19 Reg Loss: 0.032383259385824203 Val Accuracy: 97.22222222222223\n",
      "Epoch 813/1000 Loss: 0.0019340800354257226 num_feat: 6/19 Reg Loss: 0.03238178417086601 Val Accuracy: 94.44444444444444\n",
      "Epoch 814/1000 Loss: 0.002172315726056695 num_feat: 6/19 Reg Loss: 0.03238031268119812 Val Accuracy: 97.22222222222223\n",
      "Epoch 815/1000 Loss: 0.0019142220262438059 num_feat: 6/19 Reg Loss: 0.032378777861595154 Val Accuracy: 97.22222222222223\n",
      "Epoch 816/1000 Loss: 0.0019438595045357943 num_feat: 6/19 Reg Loss: 0.03237725794315338 Val Accuracy: 97.22222222222223\n",
      "Epoch 817/1000 Loss: 0.0019764944445341825 num_feat: 6/19 Reg Loss: 0.032375745475292206 Val Accuracy: 97.22222222222223\n",
      "Epoch 818/1000 Loss: 0.0019599462393671274 num_feat: 6/19 Reg Loss: 0.032374247908592224 Val Accuracy: 97.22222222222223\n",
      "Epoch 819/1000 Loss: 0.001912340521812439 num_feat: 6/19 Reg Loss: 0.032372768968343735 Val Accuracy: 97.22222222222223\n",
      "Epoch 820/1000 Loss: 0.00188656453974545 num_feat: 6/19 Reg Loss: 0.032371293753385544 Val Accuracy: 97.22222222222223\n",
      "Epoch 821/1000 Loss: 0.0018977641593664885 num_feat: 6/19 Reg Loss: 0.03236982971429825 Val Accuracy: 94.44444444444444\n",
      "Epoch 822/1000 Loss: 0.0019162754761055112 num_feat: 6/19 Reg Loss: 0.03236837312579155 Val Accuracy: 94.44444444444444\n",
      "Epoch 823/1000 Loss: 0.001909559709019959 num_feat: 6/19 Reg Loss: 0.032366931438446045 Val Accuracy: 97.22222222222223\n",
      "Epoch 824/1000 Loss: 0.0018814587965607643 num_feat: 6/19 Reg Loss: 0.03236549347639084 Val Accuracy: 97.22222222222223\n",
      "Epoch 825/1000 Loss: 0.001860606367699802 num_feat: 6/19 Reg Loss: 0.03236406296491623 Val Accuracy: 97.22222222222223\n",
      "Epoch 826/1000 Loss: 0.0018615559674799442 num_feat: 6/19 Reg Loss: 0.032362643629312515 Val Accuracy: 97.22222222222223\n",
      "Epoch 827/1000 Loss: 0.0018710398580878973 num_feat: 6/19 Reg Loss: 0.0323612317442894 Val Accuracy: 97.22222222222223\n",
      "Epoch 828/1000 Loss: 0.0018682987429201603 num_feat: 6/19 Reg Loss: 0.03235981985926628 Val Accuracy: 97.22222222222223\n",
      "Epoch 829/1000 Loss: 0.0018057766137644649 num_feat: 6/19 Reg Loss: 0.03235841915011406 Val Accuracy: 97.22222222222223\n",
      "Epoch 830/1000 Loss: 0.001842448953539133 num_feat: 6/19 Reg Loss: 0.03235703334212303 Val Accuracy: 97.22222222222223\n",
      "Epoch 831/1000 Loss: 0.001847986364737153 num_feat: 6/19 Reg Loss: 0.032355647534132004 Val Accuracy: 97.22222222222223\n",
      "Epoch 832/1000 Loss: 0.0018257214687764645 num_feat: 6/19 Reg Loss: 0.032354261726140976 Val Accuracy: 97.22222222222223\n",
      "Epoch 833/1000 Loss: 0.0018287169514223933 num_feat: 6/19 Reg Loss: 0.032352887094020844 Val Accuracy: 97.22222222222223\n",
      "Epoch 834/1000 Loss: 0.0018283834215253592 num_feat: 6/19 Reg Loss: 0.03235151618719101 Val Accuracy: 97.22222222222223\n",
      "Epoch 835/1000 Loss: 0.0018190579721704125 num_feat: 6/19 Reg Loss: 0.032350149005651474 Val Accuracy: 97.22222222222223\n",
      "Epoch 836/1000 Loss: 0.0018067315686494112 num_feat: 6/19 Reg Loss: 0.032348792999982834 Val Accuracy: 97.22222222222223\n",
      "Epoch 837/1000 Loss: 0.0017997086979448795 num_feat: 6/19 Reg Loss: 0.032347436994314194 Val Accuracy: 97.22222222222223\n",
      "Epoch 838/1000 Loss: 0.0017987455939874053 num_feat: 6/19 Reg Loss: 0.032346080988645554 Val Accuracy: 97.22222222222223\n",
      "Epoch 839/1000 Loss: 0.0017975806258618832 num_feat: 6/19 Reg Loss: 0.03234473988413811 Val Accuracy: 97.22222222222223\n",
      "Epoch 840/1000 Loss: 0.0017915336648002267 num_feat: 6/19 Reg Loss: 0.03234339877963066 Val Accuracy: 97.22222222222223\n",
      "Epoch 841/1000 Loss: 0.0017824256792664528 num_feat: 6/19 Reg Loss: 0.03234206885099411 Val Accuracy: 97.22222222222223\n",
      "Epoch 842/1000 Loss: 0.0017753432039171457 num_feat: 6/19 Reg Loss: 0.03234073147177696 Val Accuracy: 97.22222222222223\n",
      "Epoch 843/1000 Loss: 0.0017720776377245784 num_feat: 6/19 Reg Loss: 0.032339416444301605 Val Accuracy: 97.22222222222223\n",
      "Epoch 844/1000 Loss: 0.0017698946176096797 num_feat: 6/19 Reg Loss: 0.03233809396624565 Val Accuracy: 97.22222222222223\n",
      "Epoch 845/1000 Loss: 0.0017653421964496374 num_feat: 6/19 Reg Loss: 0.03233678266406059 Val Accuracy: 97.22222222222223\n",
      "Epoch 846/1000 Loss: 0.0017584154848009348 num_feat: 6/19 Reg Loss: 0.03233546391129494 Val Accuracy: 97.22222222222223\n",
      "Epoch 847/1000 Loss: 0.0017518403474241495 num_feat: 6/19 Reg Loss: 0.032334160059690475 Val Accuracy: 97.22222222222223\n",
      "Epoch 848/1000 Loss: 0.0017474843189120293 num_feat: 6/19 Reg Loss: 0.032332856208086014 Val Accuracy: 97.22222222222223\n",
      "Epoch 849/1000 Loss: 0.0017443620599806309 num_feat: 6/19 Reg Loss: 0.03233155608177185 Val Accuracy: 97.22222222222223\n",
      "Epoch 850/1000 Loss: 0.0017403612146154046 num_feat: 6/19 Reg Loss: 0.032330263406038284 Val Accuracy: 97.22222222222223\n",
      "Epoch 851/1000 Loss: 0.0017348165856674314 num_feat: 6/19 Reg Loss: 0.032328974455595016 Val Accuracy: 97.22222222222223\n",
      "Epoch 852/1000 Loss: 0.0017289306269958615 num_feat: 6/19 Reg Loss: 0.032327692955732346 Val Accuracy: 97.22222222222223\n",
      "Epoch 853/1000 Loss: 0.0017241435125470161 num_feat: 6/19 Reg Loss: 0.032326407730579376 Val Accuracy: 97.22222222222223\n",
      "Epoch 854/1000 Loss: 0.0017203809693455696 num_feat: 6/19 Reg Loss: 0.032325129956007004 Val Accuracy: 97.22222222222223\n",
      "Epoch 855/1000 Loss: 0.0017165046883746982 num_feat: 6/19 Reg Loss: 0.03232385963201523 Val Accuracy: 97.22222222222223\n",
      "Epoch 856/1000 Loss: 0.0017117343377321959 num_feat: 6/19 Reg Loss: 0.03232258930802345 Val Accuracy: 97.22222222222223\n",
      "Epoch 857/1000 Loss: 0.001706482726149261 num_feat: 6/19 Reg Loss: 0.032321326434612274 Val Accuracy: 97.22222222222223\n",
      "Epoch 858/1000 Loss: 0.001673185615800321 num_feat: 6/19 Reg Loss: 0.032320063561201096 Val Accuracy: 97.22222222222223\n",
      "Epoch 859/1000 Loss: 0.0016984062967821956 num_feat: 6/19 Reg Loss: 0.03231881931424141 Val Accuracy: 94.44444444444444\n",
      "Epoch 860/1000 Loss: 0.0016280425479635596 num_feat: 6/19 Reg Loss: 0.03231757506728172 Val Accuracy: 94.44444444444444\n",
      "Epoch 861/1000 Loss: 0.0017039302038028836 num_feat: 6/19 Reg Loss: 0.03231634199619293 Val Accuracy: 97.22222222222223\n",
      "Epoch 862/1000 Loss: 0.0017055334756150842 num_feat: 6/19 Reg Loss: 0.03231511265039444 Val Accuracy: 97.22222222222223\n",
      "Epoch 863/1000 Loss: 0.0016927208052948117 num_feat: 6/19 Reg Loss: 0.03231387957930565 Val Accuracy: 97.22222222222223\n",
      "Epoch 864/1000 Loss: 0.0016783463070169091 num_feat: 6/19 Reg Loss: 0.03231265768408775 Val Accuracy: 97.22222222222223\n",
      "Epoch 865/1000 Loss: 0.0016715434612706304 num_feat: 6/19 Reg Loss: 0.03231143578886986 Val Accuracy: 94.44444444444444\n",
      "Epoch 866/1000 Loss: 0.0016732991207391024 num_feat: 6/19 Reg Loss: 0.03231021761894226 Val Accuracy: 94.44444444444444\n",
      "Epoch 867/1000 Loss: 0.0016744743334129453 num_feat: 6/19 Reg Loss: 0.03230900317430496 Val Accuracy: 94.44444444444444\n",
      "Epoch 868/1000 Loss: 0.0016683709109202027 num_feat: 6/19 Reg Loss: 0.032307788729667664 Val Accuracy: 97.22222222222223\n",
      "Epoch 869/1000 Loss: 0.001657760003581643 num_feat: 6/19 Reg Loss: 0.032306574285030365 Val Accuracy: 97.22222222222223\n",
      "Epoch 870/1000 Loss: 0.0016499587800353765 num_feat: 6/19 Reg Loss: 0.032305363565683365 Val Accuracy: 94.44444444444444\n",
      "Epoch 871/1000 Loss: 0.0016477818135172129 num_feat: 6/19 Reg Loss: 0.03230416029691696 Val Accuracy: 97.22222222222223\n",
      "Epoch 872/1000 Loss: 0.0016474365256726742 num_feat: 6/19 Reg Loss: 0.03230295702815056 Val Accuracy: 97.22222222222223\n",
      "Epoch 873/1000 Loss: 0.0016439517494291067 num_feat: 6/19 Reg Loss: 0.03230176493525505 Val Accuracy: 97.22222222222223\n",
      "Epoch 874/1000 Loss: 0.0016367984935641289 num_feat: 6/19 Reg Loss: 0.03230056166648865 Val Accuracy: 97.22222222222223\n",
      "Epoch 875/1000 Loss: 0.0015980475582182407 num_feat: 6/19 Reg Loss: 0.03229936584830284 Val Accuracy: 97.22222222222223\n",
      "Epoch 876/1000 Loss: 0.0016295184614136815 num_feat: 6/19 Reg Loss: 0.032298147678375244 Val Accuracy: 97.22222222222223\n",
      "Epoch 877/1000 Loss: 0.0015547481598332524 num_feat: 6/19 Reg Loss: 0.032296936959028244 Val Accuracy: 97.22222222222223\n",
      "Epoch 878/1000 Loss: 0.0016427630325779319 num_feat: 6/19 Reg Loss: 0.03229573369026184 Val Accuracy: 97.22222222222223\n",
      "Epoch 879/1000 Loss: 0.0016344132600352168 num_feat: 6/19 Reg Loss: 0.032294537872076035 Val Accuracy: 97.22222222222223\n",
      "Epoch 880/1000 Loss: 0.0016145947156473994 num_feat: 6/19 Reg Loss: 0.03229334577918053 Val Accuracy: 97.22222222222223\n",
      "Epoch 881/1000 Loss: 0.0016123580280691385 num_feat: 6/19 Reg Loss: 0.032292161136865616 Val Accuracy: 94.44444444444444\n",
      "Epoch 882/1000 Loss: 0.0016104241367429495 num_feat: 6/19 Reg Loss: 0.032290972769260406 Val Accuracy: 94.44444444444444\n",
      "Epoch 883/1000 Loss: 0.0016188255976885557 num_feat: 6/19 Reg Loss: 0.0322897844016552 Val Accuracy: 94.44444444444444\n",
      "Epoch 884/1000 Loss: 0.001614440232515335 num_feat: 6/19 Reg Loss: 0.03228860720992088 Val Accuracy: 94.44444444444444\n",
      "Epoch 885/1000 Loss: 0.0015988190425559878 num_feat: 6/19 Reg Loss: 0.032287437468767166 Val Accuracy: 97.22222222222223\n",
      "Epoch 886/1000 Loss: 0.0015857595717534423 num_feat: 6/19 Reg Loss: 0.03228626400232315 Val Accuracy: 97.22222222222223\n",
      "Epoch 887/1000 Loss: 0.0016261017881333828 num_feat: 6/19 Reg Loss: 0.03228510171175003 Val Accuracy: 97.22222222222223\n",
      "Epoch 888/1000 Loss: 0.0015803074929863214 num_feat: 6/19 Reg Loss: 0.03228393569588661 Val Accuracy: 97.22222222222223\n",
      "Epoch 889/1000 Loss: 0.0015586117515340447 num_feat: 6/19 Reg Loss: 0.03228277340531349 Val Accuracy: 97.22222222222223\n",
      "Epoch 890/1000 Loss: 0.0015719376970082521 num_feat: 6/19 Reg Loss: 0.03228161856532097 Val Accuracy: 97.22222222222223\n",
      "Epoch 891/1000 Loss: 0.001567144994623959 num_feat: 6/19 Reg Loss: 0.03228047117590904 Val Accuracy: 97.22222222222223\n",
      "Epoch 892/1000 Loss: 0.0015621311031281948 num_feat: 6/19 Reg Loss: 0.032279323786497116 Val Accuracy: 97.22222222222223\n",
      "Epoch 893/1000 Loss: 0.0015581006882712245 num_feat: 6/19 Reg Loss: 0.032278187572956085 Val Accuracy: 94.44444444444444\n",
      "Epoch 894/1000 Loss: 0.0015582337509840727 num_feat: 6/19 Reg Loss: 0.032277047634124756 Val Accuracy: 94.44444444444444\n",
      "Epoch 895/1000 Loss: 0.0015511828241869807 num_feat: 6/19 Reg Loss: 0.03227591887116432 Val Accuracy: 94.44444444444444\n",
      "Epoch 896/1000 Loss: 0.0015470260987058282 num_feat: 6/19 Reg Loss: 0.03227478265762329 Val Accuracy: 94.44444444444444\n",
      "Epoch 897/1000 Loss: 0.001542980782687664 num_feat: 6/19 Reg Loss: 0.032273661345243454 Val Accuracy: 97.22222222222223\n",
      "Epoch 898/1000 Loss: 0.0015392852947115898 num_feat: 6/19 Reg Loss: 0.03227253630757332 Val Accuracy: 97.22222222222223\n",
      "Epoch 899/1000 Loss: 0.0015358241507783532 num_feat: 6/19 Reg Loss: 0.03227141872048378 Val Accuracy: 97.22222222222223\n",
      "Epoch 900/1000 Loss: 0.0015322427498176694 num_feat: 6/19 Reg Loss: 0.03227029740810394 Val Accuracy: 97.22222222222223\n",
      "Epoch 901/1000 Loss: 0.001528421533294022 num_feat: 6/19 Reg Loss: 0.0322691909968853 Val Accuracy: 97.22222222222223\n",
      "Epoch 902/1000 Loss: 0.0015245171962305903 num_feat: 6/19 Reg Loss: 0.03226807713508606 Val Accuracy: 97.22222222222223\n",
      "Epoch 903/1000 Loss: 0.0015207919059321284 num_feat: 6/19 Reg Loss: 0.032266974449157715 Val Accuracy: 94.44444444444444\n",
      "Epoch 904/1000 Loss: 0.0015172464773058891 num_feat: 6/19 Reg Loss: 0.03226587548851967 Val Accuracy: 94.44444444444444\n",
      "Epoch 905/1000 Loss: 0.0015137502923607826 num_feat: 6/19 Reg Loss: 0.03226477652788162 Val Accuracy: 94.44444444444444\n",
      "Epoch 906/1000 Loss: 0.001510134432464838 num_feat: 6/19 Reg Loss: 0.03226368501782417 Val Accuracy: 94.44444444444444\n",
      "Epoch 907/1000 Loss: 0.0015064189210534096 num_feat: 6/19 Reg Loss: 0.032262593507766724 Val Accuracy: 94.44444444444444\n",
      "Epoch 908/1000 Loss: 0.0015027289045974612 num_feat: 6/19 Reg Loss: 0.032261501997709274 Val Accuracy: 97.22222222222223\n",
      "Epoch 909/1000 Loss: 0.001499167294241488 num_feat: 6/19 Reg Loss: 0.03226041793823242 Val Accuracy: 97.22222222222223\n",
      "Epoch 910/1000 Loss: 0.0014453628100454807 num_feat: 6/19 Reg Loss: 0.03225933387875557 Val Accuracy: 94.44444444444444\n",
      "Epoch 911/1000 Loss: 0.001495643868111074 num_feat: 6/19 Reg Loss: 0.03225826472043991 Val Accuracy: 97.22222222222223\n",
      "Epoch 912/1000 Loss: 0.0014961129054427147 num_feat: 6/19 Reg Loss: 0.032257188111543655 Val Accuracy: 97.22222222222223\n",
      "Epoch 913/1000 Loss: 0.0014923734124749899 num_feat: 6/19 Reg Loss: 0.032256118953228 Val Accuracy: 97.22222222222223\n",
      "Epoch 914/1000 Loss: 0.0014850215520709753 num_feat: 6/19 Reg Loss: 0.03225505352020264 Val Accuracy: 97.22222222222223\n",
      "Epoch 915/1000 Loss: 0.0014783889055252075 num_feat: 6/19 Reg Loss: 0.032253991812467575 Val Accuracy: 94.44444444444444\n",
      "Epoch 916/1000 Loss: 0.0014752434799447656 num_feat: 6/19 Reg Loss: 0.032252922654151917 Val Accuracy: 94.44444444444444\n",
      "Epoch 917/1000 Loss: 0.0014742751372978091 num_feat: 6/19 Reg Loss: 0.03225186467170715 Val Accuracy: 94.44444444444444\n",
      "Epoch 918/1000 Loss: 0.0014722124906256795 num_feat: 6/19 Reg Loss: 0.03225081413984299 Val Accuracy: 94.44444444444444\n",
      "Epoch 919/1000 Loss: 0.0014675813727080822 num_feat: 6/19 Reg Loss: 0.03224975988268852 Val Accuracy: 94.44444444444444\n",
      "Epoch 920/1000 Loss: 0.0014618957648053765 num_feat: 6/19 Reg Loss: 0.032248713076114655 Val Accuracy: 94.44444444444444\n",
      "Epoch 921/1000 Loss: 0.001457428908906877 num_feat: 6/19 Reg Loss: 0.03224766626954079 Val Accuracy: 97.22222222222223\n",
      "Epoch 922/1000 Loss: 0.0014550398336723447 num_feat: 6/19 Reg Loss: 0.032246626913547516 Val Accuracy: 97.22222222222223\n",
      "Epoch 923/1000 Loss: 0.0014530433109030128 num_feat: 6/19 Reg Loss: 0.03224558383226395 Val Accuracy: 97.22222222222223\n",
      "Epoch 924/1000 Loss: 0.0014498558593913913 num_feat: 6/19 Reg Loss: 0.032244544476270676 Val Accuracy: 97.22222222222223\n",
      "Epoch 925/1000 Loss: 0.0014453945914283395 num_feat: 6/19 Reg Loss: 0.0322435162961483 Val Accuracy: 94.44444444444444\n",
      "Epoch 926/1000 Loss: 0.0014409376308321953 num_feat: 6/19 Reg Loss: 0.03224248066544533 Val Accuracy: 94.44444444444444\n",
      "Epoch 927/1000 Loss: 0.001437507220543921 num_feat: 6/19 Reg Loss: 0.03224145248532295 Val Accuracy: 94.44444444444444\n",
      "Epoch 928/1000 Loss: 0.0015187175013124943 num_feat: 6/19 Reg Loss: 0.032240428030490875 Val Accuracy: 97.22222222222223\n",
      "Epoch 929/1000 Loss: 0.001433925935998559 num_feat: 6/19 Reg Loss: 0.03223938122391701 Val Accuracy: 97.22222222222223\n",
      "Epoch 930/1000 Loss: 0.001441079773940146 num_feat: 6/19 Reg Loss: 0.032238345593214035 Val Accuracy: 97.22222222222223\n",
      "Epoch 931/1000 Loss: 0.001442759414203465 num_feat: 6/19 Reg Loss: 0.03223731368780136 Val Accuracy: 97.22222222222223\n",
      "Epoch 932/1000 Loss: 0.0014336531748995185 num_feat: 6/19 Reg Loss: 0.03223628178238869 Val Accuracy: 97.22222222222223\n",
      "Epoch 933/1000 Loss: 0.0014210963854566216 num_feat: 6/19 Reg Loss: 0.03223525732755661 Val Accuracy: 94.44444444444444\n",
      "Epoch 934/1000 Loss: 0.0014138869009912014 num_feat: 6/19 Reg Loss: 0.03223423287272453 Val Accuracy: 94.44444444444444\n",
      "Epoch 935/1000 Loss: 0.001415293081663549 num_feat: 6/19 Reg Loss: 0.03223321959376335 Val Accuracy: 94.44444444444444\n",
      "Epoch 936/1000 Loss: 0.001417149556800723 num_feat: 6/19 Reg Loss: 0.03223221004009247 Val Accuracy: 94.44444444444444\n",
      "Epoch 937/1000 Loss: 0.0014140283456072211 num_feat: 6/19 Reg Loss: 0.032231204211711884 Val Accuracy: 94.44444444444444\n",
      "Epoch 938/1000 Loss: 0.0014062861446291208 num_feat: 6/19 Reg Loss: 0.0322301983833313 Val Accuracy: 94.44444444444444\n",
      "Epoch 939/1000 Loss: 0.0013990551233291626 num_feat: 6/19 Reg Loss: 0.03222920000553131 Val Accuracy: 94.44444444444444\n",
      "Epoch 940/1000 Loss: 0.0013959435746073723 num_feat: 6/19 Reg Loss: 0.03222820535302162 Val Accuracy: 97.22222222222223\n",
      "Epoch 941/1000 Loss: 0.0013956933980807662 num_feat: 6/19 Reg Loss: 0.032227206975221634 Val Accuracy: 97.22222222222223\n",
      "Epoch 942/1000 Loss: 0.001394506311044097 num_feat: 6/19 Reg Loss: 0.03222621977329254 Val Accuracy: 97.22222222222223\n",
      "Epoch 943/1000 Loss: 0.0013903052313253284 num_feat: 6/19 Reg Loss: 0.03222523257136345 Val Accuracy: 94.44444444444444\n",
      "Epoch 944/1000 Loss: 0.0013844745699316263 num_feat: 6/19 Reg Loss: 0.03222424536943436 Val Accuracy: 94.44444444444444\n",
      "Epoch 945/1000 Loss: 0.0013798127183690667 num_feat: 6/19 Reg Loss: 0.03222326189279556 Val Accuracy: 94.44444444444444\n",
      "Epoch 946/1000 Loss: 0.0013774506514891982 num_feat: 6/19 Reg Loss: 0.032222289592027664 Val Accuracy: 94.44444444444444\n",
      "Epoch 947/1000 Loss: 0.0013760237488895655 num_feat: 6/19 Reg Loss: 0.03222131356596947 Val Accuracy: 94.44444444444444\n",
      "Epoch 948/1000 Loss: 0.0013734969543293118 num_feat: 6/19 Reg Loss: 0.03222033753991127 Val Accuracy: 94.44444444444444\n",
      "Epoch 949/1000 Loss: 0.0013451509876176715 num_feat: 6/19 Reg Loss: 0.03221937268972397 Val Accuracy: 94.44444444444444\n",
      "Epoch 950/1000 Loss: 0.0013680679257959127 num_feat: 6/19 Reg Loss: 0.03221840038895607 Val Accuracy: 94.44444444444444\n",
      "Epoch 951/1000 Loss: 0.0013647780288010836 num_feat: 6/19 Reg Loss: 0.03221743926405907 Val Accuracy: 94.44444444444444\n",
      "Epoch 952/1000 Loss: 0.0013599509838968515 num_feat: 6/19 Reg Loss: 0.032216478139162064 Val Accuracy: 94.44444444444444\n",
      "Epoch 953/1000 Loss: 0.0013556015910580754 num_feat: 6/19 Reg Loss: 0.03221551701426506 Val Accuracy: 94.44444444444444\n",
      "Epoch 954/1000 Loss: 0.0013528023846447468 num_feat: 6/19 Reg Loss: 0.032214563339948654 Val Accuracy: 94.44444444444444\n",
      "Epoch 955/1000 Loss: 0.0013509229756891727 num_feat: 6/19 Reg Loss: 0.03221360966563225 Val Accuracy: 94.44444444444444\n",
      "Epoch 956/1000 Loss: 0.0013394382549449801 num_feat: 6/19 Reg Loss: 0.03221266344189644 Val Accuracy: 94.44444444444444\n",
      "Epoch 957/1000 Loss: 0.0013438850874081254 num_feat: 6/19 Reg Loss: 0.03221171721816063 Val Accuracy: 94.44444444444444\n",
      "Epoch 958/1000 Loss: 0.0013408669037744403 num_feat: 6/19 Reg Loss: 0.03221077844500542 Val Accuracy: 94.44444444444444\n",
      "Epoch 959/1000 Loss: 0.0013402800541371107 num_feat: 6/19 Reg Loss: 0.032209835946559906 Val Accuracy: 94.44444444444444\n",
      "Epoch 960/1000 Loss: 0.0013351909583434463 num_feat: 6/19 Reg Loss: 0.032208897173404694 Val Accuracy: 94.44444444444444\n",
      "Epoch 961/1000 Loss: 0.0013319527497515082 num_feat: 6/19 Reg Loss: 0.03220796212553978 Val Accuracy: 94.44444444444444\n",
      "Epoch 962/1000 Loss: 0.001329531311057508 num_feat: 6/19 Reg Loss: 0.03220702335238457 Val Accuracy: 94.44444444444444\n",
      "Epoch 963/1000 Loss: 0.0013272337382659316 num_feat: 6/19 Reg Loss: 0.03220609202980995 Val Accuracy: 94.44444444444444\n",
      "Epoch 964/1000 Loss: 0.0013243986759334803 num_feat: 6/19 Reg Loss: 0.032205160707235336 Val Accuracy: 94.44444444444444\n",
      "Epoch 965/1000 Loss: 0.0013210175093263388 num_feat: 6/19 Reg Loss: 0.03220423683524132 Val Accuracy: 94.44444444444444\n",
      "Epoch 966/1000 Loss: 0.0013176717329770327 num_feat: 6/19 Reg Loss: 0.032203309237957 Val Accuracy: 94.44444444444444\n",
      "Epoch 967/1000 Loss: 0.001314770895987749 num_feat: 6/19 Reg Loss: 0.032202381640672684 Val Accuracy: 94.44444444444444\n",
      "Epoch 968/1000 Loss: 0.0013122595846652985 num_feat: 6/19 Reg Loss: 0.032201461493968964 Val Accuracy: 94.44444444444444\n",
      "Epoch 969/1000 Loss: 0.0013097149785608053 num_feat: 6/19 Reg Loss: 0.03220054507255554 Val Accuracy: 94.44444444444444\n",
      "Epoch 970/1000 Loss: 0.0013068076223134995 num_feat: 6/19 Reg Loss: 0.03219963237643242 Val Accuracy: 94.44444444444444\n",
      "Epoch 971/1000 Loss: 0.001303702825680375 num_feat: 6/19 Reg Loss: 0.032198719680309296 Val Accuracy: 94.44444444444444\n",
      "Epoch 972/1000 Loss: 0.0013007082743570209 num_feat: 6/19 Reg Loss: 0.03219781070947647 Val Accuracy: 94.44444444444444\n",
      "Epoch 973/1000 Loss: 0.0012979903258383274 num_feat: 6/19 Reg Loss: 0.032196901738643646 Val Accuracy: 94.44444444444444\n",
      "Epoch 974/1000 Loss: 0.0012954253470525146 num_feat: 6/19 Reg Loss: 0.03219599649310112 Val Accuracy: 94.44444444444444\n",
      "Epoch 975/1000 Loss: 0.0012927463976666331 num_feat: 6/19 Reg Loss: 0.032195091247558594 Val Accuracy: 94.44444444444444\n",
      "Epoch 976/1000 Loss: 0.0012899015564471483 num_feat: 6/19 Reg Loss: 0.032194193452596664 Val Accuracy: 94.44444444444444\n",
      "Epoch 977/1000 Loss: 0.0012869921047240496 num_feat: 6/19 Reg Loss: 0.032193295657634735 Val Accuracy: 94.44444444444444\n",
      "Epoch 978/1000 Loss: 0.001284180674701929 num_feat: 6/19 Reg Loss: 0.03219239413738251 Val Accuracy: 94.44444444444444\n",
      "Epoch 979/1000 Loss: 0.0012815190711989999 num_feat: 6/19 Reg Loss: 0.032191500067710876 Val Accuracy: 94.44444444444444\n",
      "Epoch 980/1000 Loss: 0.0012789217289537191 num_feat: 6/19 Reg Loss: 0.03219061717391014 Val Accuracy: 94.44444444444444\n",
      "Epoch 981/1000 Loss: 0.0012762476690113544 num_feat: 6/19 Reg Loss: 0.03218972310423851 Val Accuracy: 94.44444444444444\n",
      "Epoch 982/1000 Loss: 0.001273471862077713 num_feat: 6/19 Reg Loss: 0.03218883275985718 Val Accuracy: 94.44444444444444\n",
      "Epoch 983/1000 Loss: 0.0012707157293334603 num_feat: 6/19 Reg Loss: 0.03218794986605644 Val Accuracy: 94.44444444444444\n",
      "Epoch 984/1000 Loss: 0.0012680432992056012 num_feat: 6/19 Reg Loss: 0.032187070697546005 Val Accuracy: 94.44444444444444\n",
      "Epoch 985/1000 Loss: 0.0012654403690248728 num_feat: 6/19 Reg Loss: 0.03218619152903557 Val Accuracy: 94.44444444444444\n",
      "Epoch 986/1000 Loss: 0.0012628245167434216 num_feat: 6/19 Reg Loss: 0.03218531236052513 Val Accuracy: 94.44444444444444\n",
      "Epoch 987/1000 Loss: 0.0012601908529177308 num_feat: 6/19 Reg Loss: 0.03218444064259529 Val Accuracy: 94.44444444444444\n",
      "Epoch 988/1000 Loss: 0.001672303769737482 num_feat: 6/19 Reg Loss: 0.03218356892466545 Val Accuracy: 94.44444444444444\n",
      "Epoch 989/1000 Loss: 0.001287532038986683 num_feat: 6/19 Reg Loss: 0.03218260779976845 Val Accuracy: 94.44444444444444\n",
      "Epoch 990/1000 Loss: 0.001342272968031466 num_feat: 6/19 Reg Loss: 0.03218166157603264 Val Accuracy: 94.44444444444444\n",
      "Epoch 991/1000 Loss: 0.001354319043457508 num_feat: 6/19 Reg Loss: 0.032180726528167725 Val Accuracy: 94.44444444444444\n",
      "Epoch 992/1000 Loss: 0.0013111921725794673 num_feat: 6/19 Reg Loss: 0.032179806381464005 Val Accuracy: 94.44444444444444\n",
      "Epoch 993/1000 Loss: 0.0012478511780500412 num_feat: 6/19 Reg Loss: 0.03217888996005058 Val Accuracy: 94.44444444444444\n",
      "Epoch 994/1000 Loss: 0.001243641134351492 num_feat: 6/19 Reg Loss: 0.03217797726392746 Val Accuracy: 97.22222222222223\n",
      "Epoch 995/1000 Loss: 0.0012727732537314296 num_feat: 6/19 Reg Loss: 0.03217707574367523 Val Accuracy: 97.22222222222223\n",
      "Epoch 996/1000 Loss: 0.0013028020039200783 num_feat: 6/19 Reg Loss: 0.0321761853992939 Val Accuracy: 97.22222222222223\n",
      "Epoch 997/1000 Loss: 0.0012977420119568706 num_feat: 6/19 Reg Loss: 0.032175298780202866 Val Accuracy: 97.22222222222223\n",
      "Epoch 998/1000 Loss: 0.0012631653808057308 num_feat: 6/19 Reg Loss: 0.03217441588640213 Val Accuracy: 94.44444444444444\n",
      "Epoch 999/1000 Loss: 0.0012325873831287026 num_feat: 6/19 Reg Loss: 0.03217354416847229 Val Accuracy: 94.44444444444444\n",
      "Epoch 1000/1000 Loss: 0.0012293856125324965 num_feat: 6/19 Reg Loss: 0.03217267245054245 Val Accuracy: 94.44444444444444\n",
      "Accuracy = 100.0%\n",
      "100.0%, 34 features\n"
     ]
    }
   ],
   "source": [
    "X, y = shap.datasets.iris()\n",
    "accs = []\n",
    "total_feature = []\n",
    "warnings.simplefilter(action='ignore')\n",
    "X_train, X_test, Y_train, Y_test = insert_feature_noise(X, y, \n",
    "            num_random_noise=num_normal[-1],\n",
    "            num_overwhelemed=num_overwhelmed[-1], \n",
    "            num_shortcut=num_shortcut[-1])\n",
    "\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = gini_filter(torch.tensor(X_train.values), torch.tensor(X_test.values), \n",
    "            torch.tensor(Y_train, dtype=torch.int64), \n",
    "            torch.tensor(Y_test, dtype=torch.int64), \n",
    "            left=0.5)\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = X_train.numpy(), X_test.numpy(), Y_train.numpy(), Y_test.numpy()\n",
    "X_train, Y_train = X_train.astype(np.float32), Y_train.astype(np.int64)\n",
    "X_test, Y_test = X_test.astype(np.float32), Y_test.astype(np.int64)\n",
    "\n",
    "clf = skw.ScikitClfWrapper(\n",
    "        input_dim=X_train.shape[1],\n",
    "        number_of_classes=3,\n",
    "        hidden_dims=(16, 16), lam=0.1, epochs=1000, sigma=0.5,\n",
    "        freeze_till=0, lr=0.1, verbose=True,\n",
    "        device='cpu')\n",
    "clf.fit(X_train, Y_train)\n",
    "acc = print_accuracy(clf.predict, X_test, Y_test)\n",
    "total_feature = 2*num_normal[i] + num_overwhelmed[i] + num_shortcut[i]\n",
    "print(f\"{acc}%, {total_feature} features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epoch</th>\n",
       "      <th>valid_acc</th>\n",
       "      <th># feature</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>27.777778</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>27.777778</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>27.777778</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>27.777778</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>27.777778</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   epoch  valid_acc  # feature\n",
       "0      0  27.777778         19\n",
       "1      1  27.777778         19\n",
       "2      2  27.777778         19\n",
       "3      3  27.777778         19\n",
       "4      4  27.777778         19"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "two_step_history = clf.get_history()\n",
    "two_step_history.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x22b94d983d0>]"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\r\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\r\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\r\n<svg height=\"248.518125pt\" version=\"1.1\" viewBox=\"0 0 368.925 248.518125\" width=\"368.925pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\r\n <metadata>\r\n  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\r\n   <cc:Work>\r\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\r\n    <dc:date>2022-03-29T21:23:53.150780</dc:date>\r\n    <dc:format>image/svg+xml</dc:format>\r\n    <dc:creator>\r\n     <cc:Agent>\r\n      <dc:title>Matplotlib v3.4.3, https://matplotlib.org/</dc:title>\r\n     </cc:Agent>\r\n    </dc:creator>\r\n   </cc:Work>\r\n  </rdf:RDF>\r\n </metadata>\r\n <defs>\r\n  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\r\n </defs>\r\n <g id=\"figure_1\">\r\n  <g id=\"patch_1\">\r\n   <path d=\"M -0 248.518125 \r\nL 368.925 248.518125 \r\nL 368.925 0 \r\nL -0 0 \r\nz\r\n\" style=\"fill:none;\"/>\r\n  </g>\r\n  <g id=\"axes_1\">\r\n   <g id=\"patch_2\">\r\n    <path d=\"M 26.925 224.64 \r\nL 361.725 224.64 \r\nL 361.725 7.2 \r\nL 26.925 7.2 \r\nz\r\n\" style=\"fill:#ffffff;\"/>\r\n   </g>\r\n   <g id=\"matplotlib.axis_1\">\r\n    <g id=\"xtick_1\">\r\n     <g id=\"line2d_1\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL 0 3.5 \r\n\" id=\"m025ba57f32\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"42.143182\" xlink:href=\"#m025ba57f32\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_1\">\r\n      <!-- 0 -->\r\n      <g transform=\"translate(38.961932 239.238437)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 2034 4250 \r\nQ 1547 4250 1301 3770 \r\nQ 1056 3291 1056 2328 \r\nQ 1056 1369 1301 889 \r\nQ 1547 409 2034 409 \r\nQ 2525 409 2770 889 \r\nQ 3016 1369 3016 2328 \r\nQ 3016 3291 2770 3770 \r\nQ 2525 4250 2034 4250 \r\nz\r\nM 2034 4750 \r\nQ 2819 4750 3233 4129 \r\nQ 3647 3509 3647 2328 \r\nQ 3647 1150 3233 529 \r\nQ 2819 -91 2034 -91 \r\nQ 1250 -91 836 529 \r\nQ 422 1150 422 2328 \r\nQ 422 3509 836 4129 \r\nQ 1250 4750 2034 4750 \r\nz\r\n\" id=\"DejaVuSans-30\" transform=\"scale(0.015625)\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_2\">\r\n     <g id=\"line2d_2\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"103.076843\" xlink:href=\"#m025ba57f32\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_2\">\r\n      <!-- 200 -->\r\n      <g transform=\"translate(93.533093 239.238437)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 1228 531 \r\nL 3431 531 \r\nL 3431 0 \r\nL 469 0 \r\nL 469 531 \r\nQ 828 903 1448 1529 \r\nQ 2069 2156 2228 2338 \r\nQ 2531 2678 2651 2914 \r\nQ 2772 3150 2772 3378 \r\nQ 2772 3750 2511 3984 \r\nQ 2250 4219 1831 4219 \r\nQ 1534 4219 1204 4116 \r\nQ 875 4013 500 3803 \r\nL 500 4441 \r\nQ 881 4594 1212 4672 \r\nQ 1544 4750 1819 4750 \r\nQ 2544 4750 2975 4387 \r\nQ 3406 4025 3406 3419 \r\nQ 3406 3131 3298 2873 \r\nQ 3191 2616 2906 2266 \r\nQ 2828 2175 2409 1742 \r\nQ 1991 1309 1228 531 \r\nz\r\n\" id=\"DejaVuSans-32\" transform=\"scale(0.015625)\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-32\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_3\">\r\n     <g id=\"line2d_3\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"164.010504\" xlink:href=\"#m025ba57f32\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_3\">\r\n      <!-- 400 -->\r\n      <g transform=\"translate(154.466754 239.238437)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 2419 4116 \r\nL 825 1625 \r\nL 2419 1625 \r\nL 2419 4116 \r\nz\r\nM 2253 4666 \r\nL 3047 4666 \r\nL 3047 1625 \r\nL 3713 1625 \r\nL 3713 1100 \r\nL 3047 1100 \r\nL 3047 0 \r\nL 2419 0 \r\nL 2419 1100 \r\nL 313 1100 \r\nL 313 1709 \r\nL 2253 4666 \r\nz\r\n\" id=\"DejaVuSans-34\" transform=\"scale(0.015625)\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-34\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_4\">\r\n     <g id=\"line2d_4\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"224.944165\" xlink:href=\"#m025ba57f32\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_4\">\r\n      <!-- 600 -->\r\n      <g transform=\"translate(215.400415 239.238437)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 2113 2584 \r\nQ 1688 2584 1439 2293 \r\nQ 1191 2003 1191 1497 \r\nQ 1191 994 1439 701 \r\nQ 1688 409 2113 409 \r\nQ 2538 409 2786 701 \r\nQ 3034 994 3034 1497 \r\nQ 3034 2003 2786 2293 \r\nQ 2538 2584 2113 2584 \r\nz\r\nM 3366 4563 \r\nL 3366 3988 \r\nQ 3128 4100 2886 4159 \r\nQ 2644 4219 2406 4219 \r\nQ 1781 4219 1451 3797 \r\nQ 1122 3375 1075 2522 \r\nQ 1259 2794 1537 2939 \r\nQ 1816 3084 2150 3084 \r\nQ 2853 3084 3261 2657 \r\nQ 3669 2231 3669 1497 \r\nQ 3669 778 3244 343 \r\nQ 2819 -91 2113 -91 \r\nQ 1303 -91 875 529 \r\nQ 447 1150 447 2328 \r\nQ 447 3434 972 4092 \r\nQ 1497 4750 2381 4750 \r\nQ 2619 4750 2861 4703 \r\nQ 3103 4656 3366 4563 \r\nz\r\n\" id=\"DejaVuSans-36\" transform=\"scale(0.015625)\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-36\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_5\">\r\n     <g id=\"line2d_5\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"285.877826\" xlink:href=\"#m025ba57f32\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_5\">\r\n      <!-- 800 -->\r\n      <g transform=\"translate(276.334076 239.238437)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 2034 2216 \r\nQ 1584 2216 1326 1975 \r\nQ 1069 1734 1069 1313 \r\nQ 1069 891 1326 650 \r\nQ 1584 409 2034 409 \r\nQ 2484 409 2743 651 \r\nQ 3003 894 3003 1313 \r\nQ 3003 1734 2745 1975 \r\nQ 2488 2216 2034 2216 \r\nz\r\nM 1403 2484 \r\nQ 997 2584 770 2862 \r\nQ 544 3141 544 3541 \r\nQ 544 4100 942 4425 \r\nQ 1341 4750 2034 4750 \r\nQ 2731 4750 3128 4425 \r\nQ 3525 4100 3525 3541 \r\nQ 3525 3141 3298 2862 \r\nQ 3072 2584 2669 2484 \r\nQ 3125 2378 3379 2068 \r\nQ 3634 1759 3634 1313 \r\nQ 3634 634 3220 271 \r\nQ 2806 -91 2034 -91 \r\nQ 1263 -91 848 271 \r\nQ 434 634 434 1313 \r\nQ 434 1759 690 2068 \r\nQ 947 2378 1403 2484 \r\nz\r\nM 1172 3481 \r\nQ 1172 3119 1398 2916 \r\nQ 1625 2713 2034 2713 \r\nQ 2441 2713 2670 2916 \r\nQ 2900 3119 2900 3481 \r\nQ 2900 3844 2670 4047 \r\nQ 2441 4250 2034 4250 \r\nQ 1625 4250 1398 4047 \r\nQ 1172 3844 1172 3481 \r\nz\r\n\" id=\"DejaVuSans-38\" transform=\"scale(0.015625)\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-38\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_6\">\r\n     <g id=\"line2d_6\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"346.811486\" xlink:href=\"#m025ba57f32\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_6\">\r\n      <!-- 1000 -->\r\n      <g transform=\"translate(334.086486 239.238437)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 794 531 \r\nL 1825 531 \r\nL 1825 4091 \r\nL 703 3866 \r\nL 703 4441 \r\nL 1819 4666 \r\nL 2450 4666 \r\nL 2450 531 \r\nL 3481 531 \r\nL 3481 0 \r\nL 794 0 \r\nL 794 531 \r\nz\r\n\" id=\"DejaVuSans-31\" transform=\"scale(0.015625)\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-31\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\r\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"matplotlib.axis_2\">\r\n    <g id=\"ytick_1\">\r\n     <g id=\"line2d_7\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL -3.5 0 \r\n\" id=\"m0b80fe57f7\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#m0b80fe57f7\" y=\"223.82392\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_7\">\r\n      <!-- 0 -->\r\n      <g transform=\"translate(13.5625 227.623139)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_2\">\r\n     <g id=\"line2d_8\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#m0b80fe57f7\" y=\"199.015086\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_8\">\r\n      <!-- 2 -->\r\n      <g transform=\"translate(13.5625 202.814305)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-32\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_3\">\r\n     <g id=\"line2d_9\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#m0b80fe57f7\" y=\"174.206252\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_9\">\r\n      <!-- 4 -->\r\n      <g transform=\"translate(13.5625 178.005471)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-34\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_4\">\r\n     <g id=\"line2d_10\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#m0b80fe57f7\" y=\"149.397418\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_10\">\r\n      <!-- 6 -->\r\n      <g transform=\"translate(13.5625 153.196637)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-36\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_5\">\r\n     <g id=\"line2d_11\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#m0b80fe57f7\" y=\"124.588584\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_11\">\r\n      <!-- 8 -->\r\n      <g transform=\"translate(13.5625 128.387803)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-38\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_6\">\r\n     <g id=\"line2d_12\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#m0b80fe57f7\" y=\"99.77975\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_12\">\r\n      <!-- 10 -->\r\n      <g transform=\"translate(7.2 103.578969)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-31\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_7\">\r\n     <g id=\"line2d_13\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#m0b80fe57f7\" y=\"74.970916\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_13\">\r\n      <!-- 12 -->\r\n      <g transform=\"translate(7.2 78.770135)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-31\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-32\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_8\">\r\n     <g id=\"line2d_14\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#m0b80fe57f7\" y=\"50.162082\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_14\">\r\n      <!-- 14 -->\r\n      <g transform=\"translate(7.2 53.9613)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-31\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-34\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_9\">\r\n     <g id=\"line2d_15\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#m0b80fe57f7\" y=\"25.353248\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_15\">\r\n      <!-- 16 -->\r\n      <g transform=\"translate(7.2 29.152466)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-31\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-36\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"line2d_16\">\r\n    <path clip-path=\"url(#p9676133514)\" d=\"M 42.143182 214.756364 \r\nL 49.759889 214.756364 \r\nL 50.369226 210.222585 \r\nL 50.673894 213.849608 \r\nL 50.978563 214.756364 \r\nL 51.283231 202.96854 \r\nL 51.587899 214.756364 \r\nL 51.892568 207.502319 \r\nL 52.197236 212.942852 \r\nL 52.501904 202.061785 \r\nL 52.806572 207.502319 \r\nL 53.111241 201.155029 \r\nL 54.329914 201.155029 \r\nL 54.634582 199.341518 \r\nL 54.939251 201.155029 \r\nL 55.243919 208.409074 \r\nL 55.548587 197.528007 \r\nL 55.853256 195.714495 \r\nL 56.462592 197.528007 \r\nL 56.76726 201.155029 \r\nL 57.071929 195.714495 \r\nL 57.376597 197.528007 \r\nL 57.681265 198.434762 \r\nL 57.985934 202.96854 \r\nL 58.290602 202.061785 \r\nL 58.59527 200.248274 \r\nL 58.899939 197.528007 \r\nL 59.204607 197.528007 \r\nL 59.509275 199.341518 \r\nL 59.813943 198.434762 \r\nL 60.118612 196.621251 \r\nL 60.42328 199.341518 \r\nL 60.727948 198.434762 \r\nL 61.032617 198.434762 \r\nL 61.337285 194.80774 \r\nL 61.641953 198.434762 \r\nL 61.946622 199.341518 \r\nL 62.25129 194.80774 \r\nL 62.555958 193.900984 \r\nL 62.860627 195.714495 \r\nL 63.165295 195.714495 \r\nL 63.774631 202.96854 \r\nL 64.0793 193.900984 \r\nL 64.383968 197.528007 \r\nL 64.688636 198.434762 \r\nL 64.993305 196.621251 \r\nL 65.297973 198.434762 \r\nL 65.90731 198.434762 \r\nL 66.211978 197.528007 \r\nL 66.516646 195.714495 \r\nL 66.821314 198.679831 \r\nL 67.125983 194.954781 \r\nL 67.430651 205.198669 \r\nL 67.735319 194.023519 \r\nL 68.039988 201.473619 \r\nL 68.344656 194.023519 \r\nL 68.649324 196.817306 \r\nL 68.953993 196.817306 \r\nL 69.258661 197.981384 \r\nL 69.563329 197.024254 \r\nL 69.867998 194.152861 \r\nL 70.172666 194.43437 \r\nL 70.477334 193.305116 \r\nL 70.782002 194.289594 \r\nL 71.086671 192.407504 \r\nL 71.391339 189.367206 \r\nL 71.696007 191.394072 \r\nL 72.000676 192.407504 \r\nL 72.610012 188.323063 \r\nL 72.914681 189.367206 \r\nL 73.219349 192.499635 \r\nL 73.524017 186.136889 \r\nL 73.828686 186.136889 \r\nL 74.133354 189.367206 \r\nL 74.438022 188.290434 \r\nL 74.74269 190.443978 \r\nL 75.047359 186.136889 \r\nL 75.352027 194.751068 \r\nL 75.656695 193.674295 \r\nL 75.961364 186.032685 \r\nL 76.266032 192.701727 \r\nL 76.5707 187.070092 \r\nL 76.875369 186.906012 \r\nL 77.180037 197.981384 \r\nL 77.484705 195.748079 \r\nL 77.789373 190.643381 \r\nL 78.094042 181.983624 \r\nL 78.39871 189.367206 \r\nL 78.703378 190.597803 \r\nL 79.008047 188.136609 \r\nL 79.312715 184.614556 \r\nL 79.617383 191.743531 \r\nL 79.922052 189.367206 \r\nL 80.22672 182.986333 \r\nL 80.531388 180.433984 \r\nL 80.836057 185.538682 \r\nL 81.140725 182.740915 \r\nL 81.445393 182.740915 \r\nL 81.750061 176.962789 \r\nL 82.05473 178.341058 \r\nL 82.359398 181.097595 \r\nL 82.664066 176.962789 \r\nL 82.968735 183.854132 \r\nL 83.273403 181.097595 \r\nL 83.578071 181.097595 \r\nL 83.88274 179.317331 \r\nL 84.187408 174.386026 \r\nL 84.492076 175.271278 \r\nL 84.796744 175.271278 \r\nL 85.101413 172.138849 \r\nL 85.406081 172.138849 \r\nL 85.710749 170.572635 \r\nL 86.015418 170.572635 \r\nL 86.320086 171.318451 \r\nL 86.624754 166.970342 \r\nL 86.929423 168.693178 \r\nL 87.234091 168.693178 \r\nL 87.538759 167.605071 \r\nL 87.843428 162.164537 \r\nL 88.148096 162.164537 \r\nL 88.452764 160.351026 \r\nL 88.757432 162.164537 \r\nL 89.366769 162.164537 \r\nL 89.671437 169.418582 \r\nL 90.280774 162.164537 \r\nL 90.585442 165.79156 \r\nL 90.890111 172.138849 \r\nL 91.194779 163.524671 \r\nL 91.499447 165.247506 \r\nL 91.804115 168.693178 \r\nL 92.413452 165.247506 \r\nL 92.71812 168.693178 \r\nL 93.022789 173.861685 \r\nL 93.327457 172.138849 \r\nL 93.632125 180.753028 \r\nL 94.241462 165.247506 \r\nL 94.54613 165.247506 \r\nL 94.850799 167.605071 \r\nL 95.155467 167.605071 \r\nL 95.460135 166.970342 \r\nL 95.764803 165.247506 \r\nL 96.069472 170.416013 \r\nL 96.37414 166.970342 \r\nL 96.678808 167.605071 \r\nL 96.983477 163.978048 \r\nL 97.288145 162.164537 \r\nL 97.592813 162.164537 \r\nL 97.897482 167.605071 \r\nL 98.20215 162.164537 \r\nL 98.506818 162.164537 \r\nL 98.811486 160.351026 \r\nL 99.116155 163.978048 \r\nL 99.420823 171.232093 \r\nL 99.725491 162.164537 \r\nL 100.03016 163.978048 \r\nL 100.334828 162.164537 \r\nL 100.639496 162.164537 \r\nL 100.944165 163.978048 \r\nL 101.248833 160.653278 \r\nL 101.553501 154.910492 \r\nL 101.85817 152.883627 \r\nL 102.162838 154.910492 \r\nL 102.467506 160.991089 \r\nL 102.772174 154.910492 \r\nL 103.076843 154.910492 \r\nL 103.381511 160.991089 \r\nL 103.686179 157.064037 \r\nL 103.990848 154.910492 \r\nL 104.295516 154.910492 \r\nL 104.600184 150.603403 \r\nL 105.818857 150.603403 \r\nL 106.123526 152.756947 \r\nL 106.428194 150.603403 \r\nL 107.037531 150.603403 \r\nL 107.342199 148.449858 \r\nL 107.951536 148.449858 \r\nL 108.256204 152.756947 \r\nL 108.560872 152.756947 \r\nL 108.865541 150.603403 \r\nL 109.170209 152.756947 \r\nL 109.474877 145.722035 \r\nL 110.084214 145.722035 \r\nL 110.388882 140.143329 \r\nL 110.69355 137.682135 \r\nL 110.998219 140.143329 \r\nL 112.826229 140.143329 \r\nL 113.435565 135.220941 \r\nL 113.740233 142.604523 \r\nL 114.044902 140.143329 \r\nL 114.654238 140.143329 \r\nL 114.958907 142.604523 \r\nL 115.263575 140.143329 \r\nL 115.872912 140.143329 \r\nL 116.17758 131.055844 \r\nL 116.482248 131.055844 \r\nL 116.786916 120.453778 \r\nL 117.091585 126.196564 \r\nL 117.396253 129.067957 \r\nL 117.700921 126.196564 \r\nL 118.00559 117.32135 \r\nL 118.310258 117.32135 \r\nL 118.919595 111.056493 \r\nL 119.528931 117.32135 \r\nL 120.138268 117.32135 \r\nL 120.747604 123.586207 \r\nL 121.052273 120.453778 \r\nL 121.356941 106.671093 \r\nL 121.966278 106.671093 \r\nL 122.270946 99.77975 \r\nL 122.575614 103.225421 \r\nL 122.880283 103.225421 \r\nL 123.184951 106.671093 \r\nL 125.622297 106.671093 \r\nL 125.926966 103.225421 \r\nL 126.231634 106.671093 \r\nL 128.059644 106.671093 \r\nL 128.364312 103.225421 \r\nL 128.66898 106.671093 \r\nL 128.973649 106.671093 \r\nL 129.278317 93.654112 \r\nL 130.49699 93.654112 \r\nL 130.801658 89.825588 \r\nL 131.106327 89.825588 \r\nL 131.410995 93.654112 \r\nL 131.715663 89.825588 \r\nL 132.020332 89.825588 \r\nL 132.325 93.654112 \r\nL 136.895025 93.654112 \r\nL 137.199693 89.825588 \r\nL 137.504361 93.654112 \r\nL 138.723034 93.654112 \r\nL 139.027703 89.825588 \r\nL 139.332371 93.654112 \r\nL 139.637039 89.825588 \r\nL 139.941708 89.825588 \r\nL 140.246376 93.654112 \r\nL 141.465049 93.654112 \r\nL 141.769717 89.825588 \r\nL 142.074386 93.654112 \r\nL 142.379054 93.654112 \r\nL 142.683722 85.997064 \r\nL 142.988391 93.654112 \r\nL 143.293059 93.654112 \r\nL 143.597727 89.825588 \r\nL 143.902396 93.654112 \r\nL 145.730405 93.654112 \r\nL 146.035074 89.825588 \r\nL 146.339742 89.825588 \r\nL 146.64441 93.654112 \r\nL 148.167752 93.654112 \r\nL 148.47242 89.825588 \r\nL 148.777088 93.654112 \r\nL 149.386425 93.654112 \r\nL 149.691093 77.382886 \r\nL 150.909767 77.382886 \r\nL 151.214435 73.075796 \r\nL 151.519103 73.075796 \r\nL 151.823771 77.382886 \r\nL 157.917138 77.382886 \r\nL 158.221806 73.075796 \r\nL 158.526474 77.382886 \r\nL 160.049816 77.382886 \r\nL 160.354484 73.075796 \r\nL 160.659152 77.382886 \r\nL 167.057187 77.382886 \r\nL 167.361855 73.075796 \r\nL 167.666523 77.382886 \r\nL 211.234091 77.382886 \r\nL 211.538759 81.689975 \r\nL 211.843428 77.382886 \r\nL 261.504361 77.382886 \r\nL 261.809029 81.689975 \r\nL 262.113698 77.382886 \r\nL 310.555958 77.382886 \r\nL 310.860627 81.689975 \r\nL 311.165295 81.689975 \r\nL 311.469963 77.382886 \r\nL 312.688636 77.382886 \r\nL 312.993305 81.689975 \r\nL 313.297973 77.382886 \r\nL 315.125983 77.382886 \r\nL 315.430651 81.689975 \r\nL 315.735319 77.382886 \r\nL 316.649324 77.382886 \r\nL 316.953993 81.689975 \r\nL 317.258661 81.689975 \r\nL 317.563329 77.382886 \r\nL 333.710749 77.382886 \r\nL 334.015418 56.462738 \r\nL 337.366769 56.462738 \r\nL 337.671437 95.84184 \r\nL 337.976106 56.462738 \r\nL 341.632125 56.462738 \r\nL 341.936794 61.385126 \r\nL 342.241462 61.385126 \r\nL 342.54613 56.462738 \r\nL 342.850799 61.385126 \r\nL 343.460135 51.54035 \r\nL 343.764803 56.462738 \r\nL 344.983477 56.462738 \r\nL 345.288145 51.54035 \r\nL 345.592813 51.54035 \r\nL 345.897482 56.462738 \r\nL 346.506818 56.462738 \r\nL 346.506818 56.462738 \r\n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\r\n   </g>\r\n   <g id=\"line2d_17\">\r\n    <path clip-path=\"url(#p9676133514)\" d=\"M 42.143182 212.942852 \r\nL 44.27586 212.942852 \r\nL 45.494533 209.31583 \r\nL 45.799201 209.31583 \r\nL 46.10387 208.409074 \r\nL 46.408538 206.595563 \r\nL 46.713206 206.595563 \r\nL 47.017875 204.782052 \r\nL 47.322543 203.875296 \r\nL 50.064558 203.875296 \r\nL 50.369226 202.061785 \r\nL 50.673894 196.621251 \r\nL 51.283231 192.994229 \r\nL 346.506818 192.994229 \r\nL 346.506818 192.994229 \r\n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\r\n   </g>\r\n   <g id=\"line2d_18\">\r\n    <path clip-path=\"url(#p9676133514)\" d=\"M 42.143182 205.688807 \r\nL 51.283231 205.688807 \r\nL 51.587899 182.113161 \r\nL 51.892568 205.688807 \r\nL 52.197236 191.180717 \r\nL 52.501904 191.180717 \r\nL 52.806572 183.926672 \r\nL 53.111241 180.29965 \r\nL 55.853256 180.29965 \r\nL 56.157924 174.859116 \r\nL 56.462592 176.672627 \r\nL 56.76726 180.29965 \r\nL 57.071929 198.434762 \r\nL 57.376597 205.688807 \r\nL 57.681265 180.29965 \r\nL 58.290602 176.672627 \r\nL 58.59527 178.486138 \r\nL 58.899939 176.672627 \r\nL 59.204607 178.486138 \r\nL 59.509275 171.232093 \r\nL 59.813943 167.605071 \r\nL 60.118612 178.486138 \r\nL 60.42328 178.486138 \r\nL 60.727948 180.29965 \r\nL 61.337285 180.29965 \r\nL 61.641953 178.486138 \r\nL 61.946622 169.418582 \r\nL 62.25129 167.605071 \r\nL 62.555958 178.486138 \r\nL 62.860627 167.605071 \r\nL 63.165295 178.486138 \r\nL 63.469963 174.859116 \r\nL 63.774631 174.053111 \r\nL 64.0793 177.881635 \r\nL 64.383968 177.881635 \r\nL 64.688636 175.967373 \r\nL 65.297973 175.967373 \r\nL 65.602641 154.910492 \r\nL 65.90731 164.481802 \r\nL 66.516646 154.910492 \r\nL 66.821314 156.824754 \r\nL 67.125983 160.653278 \r\nL 67.430651 158.739016 \r\nL 67.735319 166.396063 \r\nL 68.039988 168.310325 \r\nL 68.344656 158.739016 \r\nL 68.649324 156.824754 \r\nL 68.953993 160.653278 \r\nL 69.258661 162.56754 \r\nL 69.563329 158.739016 \r\nL 69.867998 166.396063 \r\nL 70.172666 166.396063 \r\nL 70.782002 162.56754 \r\nL 71.086671 162.56754 \r\nL 71.391339 174.053111 \r\nL 71.696007 169.098551 \r\nL 72.000676 173.152282 \r\nL 72.305344 171.125416 \r\nL 72.914681 156.937358 \r\nL 73.219349 169.098551 \r\nL 73.828686 145.722035 \r\nL 74.133354 164.098949 \r\nL 75.047359 126.196564 \r\nL 75.352027 131.939349 \r\nL 75.961364 114.188921 \r\nL 76.266032 123.586207 \r\nL 76.5707 106.671093 \r\nL 76.875369 110.116764 \r\nL 77.180037 103.225421 \r\nL 77.484705 113.562435 \r\nL 77.789373 113.562435 \r\nL 78.094042 103.225421 \r\nL 78.39871 97.482636 \r\nL 79.008047 97.482636 \r\nL 79.312715 89.825588 \r\nL 79.617383 85.997064 \r\nL 80.22672 128.110826 \r\nL 80.531388 128.110826 \r\nL 80.836057 116.146689 \r\nL 81.140725 73.075796 \r\nL 81.445393 81.074677 \r\nL 81.750061 95.84184 \r\nL 82.05473 74.511493 \r\nL 82.359398 68.768707 \r\nL 82.664066 34.311993 \r\nL 82.968735 63.025922 \r\nL 83.273403 74.511493 \r\nL 83.88274 74.511493 \r\nL 84.187408 45.797565 \r\nL 84.492076 28.569208 \r\nL 84.796744 34.311993 \r\nL 85.101413 76.152289 \r\nL 85.406081 61.385126 \r\nL 86.015418 51.54035 \r\nL 86.320086 71.229901 \r\nL 86.624754 51.54035 \r\nL 86.929423 61.385126 \r\nL 87.234091 51.54035 \r\nL 87.538759 61.385126 \r\nL 87.843428 61.385126 \r\nL 88.148096 76.152289 \r\nL 88.452764 76.152289 \r\nL 88.757432 51.54035 \r\nL 89.062101 51.54035 \r\nL 89.366769 22.826422 \r\nL 89.671437 17.083636 \r\nL 89.976106 22.826422 \r\nL 90.280774 22.826422 \r\nL 90.585442 57.283136 \r\nL 90.890111 57.283136 \r\nL 91.194779 22.826422 \r\nL 91.499447 40.054779 \r\nL 91.804115 40.054779 \r\nL 92.108784 51.54035 \r\nL 92.413452 40.054779 \r\nL 92.71812 22.826422 \r\nL 93.022789 28.569208 \r\nL 93.327457 40.054779 \r\nL 93.632125 34.311993 \r\nL 93.936794 40.054779 \r\nL 94.241462 34.311993 \r\nL 94.54613 22.826422 \r\nL 95.155467 22.826422 \r\nL 95.460135 57.283136 \r\nL 95.764803 22.826422 \r\nL 96.37414 22.826422 \r\nL 96.678808 40.054779 \r\nL 96.983477 22.826422 \r\nL 98.811486 22.826422 \r\nL 99.116155 17.083636 \r\nL 99.420823 17.083636 \r\nL 99.725491 22.826422 \r\nL 100.03016 17.083636 \r\nL 100.334828 22.826422 \r\nL 101.248833 22.826422 \r\nL 101.553501 17.083636 \r\nL 101.85817 22.826422 \r\nL 102.467506 22.826422 \r\nL 102.772174 17.083636 \r\nL 103.076843 22.826422 \r\nL 103.990848 22.826422 \r\nL 104.295516 34.311993 \r\nL 104.600184 22.826422 \r\nL 104.904853 28.569208 \r\nL 105.209521 51.54035 \r\nL 105.514189 34.311993 \r\nL 106.123526 34.311993 \r\nL 106.428194 17.083636 \r\nL 106.732862 22.826422 \r\nL 107.037531 22.826422 \r\nL 107.342199 17.083636 \r\nL 107.646867 17.083636 \r\nL 107.951536 22.826422 \r\nL 108.256204 34.311993 \r\nL 108.560872 22.826422 \r\nL 108.865541 34.311993 \r\nL 109.170209 22.826422 \r\nL 109.779545 22.826422 \r\nL 110.388882 34.311993 \r\nL 110.69355 28.569208 \r\nL 110.998219 34.311993 \r\nL 111.302887 22.826422 \r\nL 112.826229 22.826422 \r\nL 113.130897 28.569208 \r\nL 113.435565 28.569208 \r\nL 113.740233 34.311993 \r\nL 114.34957 22.826422 \r\nL 117.091585 22.826422 \r\nL 117.396253 28.569208 \r\nL 117.700921 22.826422 \r\nL 118.00559 28.569208 \r\nL 118.310258 22.826422 \r\nL 121.356941 22.826422 \r\nL 121.661609 40.054779 \r\nL 121.966278 22.826422 \r\nL 123.489619 22.826422 \r\nL 123.794287 34.311993 \r\nL 124.098956 34.311993 \r\nL 124.403624 22.826422 \r\nL 126.231634 22.826422 \r\nL 126.536302 34.311993 \r\nL 126.840971 22.826422 \r\nL 129.278317 22.826422 \r\nL 129.582985 34.311993 \r\nL 129.887654 22.826422 \r\nL 130.801658 22.826422 \r\nL 131.106327 17.083636 \r\nL 131.410995 28.569208 \r\nL 131.715663 22.826422 \r\nL 133.543673 22.826422 \r\nL 133.848342 34.311993 \r\nL 134.762346 34.311993 \r\nL 135.067015 22.826422 \r\nL 136.895025 22.826422 \r\nL 137.199693 34.311993 \r\nL 137.504361 34.311993 \r\nL 137.809029 22.826422 \r\nL 139.332371 22.826422 \r\nL 139.637039 28.569208 \r\nL 139.941708 28.569208 \r\nL 140.246376 22.826422 \r\nL 141.769717 22.826422 \r\nL 142.074386 28.569208 \r\nL 142.379054 40.054779 \r\nL 142.683722 22.826422 \r\nL 144.207064 22.826422 \r\nL 144.511732 28.569208 \r\nL 144.8164 22.826422 \r\nL 145.730405 22.826422 \r\nL 146.035074 28.569208 \r\nL 146.339742 22.826422 \r\nL 147.253747 22.826422 \r\nL 147.558415 40.054779 \r\nL 147.863084 22.826422 \r\nL 175.587899 22.826422 \r\nL 175.892568 28.569208 \r\nL 176.197236 22.826422 \r\nL 177.415909 22.826422 \r\nL 177.720577 28.569208 \r\nL 178.025246 28.569208 \r\nL 178.329914 22.826422 \r\nL 179.853256 22.826422 \r\nL 180.157924 28.569208 \r\nL 180.462592 22.826422 \r\nL 181.985934 22.826422 \r\nL 182.290602 28.569208 \r\nL 182.59527 22.826422 \r\nL 184.118612 22.826422 \r\nL 184.42328 45.797565 \r\nL 184.727948 22.826422 \r\nL 185.337285 22.826422 \r\nL 185.641953 40.054779 \r\nL 185.946622 22.826422 \r\nL 189.602641 22.826422 \r\nL 189.90731 28.569208 \r\nL 190.211978 28.569208 \r\nL 190.516646 22.826422 \r\nL 199.352027 22.826422 \r\nL 199.656695 28.569208 \r\nL 199.961364 22.826422 \r\nL 208.187408 22.826422 \r\nL 208.492076 28.569208 \r\nL 208.796744 22.826422 \r\nL 211.234091 22.826422 \r\nL 211.538759 28.569208 \r\nL 211.843428 22.826422 \r\nL 215.194779 22.826422 \r\nL 215.499447 28.569208 \r\nL 215.804115 22.826422 \r\nL 219.460135 22.826422 \r\nL 219.764803 28.569208 \r\nL 220.069472 22.826422 \r\nL 220.678808 22.826422 \r\nL 221.288145 34.311993 \r\nL 221.897482 22.826422 \r\nL 222.811486 22.826422 \r\nL 223.116155 28.569208 \r\nL 223.420823 28.569208 \r\nL 223.725491 22.826422 \r\nL 224.944165 22.826422 \r\nL 225.248833 28.569208 \r\nL 225.553501 28.569208 \r\nL 225.85817 22.826422 \r\nL 233.779545 22.826422 \r\nL 234.084214 28.569208 \r\nL 234.388882 22.826422 \r\nL 250.840971 22.826422 \r\nL 251.145639 28.569208 \r\nL 251.450307 22.826422 \r\nL 253.582985 22.826422 \r\nL 253.887654 28.569208 \r\nL 254.192322 22.826422 \r\nL 258.457678 22.826422 \r\nL 258.762346 28.569208 \r\nL 259.067015 22.826422 \r\nL 279.784459 22.826422 \r\nL 280.089128 28.569208 \r\nL 280.393796 28.569208 \r\nL 280.698464 22.826422 \r\nL 283.135811 22.826422 \r\nL 283.440479 28.569208 \r\nL 283.745147 28.569208 \r\nL 284.049816 22.826422 \r\nL 287.401167 22.826422 \r\nL 287.705835 28.569208 \r\nL 288.010504 22.826422 \r\nL 289.229177 22.826422 \r\nL 289.533845 28.569208 \r\nL 289.838514 22.826422 \r\nL 291.666523 22.826422 \r\nL 291.971192 28.569208 \r\nL 292.27586 28.569208 \r\nL 292.580528 22.826422 \r\nL 303.243919 22.826422 \r\nL 303.548587 28.569208 \r\nL 303.853256 28.569208 \r\nL 304.157924 22.826422 \r\nL 305.071929 22.826422 \r\nL 305.376597 28.569208 \r\nL 305.985934 28.569208 \r\nL 306.290602 22.826422 \r\nL 306.59527 22.826422 \r\nL 306.899939 28.569208 \r\nL 307.204607 22.826422 \r\nL 309.946622 22.826422 \r\nL 310.25129 28.569208 \r\nL 311.165295 28.569208 \r\nL 311.469963 22.826422 \r\nL 313.602641 22.826422 \r\nL 313.90731 28.569208 \r\nL 314.821314 28.569208 \r\nL 315.125983 22.826422 \r\nL 316.649324 22.826422 \r\nL 316.953993 28.569208 \r\nL 318.172666 28.569208 \r\nL 318.477334 22.826422 \r\nL 318.782002 22.826422 \r\nL 319.086671 28.569208 \r\nL 319.391339 22.826422 \r\nL 320.305344 22.826422 \r\nL 320.610012 28.569208 \r\nL 322.133354 28.569208 \r\nL 322.438022 22.826422 \r\nL 323.352027 22.826422 \r\nL 323.656695 28.569208 \r\nL 324.266032 28.569208 \r\nL 324.5707 22.826422 \r\nL 325.789373 22.826422 \r\nL 326.094042 28.569208 \r\nL 327.922052 28.569208 \r\nL 328.22672 22.826422 \r\nL 328.836057 22.826422 \r\nL 329.140725 28.569208 \r\nL 344.37414 28.569208 \r\nL 344.678808 22.826422 \r\nL 345.592813 22.826422 \r\nL 345.897482 28.569208 \r\nL 346.506818 28.569208 \r\nL 346.506818 28.569208 \r\n\" style=\"fill:none;stroke:#2ca02c;stroke-linecap:square;stroke-width:1.5;\"/>\r\n   </g>\r\n   <g id=\"patch_3\">\r\n    <path d=\"M 26.925 224.64 \r\nL 26.925 7.2 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_4\">\r\n    <path d=\"M 361.725 224.64 \r\nL 361.725 7.2 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_5\">\r\n    <path d=\"M 26.925 224.64 \r\nL 361.725 224.64 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_6\">\r\n    <path d=\"M 26.925 7.2 \r\nL 361.725 7.2 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n  </g>\r\n </g>\r\n <defs>\r\n  <clipPath id=\"p9676133514\">\r\n   <rect height=\"217.44\" width=\"334.8\" x=\"26.925\" y=\"7.2\"/>\r\n  </clipPath>\r\n </defs>\r\n</svg>\r\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(stg_history['epoch'], stg_history['valid_acc']/stg_history['# feature'], \n",
    "    label=\"STG\")\n",
    "plt.plot(history['epoch'], history['valid_acc']/history['# feature'], \n",
    "    label=\"No Feature Selection\")\n",
    "plt.plot(two_step_history['epoch'], two_step_history['valid_acc']/two_step_history['# feature'],\n",
    "    label=\"GINI + STG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "stg_history = stg_history.iloc[:500, :]\n",
    "two_step_history = two_step_history.iloc[:500, :]\n",
    "history = history.iloc[:500, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "stg_history.to_csv(\"STGandGINIMotivation/stg_history.csv\")\n",
    "history.to_csv(\"STGandGINIMotivation/nofs_history.csv\")\n",
    "two_step_history.to_csv(\"STGandGINIMotivation/gini_stg_history.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smooth(y, box_pts):\n",
    "    box = np.ones(box_pts)/box_pts\n",
    "    y_smooth = np.convolve(y, box, mode='same')\n",
    "    return y_smooth[:-box_pts+1]\n",
    "y = smooth(history['valid_acc']/history['# feature'], 8)\n",
    "y_stg = smooth(stg_history['valid_acc']/stg_history['# feature'], 8)\n",
    "y_two_step = smooth(two_step_history['valid_acc']/two_step_history['# feature'], 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\r\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\r\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\r\n<svg height=\"262.19625pt\" version=\"1.1\" viewBox=\"0 0 382.603125 262.19625\" width=\"382.603125pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\r\n <metadata>\r\n  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\r\n   <cc:Work>\r\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\r\n    <dc:date>2022-03-29T21:33:48.338030</dc:date>\r\n    <dc:format>image/svg+xml</dc:format>\r\n    <dc:creator>\r\n     <cc:Agent>\r\n      <dc:title>Matplotlib v3.4.3, https://matplotlib.org/</dc:title>\r\n     </cc:Agent>\r\n    </dc:creator>\r\n   </cc:Work>\r\n  </rdf:RDF>\r\n </metadata>\r\n <defs>\r\n  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\r\n </defs>\r\n <g id=\"figure_1\">\r\n  <g id=\"patch_1\">\r\n   <path d=\"M 0 262.19625 \r\nL 382.603125 262.19625 \r\nL 382.603125 0 \r\nL 0 0 \r\nz\r\n\" style=\"fill:none;\"/>\r\n  </g>\r\n  <g id=\"axes_1\">\r\n   <g id=\"patch_2\">\r\n    <path d=\"M 40.603125 224.64 \r\nL 375.403125 224.64 \r\nL 375.403125 7.2 \r\nL 40.603125 7.2 \r\nz\r\n\" style=\"fill:#ffffff;\"/>\r\n   </g>\r\n   <g id=\"matplotlib.axis_1\">\r\n    <g id=\"xtick_1\">\r\n     <g id=\"line2d_1\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL 0 3.5 \r\n\" id=\"mecc2beaf34\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"55.821307\" xlink:href=\"#mecc2beaf34\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_1\">\r\n      <!-- 0 -->\r\n      <g transform=\"translate(52.640057 239.238437)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 2034 4250 \r\nQ 1547 4250 1301 3770 \r\nQ 1056 3291 1056 2328 \r\nQ 1056 1369 1301 889 \r\nQ 1547 409 2034 409 \r\nQ 2525 409 2770 889 \r\nQ 3016 1369 3016 2328 \r\nQ 3016 3291 2770 3770 \r\nQ 2525 4250 2034 4250 \r\nz\r\nM 2034 4750 \r\nQ 2819 4750 3233 4129 \r\nQ 3647 3509 3647 2328 \r\nQ 3647 1150 3233 529 \r\nQ 2819 -91 2034 -91 \r\nQ 1250 -91 836 529 \r\nQ 422 1150 422 2328 \r\nQ 422 3509 836 4129 \r\nQ 1250 4750 2034 4750 \r\nz\r\n\" id=\"DejaVuSans-30\" transform=\"scale(0.015625)\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_2\">\r\n     <g id=\"line2d_2\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"117.683835\" xlink:href=\"#mecc2beaf34\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_2\">\r\n      <!-- 100 -->\r\n      <g transform=\"translate(108.140085 239.238437)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 794 531 \r\nL 1825 531 \r\nL 1825 4091 \r\nL 703 3866 \r\nL 703 4441 \r\nL 1819 4666 \r\nL 2450 4666 \r\nL 2450 531 \r\nL 3481 531 \r\nL 3481 0 \r\nL 794 0 \r\nL 794 531 \r\nz\r\n\" id=\"DejaVuSans-31\" transform=\"scale(0.015625)\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-31\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_3\">\r\n     <g id=\"line2d_3\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"179.546362\" xlink:href=\"#mecc2beaf34\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_3\">\r\n      <!-- 200 -->\r\n      <g transform=\"translate(170.002612 239.238437)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 1228 531 \r\nL 3431 531 \r\nL 3431 0 \r\nL 469 0 \r\nL 469 531 \r\nQ 828 903 1448 1529 \r\nQ 2069 2156 2228 2338 \r\nQ 2531 2678 2651 2914 \r\nQ 2772 3150 2772 3378 \r\nQ 2772 3750 2511 3984 \r\nQ 2250 4219 1831 4219 \r\nQ 1534 4219 1204 4116 \r\nQ 875 4013 500 3803 \r\nL 500 4441 \r\nQ 881 4594 1212 4672 \r\nQ 1544 4750 1819 4750 \r\nQ 2544 4750 2975 4387 \r\nQ 3406 4025 3406 3419 \r\nQ 3406 3131 3298 2873 \r\nQ 3191 2616 2906 2266 \r\nQ 2828 2175 2409 1742 \r\nQ 1991 1309 1228 531 \r\nz\r\n\" id=\"DejaVuSans-32\" transform=\"scale(0.015625)\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-32\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_4\">\r\n     <g id=\"line2d_4\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"241.40889\" xlink:href=\"#mecc2beaf34\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_4\">\r\n      <!-- 300 -->\r\n      <g transform=\"translate(231.86514 239.238437)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 2597 2516 \r\nQ 3050 2419 3304 2112 \r\nQ 3559 1806 3559 1356 \r\nQ 3559 666 3084 287 \r\nQ 2609 -91 1734 -91 \r\nQ 1441 -91 1130 -33 \r\nQ 819 25 488 141 \r\nL 488 750 \r\nQ 750 597 1062 519 \r\nQ 1375 441 1716 441 \r\nQ 2309 441 2620 675 \r\nQ 2931 909 2931 1356 \r\nQ 2931 1769 2642 2001 \r\nQ 2353 2234 1838 2234 \r\nL 1294 2234 \r\nL 1294 2753 \r\nL 1863 2753 \r\nQ 2328 2753 2575 2939 \r\nQ 2822 3125 2822 3475 \r\nQ 2822 3834 2567 4026 \r\nQ 2313 4219 1838 4219 \r\nQ 1578 4219 1281 4162 \r\nQ 984 4106 628 3988 \r\nL 628 4550 \r\nQ 988 4650 1302 4700 \r\nQ 1616 4750 1894 4750 \r\nQ 2613 4750 3031 4423 \r\nQ 3450 4097 3450 3541 \r\nQ 3450 3153 3228 2886 \r\nQ 3006 2619 2597 2516 \r\nz\r\n\" id=\"DejaVuSans-33\" transform=\"scale(0.015625)\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-33\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_5\">\r\n     <g id=\"line2d_5\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"303.271418\" xlink:href=\"#mecc2beaf34\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_5\">\r\n      <!-- 400 -->\r\n      <g transform=\"translate(293.727668 239.238437)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 2419 4116 \r\nL 825 1625 \r\nL 2419 1625 \r\nL 2419 4116 \r\nz\r\nM 2253 4666 \r\nL 3047 4666 \r\nL 3047 1625 \r\nL 3713 1625 \r\nL 3713 1100 \r\nL 3047 1100 \r\nL 3047 0 \r\nL 2419 0 \r\nL 2419 1100 \r\nL 313 1100 \r\nL 313 1709 \r\nL 2253 4666 \r\nz\r\n\" id=\"DejaVuSans-34\" transform=\"scale(0.015625)\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-34\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_6\">\r\n     <g id=\"line2d_6\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"365.133945\" xlink:href=\"#mecc2beaf34\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_6\">\r\n      <!-- 500 -->\r\n      <g transform=\"translate(355.590195 239.238437)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 691 4666 \r\nL 3169 4666 \r\nL 3169 4134 \r\nL 1269 4134 \r\nL 1269 2991 \r\nQ 1406 3038 1543 3061 \r\nQ 1681 3084 1819 3084 \r\nQ 2600 3084 3056 2656 \r\nQ 3513 2228 3513 1497 \r\nQ 3513 744 3044 326 \r\nQ 2575 -91 1722 -91 \r\nQ 1428 -91 1123 -41 \r\nQ 819 9 494 109 \r\nL 494 744 \r\nQ 775 591 1075 516 \r\nQ 1375 441 1709 441 \r\nQ 2250 441 2565 725 \r\nQ 2881 1009 2881 1497 \r\nQ 2881 1984 2565 2268 \r\nQ 2250 2553 1709 2553 \r\nQ 1456 2553 1204 2497 \r\nQ 953 2441 691 2322 \r\nL 691 4666 \r\nz\r\n\" id=\"DejaVuSans-35\" transform=\"scale(0.015625)\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-35\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"text_7\">\r\n     <!-- Number of Epochs -->\r\n     <g transform=\"translate(162.003906 252.916562)scale(0.1 -0.1)\">\r\n      <defs>\r\n       <path d=\"M 628 4666 \r\nL 1478 4666 \r\nL 3547 763 \r\nL 3547 4666 \r\nL 4159 4666 \r\nL 4159 0 \r\nL 3309 0 \r\nL 1241 3903 \r\nL 1241 0 \r\nL 628 0 \r\nL 628 4666 \r\nz\r\n\" id=\"DejaVuSans-4e\" transform=\"scale(0.015625)\"/>\r\n       <path d=\"M 544 1381 \r\nL 544 3500 \r\nL 1119 3500 \r\nL 1119 1403 \r\nQ 1119 906 1312 657 \r\nQ 1506 409 1894 409 \r\nQ 2359 409 2629 706 \r\nQ 2900 1003 2900 1516 \r\nL 2900 3500 \r\nL 3475 3500 \r\nL 3475 0 \r\nL 2900 0 \r\nL 2900 538 \r\nQ 2691 219 2414 64 \r\nQ 2138 -91 1772 -91 \r\nQ 1169 -91 856 284 \r\nQ 544 659 544 1381 \r\nz\r\nM 1991 3584 \r\nL 1991 3584 \r\nz\r\n\" id=\"DejaVuSans-75\" transform=\"scale(0.015625)\"/>\r\n       <path d=\"M 3328 2828 \r\nQ 3544 3216 3844 3400 \r\nQ 4144 3584 4550 3584 \r\nQ 5097 3584 5394 3201 \r\nQ 5691 2819 5691 2113 \r\nL 5691 0 \r\nL 5113 0 \r\nL 5113 2094 \r\nQ 5113 2597 4934 2840 \r\nQ 4756 3084 4391 3084 \r\nQ 3944 3084 3684 2787 \r\nQ 3425 2491 3425 1978 \r\nL 3425 0 \r\nL 2847 0 \r\nL 2847 2094 \r\nQ 2847 2600 2669 2842 \r\nQ 2491 3084 2119 3084 \r\nQ 1678 3084 1418 2786 \r\nQ 1159 2488 1159 1978 \r\nL 1159 0 \r\nL 581 0 \r\nL 581 3500 \r\nL 1159 3500 \r\nL 1159 2956 \r\nQ 1356 3278 1631 3431 \r\nQ 1906 3584 2284 3584 \r\nQ 2666 3584 2933 3390 \r\nQ 3200 3197 3328 2828 \r\nz\r\n\" id=\"DejaVuSans-6d\" transform=\"scale(0.015625)\"/>\r\n       <path d=\"M 3116 1747 \r\nQ 3116 2381 2855 2742 \r\nQ 2594 3103 2138 3103 \r\nQ 1681 3103 1420 2742 \r\nQ 1159 2381 1159 1747 \r\nQ 1159 1113 1420 752 \r\nQ 1681 391 2138 391 \r\nQ 2594 391 2855 752 \r\nQ 3116 1113 3116 1747 \r\nz\r\nM 1159 2969 \r\nQ 1341 3281 1617 3432 \r\nQ 1894 3584 2278 3584 \r\nQ 2916 3584 3314 3078 \r\nQ 3713 2572 3713 1747 \r\nQ 3713 922 3314 415 \r\nQ 2916 -91 2278 -91 \r\nQ 1894 -91 1617 61 \r\nQ 1341 213 1159 525 \r\nL 1159 0 \r\nL 581 0 \r\nL 581 4863 \r\nL 1159 4863 \r\nL 1159 2969 \r\nz\r\n\" id=\"DejaVuSans-62\" transform=\"scale(0.015625)\"/>\r\n       <path d=\"M 3597 1894 \r\nL 3597 1613 \r\nL 953 1613 \r\nQ 991 1019 1311 708 \r\nQ 1631 397 2203 397 \r\nQ 2534 397 2845 478 \r\nQ 3156 559 3463 722 \r\nL 3463 178 \r\nQ 3153 47 2828 -22 \r\nQ 2503 -91 2169 -91 \r\nQ 1331 -91 842 396 \r\nQ 353 884 353 1716 \r\nQ 353 2575 817 3079 \r\nQ 1281 3584 2069 3584 \r\nQ 2775 3584 3186 3129 \r\nQ 3597 2675 3597 1894 \r\nz\r\nM 3022 2063 \r\nQ 3016 2534 2758 2815 \r\nQ 2500 3097 2075 3097 \r\nQ 1594 3097 1305 2825 \r\nQ 1016 2553 972 2059 \r\nL 3022 2063 \r\nz\r\n\" id=\"DejaVuSans-65\" transform=\"scale(0.015625)\"/>\r\n       <path d=\"M 2631 2963 \r\nQ 2534 3019 2420 3045 \r\nQ 2306 3072 2169 3072 \r\nQ 1681 3072 1420 2755 \r\nQ 1159 2438 1159 1844 \r\nL 1159 0 \r\nL 581 0 \r\nL 581 3500 \r\nL 1159 3500 \r\nL 1159 2956 \r\nQ 1341 3275 1631 3429 \r\nQ 1922 3584 2338 3584 \r\nQ 2397 3584 2469 3576 \r\nQ 2541 3569 2628 3553 \r\nL 2631 2963 \r\nz\r\n\" id=\"DejaVuSans-72\" transform=\"scale(0.015625)\"/>\r\n       <path id=\"DejaVuSans-20\" transform=\"scale(0.015625)\"/>\r\n       <path d=\"M 1959 3097 \r\nQ 1497 3097 1228 2736 \r\nQ 959 2375 959 1747 \r\nQ 959 1119 1226 758 \r\nQ 1494 397 1959 397 \r\nQ 2419 397 2687 759 \r\nQ 2956 1122 2956 1747 \r\nQ 2956 2369 2687 2733 \r\nQ 2419 3097 1959 3097 \r\nz\r\nM 1959 3584 \r\nQ 2709 3584 3137 3096 \r\nQ 3566 2609 3566 1747 \r\nQ 3566 888 3137 398 \r\nQ 2709 -91 1959 -91 \r\nQ 1206 -91 779 398 \r\nQ 353 888 353 1747 \r\nQ 353 2609 779 3096 \r\nQ 1206 3584 1959 3584 \r\nz\r\n\" id=\"DejaVuSans-6f\" transform=\"scale(0.015625)\"/>\r\n       <path d=\"M 2375 4863 \r\nL 2375 4384 \r\nL 1825 4384 \r\nQ 1516 4384 1395 4259 \r\nQ 1275 4134 1275 3809 \r\nL 1275 3500 \r\nL 2222 3500 \r\nL 2222 3053 \r\nL 1275 3053 \r\nL 1275 0 \r\nL 697 0 \r\nL 697 3053 \r\nL 147 3053 \r\nL 147 3500 \r\nL 697 3500 \r\nL 697 3744 \r\nQ 697 4328 969 4595 \r\nQ 1241 4863 1831 4863 \r\nL 2375 4863 \r\nz\r\n\" id=\"DejaVuSans-66\" transform=\"scale(0.015625)\"/>\r\n       <path d=\"M 628 4666 \r\nL 3578 4666 \r\nL 3578 4134 \r\nL 1259 4134 \r\nL 1259 2753 \r\nL 3481 2753 \r\nL 3481 2222 \r\nL 1259 2222 \r\nL 1259 531 \r\nL 3634 531 \r\nL 3634 0 \r\nL 628 0 \r\nL 628 4666 \r\nz\r\n\" id=\"DejaVuSans-45\" transform=\"scale(0.015625)\"/>\r\n       <path d=\"M 1159 525 \r\nL 1159 -1331 \r\nL 581 -1331 \r\nL 581 3500 \r\nL 1159 3500 \r\nL 1159 2969 \r\nQ 1341 3281 1617 3432 \r\nQ 1894 3584 2278 3584 \r\nQ 2916 3584 3314 3078 \r\nQ 3713 2572 3713 1747 \r\nQ 3713 922 3314 415 \r\nQ 2916 -91 2278 -91 \r\nQ 1894 -91 1617 61 \r\nQ 1341 213 1159 525 \r\nz\r\nM 3116 1747 \r\nQ 3116 2381 2855 2742 \r\nQ 2594 3103 2138 3103 \r\nQ 1681 3103 1420 2742 \r\nQ 1159 2381 1159 1747 \r\nQ 1159 1113 1420 752 \r\nQ 1681 391 2138 391 \r\nQ 2594 391 2855 752 \r\nQ 3116 1113 3116 1747 \r\nz\r\n\" id=\"DejaVuSans-70\" transform=\"scale(0.015625)\"/>\r\n       <path d=\"M 3122 3366 \r\nL 3122 2828 \r\nQ 2878 2963 2633 3030 \r\nQ 2388 3097 2138 3097 \r\nQ 1578 3097 1268 2742 \r\nQ 959 2388 959 1747 \r\nQ 959 1106 1268 751 \r\nQ 1578 397 2138 397 \r\nQ 2388 397 2633 464 \r\nQ 2878 531 3122 666 \r\nL 3122 134 \r\nQ 2881 22 2623 -34 \r\nQ 2366 -91 2075 -91 \r\nQ 1284 -91 818 406 \r\nQ 353 903 353 1747 \r\nQ 353 2603 823 3093 \r\nQ 1294 3584 2113 3584 \r\nQ 2378 3584 2631 3529 \r\nQ 2884 3475 3122 3366 \r\nz\r\n\" id=\"DejaVuSans-63\" transform=\"scale(0.015625)\"/>\r\n       <path d=\"M 3513 2113 \r\nL 3513 0 \r\nL 2938 0 \r\nL 2938 2094 \r\nQ 2938 2591 2744 2837 \r\nQ 2550 3084 2163 3084 \r\nQ 1697 3084 1428 2787 \r\nQ 1159 2491 1159 1978 \r\nL 1159 0 \r\nL 581 0 \r\nL 581 4863 \r\nL 1159 4863 \r\nL 1159 2956 \r\nQ 1366 3272 1645 3428 \r\nQ 1925 3584 2291 3584 \r\nQ 2894 3584 3203 3211 \r\nQ 3513 2838 3513 2113 \r\nz\r\n\" id=\"DejaVuSans-68\" transform=\"scale(0.015625)\"/>\r\n       <path d=\"M 2834 3397 \r\nL 2834 2853 \r\nQ 2591 2978 2328 3040 \r\nQ 2066 3103 1784 3103 \r\nQ 1356 3103 1142 2972 \r\nQ 928 2841 928 2578 \r\nQ 928 2378 1081 2264 \r\nQ 1234 2150 1697 2047 \r\nL 1894 2003 \r\nQ 2506 1872 2764 1633 \r\nQ 3022 1394 3022 966 \r\nQ 3022 478 2636 193 \r\nQ 2250 -91 1575 -91 \r\nQ 1294 -91 989 -36 \r\nQ 684 19 347 128 \r\nL 347 722 \r\nQ 666 556 975 473 \r\nQ 1284 391 1588 391 \r\nQ 1994 391 2212 530 \r\nQ 2431 669 2431 922 \r\nQ 2431 1156 2273 1281 \r\nQ 2116 1406 1581 1522 \r\nL 1381 1569 \r\nQ 847 1681 609 1914 \r\nQ 372 2147 372 2553 \r\nQ 372 3047 722 3315 \r\nQ 1072 3584 1716 3584 \r\nQ 2034 3584 2315 3537 \r\nQ 2597 3491 2834 3397 \r\nz\r\n\" id=\"DejaVuSans-73\" transform=\"scale(0.015625)\"/>\r\n      </defs>\r\n      <use xlink:href=\"#DejaVuSans-4e\"/>\r\n      <use x=\"74.804688\" xlink:href=\"#DejaVuSans-75\"/>\r\n      <use x=\"138.183594\" xlink:href=\"#DejaVuSans-6d\"/>\r\n      <use x=\"235.595703\" xlink:href=\"#DejaVuSans-62\"/>\r\n      <use x=\"299.072266\" xlink:href=\"#DejaVuSans-65\"/>\r\n      <use x=\"360.595703\" xlink:href=\"#DejaVuSans-72\"/>\r\n      <use x=\"401.708984\" xlink:href=\"#DejaVuSans-20\"/>\r\n      <use x=\"433.496094\" xlink:href=\"#DejaVuSans-6f\"/>\r\n      <use x=\"494.677734\" xlink:href=\"#DejaVuSans-66\"/>\r\n      <use x=\"529.882812\" xlink:href=\"#DejaVuSans-20\"/>\r\n      <use x=\"561.669922\" xlink:href=\"#DejaVuSans-45\"/>\r\n      <use x=\"624.853516\" xlink:href=\"#DejaVuSans-70\"/>\r\n      <use x=\"688.330078\" xlink:href=\"#DejaVuSans-6f\"/>\r\n      <use x=\"749.511719\" xlink:href=\"#DejaVuSans-63\"/>\r\n      <use x=\"804.492188\" xlink:href=\"#DejaVuSans-68\"/>\r\n      <use x=\"867.871094\" xlink:href=\"#DejaVuSans-73\"/>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"matplotlib.axis_2\">\r\n    <g id=\"ytick_1\">\r\n     <g id=\"line2d_7\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL -3.5 0 \r\n\" id=\"m7e88ec9cf6\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"40.603125\" xlink:href=\"#m7e88ec9cf6\" y=\"219.268581\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_8\">\r\n      <!-- 0 -->\r\n      <g transform=\"translate(27.240625 223.0678)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_2\">\r\n     <g id=\"line2d_8\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"40.603125\" xlink:href=\"#m7e88ec9cf6\" y=\"194.577727\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_9\">\r\n      <!-- 2 -->\r\n      <g transform=\"translate(27.240625 198.376946)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-32\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_3\">\r\n     <g id=\"line2d_9\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"40.603125\" xlink:href=\"#m7e88ec9cf6\" y=\"169.886873\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_10\">\r\n      <!-- 4 -->\r\n      <g transform=\"translate(27.240625 173.686092)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-34\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_4\">\r\n     <g id=\"line2d_10\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"40.603125\" xlink:href=\"#m7e88ec9cf6\" y=\"145.196019\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_11\">\r\n      <!-- 6 -->\r\n      <g transform=\"translate(27.240625 148.995238)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 2113 2584 \r\nQ 1688 2584 1439 2293 \r\nQ 1191 2003 1191 1497 \r\nQ 1191 994 1439 701 \r\nQ 1688 409 2113 409 \r\nQ 2538 409 2786 701 \r\nQ 3034 994 3034 1497 \r\nQ 3034 2003 2786 2293 \r\nQ 2538 2584 2113 2584 \r\nz\r\nM 3366 4563 \r\nL 3366 3988 \r\nQ 3128 4100 2886 4159 \r\nQ 2644 4219 2406 4219 \r\nQ 1781 4219 1451 3797 \r\nQ 1122 3375 1075 2522 \r\nQ 1259 2794 1537 2939 \r\nQ 1816 3084 2150 3084 \r\nQ 2853 3084 3261 2657 \r\nQ 3669 2231 3669 1497 \r\nQ 3669 778 3244 343 \r\nQ 2819 -91 2113 -91 \r\nQ 1303 -91 875 529 \r\nQ 447 1150 447 2328 \r\nQ 447 3434 972 4092 \r\nQ 1497 4750 2381 4750 \r\nQ 2619 4750 2861 4703 \r\nQ 3103 4656 3366 4563 \r\nz\r\n\" id=\"DejaVuSans-36\" transform=\"scale(0.015625)\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-36\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_5\">\r\n     <g id=\"line2d_11\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"40.603125\" xlink:href=\"#m7e88ec9cf6\" y=\"120.505165\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_12\">\r\n      <!-- 8 -->\r\n      <g transform=\"translate(27.240625 124.304384)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 2034 2216 \r\nQ 1584 2216 1326 1975 \r\nQ 1069 1734 1069 1313 \r\nQ 1069 891 1326 650 \r\nQ 1584 409 2034 409 \r\nQ 2484 409 2743 651 \r\nQ 3003 894 3003 1313 \r\nQ 3003 1734 2745 1975 \r\nQ 2488 2216 2034 2216 \r\nz\r\nM 1403 2484 \r\nQ 997 2584 770 2862 \r\nQ 544 3141 544 3541 \r\nQ 544 4100 942 4425 \r\nQ 1341 4750 2034 4750 \r\nQ 2731 4750 3128 4425 \r\nQ 3525 4100 3525 3541 \r\nQ 3525 3141 3298 2862 \r\nQ 3072 2584 2669 2484 \r\nQ 3125 2378 3379 2068 \r\nQ 3634 1759 3634 1313 \r\nQ 3634 634 3220 271 \r\nQ 2806 -91 2034 -91 \r\nQ 1263 -91 848 271 \r\nQ 434 634 434 1313 \r\nQ 434 1759 690 2068 \r\nQ 947 2378 1403 2484 \r\nz\r\nM 1172 3481 \r\nQ 1172 3119 1398 2916 \r\nQ 1625 2713 2034 2713 \r\nQ 2441 2713 2670 2916 \r\nQ 2900 3119 2900 3481 \r\nQ 2900 3844 2670 4047 \r\nQ 2441 4250 2034 4250 \r\nQ 1625 4250 1398 4047 \r\nQ 1172 3844 1172 3481 \r\nz\r\n\" id=\"DejaVuSans-38\" transform=\"scale(0.015625)\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-38\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_6\">\r\n     <g id=\"line2d_12\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"40.603125\" xlink:href=\"#m7e88ec9cf6\" y=\"95.814311\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_13\">\r\n      <!-- 10 -->\r\n      <g transform=\"translate(20.878125 99.61353)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-31\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_7\">\r\n     <g id=\"line2d_13\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"40.603125\" xlink:href=\"#m7e88ec9cf6\" y=\"71.123457\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_14\">\r\n      <!-- 12 -->\r\n      <g transform=\"translate(20.878125 74.922676)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-31\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-32\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_8\">\r\n     <g id=\"line2d_14\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"40.603125\" xlink:href=\"#m7e88ec9cf6\" y=\"46.432603\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_15\">\r\n      <!-- 14 -->\r\n      <g transform=\"translate(20.878125 50.231822)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-31\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-34\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_9\">\r\n     <g id=\"line2d_15\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"40.603125\" xlink:href=\"#m7e88ec9cf6\" y=\"21.741749\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_16\">\r\n      <!-- 16 -->\r\n      <g transform=\"translate(20.878125 25.540968)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-31\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-36\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"text_17\">\r\n     <!-- Accuracy/# Features Ratio -->\r\n     <g transform=\"translate(14.798438 182.129375)rotate(-90)scale(0.1 -0.1)\">\r\n      <defs>\r\n       <path d=\"M 2188 4044 \r\nL 1331 1722 \r\nL 3047 1722 \r\nL 2188 4044 \r\nz\r\nM 1831 4666 \r\nL 2547 4666 \r\nL 4325 0 \r\nL 3669 0 \r\nL 3244 1197 \r\nL 1141 1197 \r\nL 716 0 \r\nL 50 0 \r\nL 1831 4666 \r\nz\r\n\" id=\"DejaVuSans-41\" transform=\"scale(0.015625)\"/>\r\n       <path d=\"M 2194 1759 \r\nQ 1497 1759 1228 1600 \r\nQ 959 1441 959 1056 \r\nQ 959 750 1161 570 \r\nQ 1363 391 1709 391 \r\nQ 2188 391 2477 730 \r\nQ 2766 1069 2766 1631 \r\nL 2766 1759 \r\nL 2194 1759 \r\nz\r\nM 3341 1997 \r\nL 3341 0 \r\nL 2766 0 \r\nL 2766 531 \r\nQ 2569 213 2275 61 \r\nQ 1981 -91 1556 -91 \r\nQ 1019 -91 701 211 \r\nQ 384 513 384 1019 \r\nQ 384 1609 779 1909 \r\nQ 1175 2209 1959 2209 \r\nL 2766 2209 \r\nL 2766 2266 \r\nQ 2766 2663 2505 2880 \r\nQ 2244 3097 1772 3097 \r\nQ 1472 3097 1187 3025 \r\nQ 903 2953 641 2809 \r\nL 641 3341 \r\nQ 956 3463 1253 3523 \r\nQ 1550 3584 1831 3584 \r\nQ 2591 3584 2966 3190 \r\nQ 3341 2797 3341 1997 \r\nz\r\n\" id=\"DejaVuSans-61\" transform=\"scale(0.015625)\"/>\r\n       <path d=\"M 2059 -325 \r\nQ 1816 -950 1584 -1140 \r\nQ 1353 -1331 966 -1331 \r\nL 506 -1331 \r\nL 506 -850 \r\nL 844 -850 \r\nQ 1081 -850 1212 -737 \r\nQ 1344 -625 1503 -206 \r\nL 1606 56 \r\nL 191 3500 \r\nL 800 3500 \r\nL 1894 763 \r\nL 2988 3500 \r\nL 3597 3500 \r\nL 2059 -325 \r\nz\r\n\" id=\"DejaVuSans-79\" transform=\"scale(0.015625)\"/>\r\n       <path d=\"M 1625 4666 \r\nL 2156 4666 \r\nL 531 -594 \r\nL 0 -594 \r\nL 1625 4666 \r\nz\r\n\" id=\"DejaVuSans-2f\" transform=\"scale(0.015625)\"/>\r\n       <path d=\"M 3272 2816 \r\nL 2363 2816 \r\nL 2100 1772 \r\nL 3016 1772 \r\nL 3272 2816 \r\nz\r\nM 2803 4594 \r\nL 2478 3297 \r\nL 3391 3297 \r\nL 3719 4594 \r\nL 4219 4594 \r\nL 3897 3297 \r\nL 4872 3297 \r\nL 4872 2816 \r\nL 3775 2816 \r\nL 3519 1772 \r\nL 4513 1772 \r\nL 4513 1294 \r\nL 3397 1294 \r\nL 3072 0 \r\nL 2572 0 \r\nL 2894 1294 \r\nL 1978 1294 \r\nL 1656 0 \r\nL 1153 0 \r\nL 1478 1294 \r\nL 494 1294 \r\nL 494 1772 \r\nL 1594 1772 \r\nL 1856 2816 \r\nL 850 2816 \r\nL 850 3297 \r\nL 1978 3297 \r\nL 2297 4594 \r\nL 2803 4594 \r\nz\r\n\" id=\"DejaVuSans-23\" transform=\"scale(0.015625)\"/>\r\n       <path d=\"M 628 4666 \r\nL 3309 4666 \r\nL 3309 4134 \r\nL 1259 4134 \r\nL 1259 2759 \r\nL 3109 2759 \r\nL 3109 2228 \r\nL 1259 2228 \r\nL 1259 0 \r\nL 628 0 \r\nL 628 4666 \r\nz\r\n\" id=\"DejaVuSans-46\" transform=\"scale(0.015625)\"/>\r\n       <path d=\"M 1172 4494 \r\nL 1172 3500 \r\nL 2356 3500 \r\nL 2356 3053 \r\nL 1172 3053 \r\nL 1172 1153 \r\nQ 1172 725 1289 603 \r\nQ 1406 481 1766 481 \r\nL 2356 481 \r\nL 2356 0 \r\nL 1766 0 \r\nQ 1100 0 847 248 \r\nQ 594 497 594 1153 \r\nL 594 3053 \r\nL 172 3053 \r\nL 172 3500 \r\nL 594 3500 \r\nL 594 4494 \r\nL 1172 4494 \r\nz\r\n\" id=\"DejaVuSans-74\" transform=\"scale(0.015625)\"/>\r\n       <path d=\"M 2841 2188 \r\nQ 3044 2119 3236 1894 \r\nQ 3428 1669 3622 1275 \r\nL 4263 0 \r\nL 3584 0 \r\nL 2988 1197 \r\nQ 2756 1666 2539 1819 \r\nQ 2322 1972 1947 1972 \r\nL 1259 1972 \r\nL 1259 0 \r\nL 628 0 \r\nL 628 4666 \r\nL 2053 4666 \r\nQ 2853 4666 3247 4331 \r\nQ 3641 3997 3641 3322 \r\nQ 3641 2881 3436 2590 \r\nQ 3231 2300 2841 2188 \r\nz\r\nM 1259 4147 \r\nL 1259 2491 \r\nL 2053 2491 \r\nQ 2509 2491 2742 2702 \r\nQ 2975 2913 2975 3322 \r\nQ 2975 3731 2742 3939 \r\nQ 2509 4147 2053 4147 \r\nL 1259 4147 \r\nz\r\n\" id=\"DejaVuSans-52\" transform=\"scale(0.015625)\"/>\r\n       <path d=\"M 603 3500 \r\nL 1178 3500 \r\nL 1178 0 \r\nL 603 0 \r\nL 603 3500 \r\nz\r\nM 603 4863 \r\nL 1178 4863 \r\nL 1178 4134 \r\nL 603 4134 \r\nL 603 4863 \r\nz\r\n\" id=\"DejaVuSans-69\" transform=\"scale(0.015625)\"/>\r\n      </defs>\r\n      <use xlink:href=\"#DejaVuSans-41\"/>\r\n      <use x=\"66.658203\" xlink:href=\"#DejaVuSans-63\"/>\r\n      <use x=\"121.638672\" xlink:href=\"#DejaVuSans-63\"/>\r\n      <use x=\"176.619141\" xlink:href=\"#DejaVuSans-75\"/>\r\n      <use x=\"239.998047\" xlink:href=\"#DejaVuSans-72\"/>\r\n      <use x=\"281.111328\" xlink:href=\"#DejaVuSans-61\"/>\r\n      <use x=\"342.390625\" xlink:href=\"#DejaVuSans-63\"/>\r\n      <use x=\"397.371094\" xlink:href=\"#DejaVuSans-79\"/>\r\n      <use x=\"456.550781\" xlink:href=\"#DejaVuSans-2f\"/>\r\n      <use x=\"490.242188\" xlink:href=\"#DejaVuSans-23\"/>\r\n      <use x=\"574.03125\" xlink:href=\"#DejaVuSans-20\"/>\r\n      <use x=\"605.818359\" xlink:href=\"#DejaVuSans-46\"/>\r\n      <use x=\"657.837891\" xlink:href=\"#DejaVuSans-65\"/>\r\n      <use x=\"719.361328\" xlink:href=\"#DejaVuSans-61\"/>\r\n      <use x=\"780.640625\" xlink:href=\"#DejaVuSans-74\"/>\r\n      <use x=\"819.849609\" xlink:href=\"#DejaVuSans-75\"/>\r\n      <use x=\"883.228516\" xlink:href=\"#DejaVuSans-72\"/>\r\n      <use x=\"922.091797\" xlink:href=\"#DejaVuSans-65\"/>\r\n      <use x=\"983.615234\" xlink:href=\"#DejaVuSans-73\"/>\r\n      <use x=\"1035.714844\" xlink:href=\"#DejaVuSans-20\"/>\r\n      <use x=\"1067.501953\" xlink:href=\"#DejaVuSans-52\"/>\r\n      <use x=\"1134.734375\" xlink:href=\"#DejaVuSans-61\"/>\r\n      <use x=\"1196.013672\" xlink:href=\"#DejaVuSans-74\"/>\r\n      <use x=\"1235.222656\" xlink:href=\"#DejaVuSans-69\"/>\r\n      <use x=\"1263.005859\" xlink:href=\"#DejaVuSans-6f\"/>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"line2d_16\">\r\n    <path clip-path=\"url(#pd45b93ff69)\" d=\"M 55.821307 213.85392 \r\nL 58.295808 208.439259 \r\nL 59.533058 208.100843 \r\nL 60.770309 207.311205 \r\nL 62.00756 206.295956 \r\nL 63.863435 203.814236 \r\nL 65.100686 202.122155 \r\nL 66.956562 200.204462 \r\nL 68.193812 199.52763 \r\nL 69.431063 199.414824 \r\nL 70.049688 199.414824 \r\nL 70.668313 199.189213 \r\nL 71.286939 198.28677 \r\nL 72.524189 195.80505 \r\nL 75.617316 189.262335 \r\nL 76.235941 188.811113 \r\nL 76.854566 188.585502 \r\nL 360.184943 188.585502 \r\nL 360.184943 188.585502 \r\n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\r\n   </g>\r\n   <g id=\"line2d_17\">\r\n    <path clip-path=\"url(#pd45b93ff69)\" d=\"M 55.821307 214.756364 \r\nL 58.295808 210.244146 \r\nL 69.431063 210.244146 \r\nL 70.049688 209.90573 \r\nL 70.668313 209.341703 \r\nL 71.905564 209.228897 \r\nL 72.524189 207.762427 \r\nL 73.142815 207.762427 \r\nL 73.76144 206.859983 \r\nL 74.380065 206.634372 \r\nL 74.99869 205.393512 \r\nL 75.617316 205.055096 \r\nL 76.854566 201.783738 \r\nL 77.473192 201.558128 \r\nL 78.091817 199.866046 \r\nL 78.710442 199.076408 \r\nL 79.329067 197.384326 \r\nL 79.947693 197.271521 \r\nL 80.566318 197.384326 \r\nL 81.184943 196.933105 \r\nL 82.422194 195.692245 \r\nL 83.040819 195.241023 \r\nL 83.659444 195.241023 \r\nL 84.896695 194.33858 \r\nL 85.51532 193.09772 \r\nL 87.371196 195.015412 \r\nL 87.989821 195.015412 \r\nL 88.608447 194.56419 \r\nL 89.227072 195.015412 \r\nL 89.845697 195.128218 \r\nL 90.464322 194.902607 \r\nL 91.701573 194.000163 \r\nL 92.938823 193.436136 \r\nL 94.176074 193.548941 \r\nL 96.03195 192.308082 \r\nL 96.650575 191.969665 \r\nL 97.2692 192.082471 \r\nL 97.887826 193.09772 \r\nL 98.506451 192.533693 \r\nL 99.125076 192.308082 \r\nL 100.362327 193.09772 \r\nL 101.599577 193.774552 \r\nL 102.218203 193.661747 \r\nL 102.836828 192.984914 \r\nL 104.074078 193.353819 \r\nL 104.692704 192.92089 \r\nL 105.311329 193.987968 \r\nL 105.929954 193.439185 \r\nL 106.54858 193.817236 \r\nL 107.167205 193.268452 \r\nL 107.78583 193.180037 \r\nL 108.404455 193.317233 \r\nL 109.023081 193.230342 \r\nL 109.641706 193.487796 \r\nL 110.260331 192.113636 \r\nL 110.878956 192.164748 \r\nL 111.497582 191.148541 \r\nL 112.116207 191.181643 \r\nL 112.734832 190.633039 \r\nL 113.972083 188.886706 \r\nL 116.446584 186.592893 \r\nL 117.065209 186.370213 \r\nL 117.683835 185.590113 \r\nL 118.30246 185.188244 \r\nL 118.921085 184.936091 \r\nL 119.53971 184.423904 \r\nL 120.158336 184.431784 \r\nL 120.776961 184.159812 \r\nL 121.395586 184.829594 \r\nL 122.014211 184.975728 \r\nL 122.632837 184.962765 \r\nL 123.251462 185.779467 \r\nL 124.488713 185.321464 \r\nL 125.107338 186.259159 \r\nL 125.725963 187.454844 \r\nL 126.344588 186.943825 \r\nL 126.963214 185.489441 \r\nL 127.581839 185.904274 \r\nL 128.200464 185.642534 \r\nL 128.81909 185.775214 \r\nL 129.437715 185.490145 \r\nL 130.674965 183.920305 \r\nL 131.293591 182.967726 \r\nL 131.912216 182.774942 \r\nL 132.530841 182.298652 \r\nL 133.149466 181.321212 \r\nL 133.768092 180.649957 \r\nL 134.386717 179.698035 \r\nL 135.005342 178.030693 \r\nL 135.623968 177.001907 \r\nL 136.242593 176.252545 \r\nL 136.861218 176.67803 \r\nL 137.479843 176.125534 \r\nL 138.098469 175.921096 \r\nL 139.335719 175.17462 \r\nL 139.954345 174.792722 \r\nL 141.191595 173.467771 \r\nL 142.428846 170.700961 \r\nL 143.047471 169.391598 \r\nL 144.284721 167.47394 \r\nL 145.521972 165.837236 \r\nL 146.140597 165.273209 \r\nL 147.996473 161.71471 \r\nL 148.615098 160.575912 \r\nL 149.233724 159.978043 \r\nL 149.852349 159.165844 \r\nL 150.470974 159.256089 \r\nL 151.0896 159.030478 \r\nL 151.708225 159.030478 \r\nL 152.32685 159.4817 \r\nL 152.945475 160.94817 \r\nL 153.564101 161.117378 \r\nL 154.182726 161.500917 \r\nL 154.801351 162.313116 \r\nL 155.419976 162.008541 \r\nL 156.038602 161.940858 \r\nL 156.657227 162.753057 \r\nL 157.275852 163.757026 \r\nL 157.894478 163.757026 \r\nL 158.513103 165.900329 \r\nL 159.131728 166.75765 \r\nL 159.750353 166.32899 \r\nL 160.368979 166.114659 \r\nL 160.987604 166.407953 \r\nL 161.606229 166.272587 \r\nL 162.84348 164.557944 \r\nL 163.462105 163.271962 \r\nL 164.08073 162.628971 \r\nL 164.699356 162.922265 \r\nL 165.317981 162.764338 \r\nL 166.555231 161.410673 \r\nL 167.173857 161.489636 \r\nL 167.792482 161.106098 \r\nL 169.029733 159.256089 \r\nL 169.648358 158.804867 \r\nL 170.266983 159.70731 \r\nL 170.885608 159.70731 \r\nL 171.504234 159.932921 \r\nL 172.122859 159.256089 \r\nL 172.741484 159.256089 \r\nL 173.360109 159.4817 \r\nL 173.978735 159.519301 \r\nL 174.59736 158.391247 \r\nL 175.215985 156.108596 \r\nL 175.834611 155.206152 \r\nL 176.453236 154.834558 \r\nL 177.690486 153.029671 \r\nL 178.927737 152.211555 \r\nL 179.546362 152.211555 \r\nL 180.164988 152.463708 \r\nL 180.783613 151.927883 \r\nL 181.402238 150.635597 \r\nL 182.639489 149.563945 \r\nL 183.258114 148.271659 \r\nL 185.11399 146.664182 \r\nL 185.732615 146.664182 \r\nL 187.588491 145.860443 \r\nL 188.207116 146.128356 \r\nL 189.444367 146.128356 \r\nL 190.062992 146.396269 \r\nL 190.681617 145.789 \r\nL 191.918868 145.110287 \r\nL 192.537493 144.076909 \r\nL 193.774744 140.632314 \r\nL 194.393369 139.331023 \r\nL 195.011994 137.761818 \r\nL 196.86787 135.679752 \r\nL 198.105121 135.679752 \r\nL 198.723746 135.06738 \r\nL 199.342371 135.373566 \r\nL 201.198247 135.373566 \r\nL 201.816872 135.679752 \r\nL 202.435498 135.679752 \r\nL 203.054123 135.985939 \r\nL 203.672748 136.598311 \r\nL 204.291373 135.161591 \r\nL 204.909999 134.031058 \r\nL 205.528624 131.581568 \r\nL 207.3845 126.427434 \r\nL 208.62175 120.749072 \r\nL 209.859001 116.162703 \r\nL 210.477626 115.38332 \r\nL 211.096251 114.279194 \r\nL 211.714877 112.81785 \r\nL 212.333502 111.713725 \r\nL 212.952127 112.103416 \r\nL 214.189378 113.662182 \r\nL 214.808003 113.116614 \r\nL 215.426628 112.181354 \r\nL 216.045254 110.856403 \r\nL 216.663879 108.674131 \r\nL 217.282504 106.920519 \r\nL 218.519755 102.672882 \r\nL 219.13838 100.958239 \r\nL 220.994256 100.958239 \r\nL 221.612881 101.81556 \r\nL 222.850132 102.672882 \r\nL 223.468757 102.672882 \r\nL 224.087382 102.244221 \r\nL 230.273635 102.244221 \r\nL 233.985387 92.480284 \r\nL 234.604012 90.384609 \r\nL 235.222637 88.765225 \r\nL 236.459888 87.812645 \r\nL 238.315764 87.812645 \r\nL 239.553014 88.765225 \r\nL 240.171639 88.765225 \r\nL 241.40889 89.717804 \r\nL 246.357892 89.717804 \r\nL 246.976517 89.241514 \r\nL 250.069644 89.241514 \r\nL 250.688269 88.765225 \r\nL 251.92552 88.765225 \r\nL 252.544145 88.288935 \r\nL 255.018646 88.288935 \r\nL 255.637271 88.765225 \r\nL 256.255897 88.288935 \r\nL 257.493147 89.241514 \r\nL 258.111772 88.288935 \r\nL 259.349023 88.288935 \r\nL 259.967648 87.812645 \r\nL 260.586274 87.812645 \r\nL 261.204899 88.288935 \r\nL 262.442149 88.288935 \r\nL 263.060775 89.241514 \r\nL 264.91665 89.241514 \r\nL 265.535276 88.765225 \r\nL 269.865653 88.765225 \r\nL 270.484278 89.241514 \r\nL 271.721529 89.241514 \r\nL 274.19603 81.144591 \r\nL 274.814655 79.596649 \r\nL 276.051905 74.476536 \r\nL 276.670531 72.452305 \r\nL 279.763657 72.452305 \r\nL 281.000908 73.523957 \r\nL 289.043036 73.523957 \r\nL 289.661662 72.988131 \r\nL 293.373413 72.988131 \r\nL 293.992039 72.452305 \r\nL 294.610664 72.988131 \r\nL 298.322415 72.988131 \r\nL 298.941041 73.523957 \r\nL 307.601795 73.523957 \r\nL 308.22042 72.988131 \r\nL 312.550797 72.988131 \r\nL 313.169422 73.523957 \r\nL 360.184943 73.523957 \r\nL 360.184943 73.523957 \r\n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\r\n   </g>\r\n   <g id=\"line2d_18\">\r\n    <path clip-path=\"url(#pd45b93ff69)\" d=\"M 55.821307 210.244146 \r\nL 58.295808 201.219711 \r\nL 72.524189 201.219711 \r\nL 73.142815 198.28677 \r\nL 73.76144 198.28677 \r\nL 74.99869 194.676996 \r\nL 76.235941 188.811113 \r\nL 77.473192 182.494009 \r\nL 78.091817 182.268398 \r\nL 78.710442 179.109846 \r\nL 79.947693 176.402515 \r\nL 80.566318 175.951293 \r\nL 81.803568 175.951293 \r\nL 82.422194 175.274461 \r\nL 83.040819 174.823239 \r\nL 83.659444 174.823239 \r\nL 84.27807 177.079348 \r\nL 84.896695 180.2379 \r\nL 85.51532 180.2379 \r\nL 86.133945 180.012289 \r\nL 86.752571 179.561067 \r\nL 87.371196 180.012289 \r\nL 87.989821 180.012289 \r\nL 88.608447 179.786678 \r\nL 89.227072 176.402515 \r\nL 89.845697 171.664687 \r\nL 90.464322 171.439076 \r\nL 91.082948 171.439076 \r\nL 91.701573 171.890298 \r\nL 92.320198 172.115908 \r\nL 92.938823 172.56713 \r\nL 93.557449 172.56713 \r\nL 94.176074 172.341519 \r\nL 95.413325 172.341519 \r\nL 96.03195 170.987854 \r\nL 96.650575 170.762243 \r\nL 97.887826 169.308307 \r\nL 98.506451 169.233103 \r\nL 99.743701 171.32627 \r\nL 100.362327 171.012922 \r\nL 100.980952 172.053239 \r\nL 101.599577 169.120297 \r\nL 102.218203 167.829302 \r\nL 102.836828 165.924143 \r\nL 104.074078 160.446813 \r\nL 105.311329 156.398351 \r\nL 105.929954 155.207627 \r\nL 106.54858 156.874641 \r\nL 107.167205 156.160206 \r\nL 107.78583 155.922061 \r\nL 109.023081 157.35093 \r\nL 109.641706 157.112785 \r\nL 110.260331 158.065365 \r\nL 110.878956 158.065365 \r\nL 111.497582 157.589075 \r\nL 112.116207 158.065365 \r\nL 112.734832 158.779799 \r\nL 113.353458 160.446813 \r\nL 113.972083 161.259307 \r\nL 114.590708 163.052397 \r\nL 115.209333 163.640755 \r\nL 115.827959 163.220499 \r\nL 116.446584 162.281929 \r\nL 117.065209 163.094423 \r\nL 117.683835 162.393997 \r\nL 118.30246 158.869454 \r\nL 118.921085 158.247475 \r\nL 119.53971 155.648356 \r\nL 120.158336 152.312174 \r\nL 120.776961 147.731388 \r\nL 121.395586 144.621497 \r\nL 122.632837 133.608892 \r\nL 123.251462 130.855072 \r\nL 123.870087 123.710727 \r\nL 125.107338 113.356924 \r\nL 125.725963 111.785168 \r\nL 127.581839 104.920103 \r\nL 128.200464 101.672673 \r\nL 128.81909 100.529578 \r\nL 130.05634 95.86194 \r\nL 130.674965 94.814103 \r\nL 131.293591 96.624003 \r\nL 131.912216 99.719886 \r\nL 132.530841 102.041798 \r\nL 133.149466 99.005451 \r\nL 133.768092 96.96421 \r\nL 134.386717 97.712665 \r\nL 135.005342 96.283796 \r\nL 135.623968 91.759045 \r\nL 136.242593 80.089949 \r\nL 136.861218 71.993026 \r\nL 137.479843 66.813376 \r\nL 138.098469 66.991985 \r\nL 138.717094 66.175488 \r\nL 139.954345 54.234227 \r\nL 140.57297 49.94762 \r\nL 141.191595 55.152785 \r\nL 141.81022 54.948661 \r\nL 142.428846 52.703296 \r\nL 143.047471 49.845558 \r\nL 143.666096 49.43731 \r\nL 144.284721 50.151744 \r\nL 144.903347 54.234227 \r\nL 145.521972 56.37753 \r\nL 146.140597 54.540413 \r\nL 146.759223 54.540413 \r\nL 147.377848 56.989903 \r\nL 147.996473 60.051764 \r\nL 148.615098 57.602275 \r\nL 149.233724 57.602275 \r\nL 151.708225 38.924918 \r\nL 152.945475 34.230063 \r\nL 153.564101 30.657891 \r\nL 154.182726 29.229022 \r\nL 154.801351 31.372325 \r\nL 155.419976 35.658932 \r\nL 156.038602 37.802235 \r\nL 156.657227 37.802235 \r\nL 157.275852 34.230063 \r\nL 157.894478 32.086759 \r\nL 158.513103 33.515628 \r\nL 159.131728 33.515628 \r\nL 159.750353 32.801194 \r\nL 160.368979 29.229022 \r\nL 160.987604 27.085718 \r\nL 161.606229 27.085718 \r\nL 162.224854 30.657891 \r\nL 162.84348 28.514587 \r\nL 163.462105 27.085718 \r\nL 164.08073 24.942415 \r\nL 164.699356 25.65685 \r\nL 166.555231 25.65685 \r\nL 167.173857 21.370243 \r\nL 169.029733 21.370243 \r\nL 169.648358 18.512505 \r\nL 170.266983 17.798071 \r\nL 170.885608 17.798071 \r\nL 171.504234 17.083636 \r\nL 174.59736 17.083636 \r\nL 175.215985 17.798071 \r\nL 175.834611 17.798071 \r\nL 176.453236 18.512505 \r\nL 177.071861 17.798071 \r\nL 178.927737 17.798071 \r\nL 179.546362 18.512505 \r\nL 180.164988 19.941374 \r\nL 180.783613 19.941374 \r\nL 181.402238 20.655809 \r\nL 182.020863 24.942415 \r\nL 183.876739 29.229022 \r\nL 184.495364 28.514587 \r\nL 185.11399 27.085718 \r\nL 185.732615 27.085718 \r\nL 186.35124 25.65685 \r\nL 186.969866 21.370243 \r\nL 187.588491 19.941374 \r\nL 188.207116 19.941374 \r\nL 188.825741 18.512505 \r\nL 189.444367 20.655809 \r\nL 190.681617 20.655809 \r\nL 191.300243 21.370243 \r\nL 192.537493 24.227981 \r\nL 193.156118 23.513546 \r\nL 193.774744 24.942415 \r\nL 194.393369 23.513546 \r\nL 196.249245 23.513546 \r\nL 196.86787 22.799112 \r\nL 197.486495 21.370243 \r\nL 198.105121 21.370243 \r\nL 198.723746 20.655809 \r\nL 199.342371 22.084677 \r\nL 199.960996 22.799112 \r\nL 202.435498 22.799112 \r\nL 203.672748 21.370243 \r\nL 204.291373 19.941374 \r\nL 204.909999 19.22694 \r\nL 206.147249 19.22694 \r\nL 206.765874 19.941374 \r\nL 207.3845 19.941374 \r\nL 208.003125 20.655809 \r\nL 211.096251 20.655809 \r\nL 211.714877 19.941374 \r\nL 212.333502 19.941374 \r\nL 212.952127 19.22694 \r\nL 214.808003 19.22694 \r\nL 215.426628 21.370243 \r\nL 219.13838 21.370243 \r\nL 219.757005 22.799112 \r\nL 220.375631 22.084677 \r\nL 224.087382 22.084677 \r\nL 224.706007 20.655809 \r\nL 229.65501 20.655809 \r\nL 230.273635 19.22694 \r\nL 230.89226 19.22694 \r\nL 231.510886 20.655809 \r\nL 233.985387 20.655809 \r\nL 234.604012 19.941374 \r\nL 235.222637 20.655809 \r\nL 235.841262 20.655809 \r\nL 236.459888 19.22694 \r\nL 238.934389 19.22694 \r\nL 240.171639 20.655809 \r\nL 242.027515 24.942415 \r\nL 244.502016 24.942415 \r\nL 246.357892 20.655809 \r\nL 246.976517 20.655809 \r\nL 247.595143 22.084677 \r\nL 251.306894 22.084677 \r\nL 252.544145 20.655809 \r\nL 256.874522 20.655809 \r\nL 257.493147 22.084677 \r\nL 261.823524 22.084677 \r\nL 262.442149 19.941374 \r\nL 264.298025 19.941374 \r\nL 264.91665 20.655809 \r\nL 266.153901 20.655809 \r\nL 266.772526 19.941374 \r\nL 267.391152 19.941374 \r\nL 268.009777 22.084677 \r\nL 269.247027 22.084677 \r\nL 269.865653 21.370243 \r\nL 272.340154 21.370243 \r\nL 272.958779 19.22694 \r\nL 324.923302 19.22694 \r\nL 325.541928 19.941374 \r\nL 328.635054 19.941374 \r\nL 329.872305 21.370243 \r\nL 330.49093 20.655809 \r\nL 334.202682 20.655809 \r\nL 334.821307 19.941374 \r\nL 337.914433 19.941374 \r\nL 338.533058 20.655809 \r\nL 339.151684 19.941374 \r\nL 342.24481 19.941374 \r\nL 342.863435 22.799112 \r\nL 343.482061 22.084677 \r\nL 344.719311 22.084677 \r\nL 345.337937 24.227981 \r\nL 347.193812 24.227981 \r\nL 347.812438 21.370243 \r\nL 349.668313 21.370243 \r\nL 350.286939 19.22694 \r\nL 353.380065 19.22694 \r\nL 354.617316 20.655809 \r\nL 358.329067 20.655809 \r\nL 359.566318 19.22694 \r\nL 360.184943 19.22694 \r\nL 360.184943 19.22694 \r\n\" style=\"fill:none;stroke:#2ca02c;stroke-linecap:square;stroke-width:1.5;\"/>\r\n   </g>\r\n   <g id=\"patch_3\">\r\n    <path d=\"M 40.603125 224.64 \r\nL 40.603125 7.2 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_4\">\r\n    <path d=\"M 375.403125 224.64 \r\nL 375.403125 7.2 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_5\">\r\n    <path d=\"M 40.603125 224.64 \r\nL 375.403125 224.64 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_6\">\r\n    <path d=\"M 40.603125 7.2 \r\nL 375.403125 7.2 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"legend_1\">\r\n    <g id=\"patch_7\">\r\n     <path d=\"M 232.58125 139.437187 \r\nL 368.403125 139.437187 \r\nQ 370.403125 139.437187 370.403125 137.437187 \r\nL 370.403125 94.402812 \r\nQ 370.403125 92.402812 368.403125 92.402812 \r\nL 232.58125 92.402812 \r\nQ 230.58125 92.402812 230.58125 94.402812 \r\nL 230.58125 137.437187 \r\nQ 230.58125 139.437187 232.58125 139.437187 \r\nz\r\n\" style=\"fill:#ffffff;opacity:0.8;stroke:#cccccc;stroke-linejoin:miter;\"/>\r\n    </g>\r\n    <g id=\"line2d_19\">\r\n     <path d=\"M 234.58125 100.50125 \r\nL 254.58125 100.50125 \r\n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\r\n    </g>\r\n    <g id=\"line2d_20\"/>\r\n    <g id=\"text_18\">\r\n     <!-- No Feature Selection -->\r\n     <g transform=\"translate(262.58125 104.00125)scale(0.1 -0.1)\">\r\n      <defs>\r\n       <path d=\"M 3425 4513 \r\nL 3425 3897 \r\nQ 3066 4069 2747 4153 \r\nQ 2428 4238 2131 4238 \r\nQ 1616 4238 1336 4038 \r\nQ 1056 3838 1056 3469 \r\nQ 1056 3159 1242 3001 \r\nQ 1428 2844 1947 2747 \r\nL 2328 2669 \r\nQ 3034 2534 3370 2195 \r\nQ 3706 1856 3706 1288 \r\nQ 3706 609 3251 259 \r\nQ 2797 -91 1919 -91 \r\nQ 1588 -91 1214 -16 \r\nQ 841 59 441 206 \r\nL 441 856 \r\nQ 825 641 1194 531 \r\nQ 1563 422 1919 422 \r\nQ 2459 422 2753 634 \r\nQ 3047 847 3047 1241 \r\nQ 3047 1584 2836 1778 \r\nQ 2625 1972 2144 2069 \r\nL 1759 2144 \r\nQ 1053 2284 737 2584 \r\nQ 422 2884 422 3419 \r\nQ 422 4038 858 4394 \r\nQ 1294 4750 2059 4750 \r\nQ 2388 4750 2728 4690 \r\nQ 3069 4631 3425 4513 \r\nz\r\n\" id=\"DejaVuSans-53\" transform=\"scale(0.015625)\"/>\r\n       <path d=\"M 603 4863 \r\nL 1178 4863 \r\nL 1178 0 \r\nL 603 0 \r\nL 603 4863 \r\nz\r\n\" id=\"DejaVuSans-6c\" transform=\"scale(0.015625)\"/>\r\n       <path d=\"M 3513 2113 \r\nL 3513 0 \r\nL 2938 0 \r\nL 2938 2094 \r\nQ 2938 2591 2744 2837 \r\nQ 2550 3084 2163 3084 \r\nQ 1697 3084 1428 2787 \r\nQ 1159 2491 1159 1978 \r\nL 1159 0 \r\nL 581 0 \r\nL 581 3500 \r\nL 1159 3500 \r\nL 1159 2956 \r\nQ 1366 3272 1645 3428 \r\nQ 1925 3584 2291 3584 \r\nQ 2894 3584 3203 3211 \r\nQ 3513 2838 3513 2113 \r\nz\r\n\" id=\"DejaVuSans-6e\" transform=\"scale(0.015625)\"/>\r\n      </defs>\r\n      <use xlink:href=\"#DejaVuSans-4e\"/>\r\n      <use x=\"74.804688\" xlink:href=\"#DejaVuSans-6f\"/>\r\n      <use x=\"135.986328\" xlink:href=\"#DejaVuSans-20\"/>\r\n      <use x=\"167.773438\" xlink:href=\"#DejaVuSans-46\"/>\r\n      <use x=\"219.792969\" xlink:href=\"#DejaVuSans-65\"/>\r\n      <use x=\"281.316406\" xlink:href=\"#DejaVuSans-61\"/>\r\n      <use x=\"342.595703\" xlink:href=\"#DejaVuSans-74\"/>\r\n      <use x=\"381.804688\" xlink:href=\"#DejaVuSans-75\"/>\r\n      <use x=\"445.183594\" xlink:href=\"#DejaVuSans-72\"/>\r\n      <use x=\"484.046875\" xlink:href=\"#DejaVuSans-65\"/>\r\n      <use x=\"545.570312\" xlink:href=\"#DejaVuSans-20\"/>\r\n      <use x=\"577.357422\" xlink:href=\"#DejaVuSans-53\"/>\r\n      <use x=\"640.833984\" xlink:href=\"#DejaVuSans-65\"/>\r\n      <use x=\"702.357422\" xlink:href=\"#DejaVuSans-6c\"/>\r\n      <use x=\"730.140625\" xlink:href=\"#DejaVuSans-65\"/>\r\n      <use x=\"791.664062\" xlink:href=\"#DejaVuSans-63\"/>\r\n      <use x=\"846.644531\" xlink:href=\"#DejaVuSans-74\"/>\r\n      <use x=\"885.853516\" xlink:href=\"#DejaVuSans-69\"/>\r\n      <use x=\"913.636719\" xlink:href=\"#DejaVuSans-6f\"/>\r\n      <use x=\"974.818359\" xlink:href=\"#DejaVuSans-6e\"/>\r\n     </g>\r\n    </g>\r\n    <g id=\"line2d_21\">\r\n     <path d=\"M 234.58125 115.179375 \r\nL 254.58125 115.179375 \r\n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\r\n    </g>\r\n    <g id=\"line2d_22\"/>\r\n    <g id=\"text_19\">\r\n     <!-- STG -->\r\n     <g transform=\"translate(262.58125 118.679375)scale(0.1 -0.1)\">\r\n      <defs>\r\n       <path d=\"M -19 4666 \r\nL 3928 4666 \r\nL 3928 4134 \r\nL 2272 4134 \r\nL 2272 0 \r\nL 1638 0 \r\nL 1638 4134 \r\nL -19 4134 \r\nL -19 4666 \r\nz\r\n\" id=\"DejaVuSans-54\" transform=\"scale(0.015625)\"/>\r\n       <path d=\"M 3809 666 \r\nL 3809 1919 \r\nL 2778 1919 \r\nL 2778 2438 \r\nL 4434 2438 \r\nL 4434 434 \r\nQ 4069 175 3628 42 \r\nQ 3188 -91 2688 -91 \r\nQ 1594 -91 976 548 \r\nQ 359 1188 359 2328 \r\nQ 359 3472 976 4111 \r\nQ 1594 4750 2688 4750 \r\nQ 3144 4750 3555 4637 \r\nQ 3966 4525 4313 4306 \r\nL 4313 3634 \r\nQ 3963 3931 3569 4081 \r\nQ 3175 4231 2741 4231 \r\nQ 1884 4231 1454 3753 \r\nQ 1025 3275 1025 2328 \r\nQ 1025 1384 1454 906 \r\nQ 1884 428 2741 428 \r\nQ 3075 428 3337 486 \r\nQ 3600 544 3809 666 \r\nz\r\n\" id=\"DejaVuSans-47\" transform=\"scale(0.015625)\"/>\r\n      </defs>\r\n      <use xlink:href=\"#DejaVuSans-53\"/>\r\n      <use x=\"63.476562\" xlink:href=\"#DejaVuSans-54\"/>\r\n      <use x=\"124.560547\" xlink:href=\"#DejaVuSans-47\"/>\r\n     </g>\r\n    </g>\r\n    <g id=\"line2d_23\">\r\n     <path d=\"M 234.58125 129.8575 \r\nL 254.58125 129.8575 \r\n\" style=\"fill:none;stroke:#2ca02c;stroke-linecap:square;stroke-width:1.5;\"/>\r\n    </g>\r\n    <g id=\"line2d_24\"/>\r\n    <g id=\"text_20\">\r\n     <!-- GINI + STG -->\r\n     <g transform=\"translate(262.58125 133.3575)scale(0.1 -0.1)\">\r\n      <defs>\r\n       <path d=\"M 628 4666 \r\nL 1259 4666 \r\nL 1259 0 \r\nL 628 0 \r\nL 628 4666 \r\nz\r\n\" id=\"DejaVuSans-49\" transform=\"scale(0.015625)\"/>\r\n       <path d=\"M 2944 4013 \r\nL 2944 2272 \r\nL 4684 2272 \r\nL 4684 1741 \r\nL 2944 1741 \r\nL 2944 0 \r\nL 2419 0 \r\nL 2419 1741 \r\nL 678 1741 \r\nL 678 2272 \r\nL 2419 2272 \r\nL 2419 4013 \r\nL 2944 4013 \r\nz\r\n\" id=\"DejaVuSans-2b\" transform=\"scale(0.015625)\"/>\r\n      </defs>\r\n      <use xlink:href=\"#DejaVuSans-47\"/>\r\n      <use x=\"77.490234\" xlink:href=\"#DejaVuSans-49\"/>\r\n      <use x=\"106.982422\" xlink:href=\"#DejaVuSans-4e\"/>\r\n      <use x=\"181.787109\" xlink:href=\"#DejaVuSans-49\"/>\r\n      <use x=\"211.279297\" xlink:href=\"#DejaVuSans-20\"/>\r\n      <use x=\"243.066406\" xlink:href=\"#DejaVuSans-2b\"/>\r\n      <use x=\"326.855469\" xlink:href=\"#DejaVuSans-20\"/>\r\n      <use x=\"358.642578\" xlink:href=\"#DejaVuSans-53\"/>\r\n      <use x=\"422.119141\" xlink:href=\"#DejaVuSans-54\"/>\r\n      <use x=\"483.203125\" xlink:href=\"#DejaVuSans-47\"/>\r\n     </g>\r\n    </g>\r\n   </g>\r\n  </g>\r\n </g>\r\n <defs>\r\n  <clipPath id=\"pd45b93ff69\">\r\n   <rect height=\"217.44\" width=\"334.8\" x=\"40.603125\" y=\"7.2\"/>\r\n  </clipPath>\r\n </defs>\r\n</svg>\r\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(np.arange(len(y)), y, label=\"No Feature Selection\")\n",
    "plt.plot(np.arange(len(y_stg)), y_stg, label='STG')\n",
    "plt.plot(np.arange(len(y_two_step)), y_two_step, label= 'GINI + STG')\n",
    "plt.xlabel(\"Number of Epochs\")\n",
    "plt.ylabel(\"Accuracy/# Features Ratio\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1579d4a348c2ef16482c05d3cfac916f73c8945ddf1938a1e045b3bdea82eece"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('VFL')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
