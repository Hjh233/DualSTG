{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rebuttal & Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We thank the reviewer for their constructive feedback. Below we respond, point-by-point, to each point and how we updated the paper:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reviewer 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> As there exist iterations in the designed algorithms, the analysis of convergence, time and storage complexities should be provided. However, these theoretical analyses are missing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Putting aside the context of federated learning because the encryption approaches we employed do not affect convergence, our method employs gradient-based optimization to educate stochastic gates to pick the optimal features. \n",
    "\n",
    "We present the convergence analysis of our method through 3 steps. First of all, we define what the best subset of features is. Secondly, we show that optimizing the objective function in Eq.4 converges to finding the best subset. Then, the convergence of our method only relies on the convergence of the gradient-optimization method, SGD. Thus, we present the convergence analysis of SGD and corresponding assumptions at last.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without loss of generality, we only consider the bottom level stochastic gates here.\n",
    "For a dataset $\\{\\bm{X}, Y\\}$ the best subset of features with size $k$ can be defined as:\n",
    "$$\\max_{S} I(X_{S}, Y)  \\quad\\text{ s.t. } \\quad |S| = k $$\n",
    "Where $I$ is the mutual information. \n",
    "In Proposition 1 of [1], it is proven that under two mild assumptions on the existence of an optimal subset of feature $S^*$, the equation above is equivalent to \n",
    "$$ \\max _{\\mathbf{0} \\leq \\boldsymbol{\\pi} \\leq \\mathbf{1}} I(\\boldsymbol{X} \\odot \\tilde{\\boldsymbol{S}} ; \\boldsymbol{Y}) \\quad \\text { s.t. } \\quad \\sum_{i} \\mathbb{E}\\left[\\tilde{S}_{i}\\right] \\leq k $$\n",
    "where $\\tilde{S}$ are independtly sampled from the Bernoulli distribution with parameter ${\\bm{\\pi}}$.\n",
    "Since the \n",
    "Let's call the maximization problem $\\tilde{R}$.\n",
    "\n",
    "If the maximization problem is learnable (here, learnable means it satisfies uniform convergence [3]), the empirical risk $R$ (for simiplicity, we omit the parameter) in Eq.3 converges to $\\tilde{R}$ as:\n",
    "$$ R \\rightarrow^{p} \\tilde{R} $$ \n",
    "where $\\rightarrow^{p}$ means convergence in probability. We adopts gradient-based method to optimize the empirical minimization \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
